{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "37zvqqgDBult",
        "PcZdEkzIsHhd",
        "4fXXP23UsMMq",
        "QZHY3IHKgttQ",
        "jb1dvyrsBrWm",
        "HQdU8UlnEOlf",
        "qKRBIIg2EUMY",
        "GHQsT2f4MDgq",
        "tSisDqlU1cxn",
        "NfLNVwe2V1Gz",
        "n1cZXQagV3_Y",
        "K970gbjDQcfo",
        "pF0bJmu0WHVH",
        "fj3RBXmKX5um",
        "NczsSPPefH8Z",
        "xpKEeJRno3TD",
        "BWEMb7rdfKUc",
        "VH90V4sDUCYA",
        "RPNXmCOJUELb",
        "KyTaigCfYq6M",
        "qbUYiFEY_yjM",
        "3xbGEie5_yjR",
        "9jMHkUtn_yjS",
        "Y99SmONsOaot",
        "M1CeWFYev5wk"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Preloads"
      ],
      "metadata": {
        "id": "37zvqqgDBult"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install catboost\n",
        "!pip install bayesian-optimization\n",
        "!pip install xgboost\n",
        "!pip install lightgbm"
      ],
      "metadata": {
        "id": "1789TJIGAxb2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21a025d0-9e40-4dba-d83a-0819069fdc45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from catboost) (0.21)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from catboost) (1.16.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (4.59.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (3.2.3)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly->catboost) (8.5.0)\n",
            "Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl (99.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: catboost\n",
            "Successfully installed catboost-1.2.8\n",
            "Collecting bayesian-optimization\n",
            "  Downloading bayesian_optimization-3.1.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting colorama>=0.4.6 (from bayesian-optimization)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: numpy>=1.25 in /usr/local/lib/python3.12/dist-packages (from bayesian-optimization) (2.0.2)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from bayesian-optimization) (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from bayesian-optimization) (1.16.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.0.0->bayesian-optimization) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.0.0->bayesian-optimization) (3.6.0)\n",
            "Downloading bayesian_optimization-3.1.0-py3-none-any.whl (36 kB)\n",
            "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Installing collected packages: colorama, bayesian-optimization\n",
            "Successfully installed bayesian-optimization-3.1.0 colorama-0.4.6\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.12/dist-packages (3.0.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from xgboost) (2.0.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.12/dist-packages (from xgboost) (2.27.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from xgboost) (1.16.1)\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.12/dist-packages (4.6.0)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from lightgbm) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from lightgbm) (1.16.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1aU7Tnc3-DOT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31f29623-74ed-4e34-e683-aa8235cb1d36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading store-sales-time-series-forecasting.zip to /content\n",
            "\r  0% 0.00/21.4M [00:00<?, ?B/s]\n",
            "\r100% 21.4M/21.4M [00:00<00:00, 914MB/s]\n",
            "Archive:  store-sales-time-series-forecasting.zip\n",
            "  inflating: store-sales/holidays_events.csv  \n",
            "  inflating: store-sales/oil.csv     \n",
            "  inflating: store-sales/sample_submission.csv  \n",
            "  inflating: store-sales/stores.csv  \n",
            "  inflating: store-sales/test.csv    \n",
            "  inflating: store-sales/train.csv   \n",
            "  inflating: store-sales/transactions.csv  \n"
          ]
        }
      ],
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "os.environ['KAGGLE_USERNAME'] = userdata.get('KAGGLE_USERNAME')\n",
        "os.environ['KAGGLE_KEY'] = userdata.get('KAGGLE_KEY')\n",
        "\n",
        "!kaggle competitions download -c store-sales-time-series-forecasting\n",
        "!unzip -o store-sales-time-series-forecasting.zip -d store-sales"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from collections import deque, defaultdict\n",
        "from catboost import CatBoostRegressor, Pool\n",
        "from sklearn.metrics import root_mean_squared_log_error\n",
        "from bayes_opt import BayesianOptimization\n",
        "import math\n",
        "\n",
        "from sklearn.linear_model import GammaRegressor\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device  )"
      ],
      "metadata": {
        "id": "EusLSyE3As5I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8df18093-da30-4ff7-8c96-44be0d1547db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option(\"display.max_rows\", None)\n",
        "pd.set_option(\"display.max_columns\", None)\n",
        "pd.set_option(\"display.max_colwidth\", None)\n",
        "pd.set_option(\"display.float_format\", '{:.2f}'.format)\n"
      ],
      "metadata": {
        "id": "v7QuJnY_lOyb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_PATH = 'store-sales/'\n",
        "data_files = [f for f in os.listdir(DATA_PATH) if f.endswith('.csv')]\n",
        "\n",
        "df = {}\n",
        "for filename in data_files:\n",
        "  df_name = filename.split('.')[0]\n",
        "  df[df_name] = pd.read_csv(os.path.join(DATA_PATH, filename))\n",
        "  print(f'{filename} loaded')\n",
        "\n",
        "meta_features = ['date', 'store_nbr', 'id', 'sales', 'family']"
      ],
      "metadata": {
        "id": "rCC8zDFBArot",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51841cac-5972-4bdb-e9d8-dafc65c506a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_submission.csv loaded\n",
            "transactions.csv loaded\n",
            "test.csv loaded\n",
            "stores.csv loaded\n",
            "oil.csv loaded\n",
            "train.csv loaded\n",
            "holidays_events.csv loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load data"
      ],
      "metadata": {
        "id": "R-iVWr84M0aV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['train']['date'] = pd.to_datetime(df['train']['date'])\n",
        "start_date = df['train'].loc[df['train'].index[0], 'date']\n",
        "end_date = df['train'].loc[df['train'].index[-1], 'date']\n",
        "DAYS_TO_PREDICT = 15\n",
        "end_of_train = end_date - pd.Timedelta(days=DAYS_TO_PREDICT)\n",
        "start_valid_date = end_of_train + pd.Timedelta(days=1)\n",
        "\n",
        "\n",
        "all_dates = pd.date_range(start_date, end_date, freq='D')\n",
        "all_stores = df['train']['store_nbr'].unique()\n",
        "all_families = df['train']['family'].unique()\n",
        "\n",
        "full_train = df['train'].copy()\n",
        "_train = full_train[full_train['date'] <= end_of_train]\n",
        "_valid = full_train[full_train['date'] > end_of_train]"
      ],
      "metadata": {
        "id": "_T4Aemy-B418"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "discrete_sales_families = []\n",
        "continuous_sales_families = []\n",
        "for family, group in _train.groupby('family'):\n",
        "  if np.allclose(group['sales'], group['sales'].astype('int')):\n",
        "    discrete_sales_families.append(family)\n",
        "  else:\n",
        "    continuous_sales_families.append(family)\n",
        "\n",
        "print(discrete_sales_families)\n",
        "print(continuous_sales_families)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QY6HRXtzrG8T",
        "outputId": "10fa294c-6a17-4313-aad0-d8a69c0f59b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['AUTOMOTIVE', 'BABY CARE', 'BEAUTY', 'BEVERAGES', 'BOOKS', 'CELEBRATION', 'CLEANING', 'DAIRY', 'EGGS', 'GROCERY II', 'HARDWARE', 'HOME AND KITCHEN I', 'HOME AND KITCHEN II', 'HOME APPLIANCES', 'HOME CARE', 'LADIESWEAR', 'LAWN AND GARDEN', 'LINGERIE', 'LIQUOR,WINE,BEER', 'MAGAZINES', 'PERSONAL CARE', 'PET SUPPLIES', 'PLAYERS AND ELECTRONICS', 'SCHOOL AND OFFICE SUPPLIES']\n",
            "['BREAD/BAKERY', 'DELI', 'FROZEN FOODS', 'GROCERY I', 'MEATS', 'POULTRY', 'PREPARED FOODS', 'PRODUCE', 'SEAFOOD']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "holidays_events = df['holidays_events'].copy()\n",
        "holidays_events['date'] = pd.to_datetime(holidays_events['date'])\n",
        "holidays_events = holidays_events[\n",
        "    (holidays_events['type'] != 'Work Day') | (holidays_events['transferred'] == True)\n",
        "]\n",
        "holidays_events['type'] = holidays_events['type'].replace({'Bridge': 'Holiday', 'Additional': 'Event', 'Transfer': 'Holiday'})"
      ],
      "metadata": {
        "id": "mT6MWmY7bvH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "  In a good way, we need to come up with two more models -\n",
        "  one predicts oil, the second predicts transactions, but I'll make it easier - I'll fill in the data last year."
      ],
      "metadata": {
        "id": "i3vYINDcoKku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "oil_data = df['oil'].copy()\n",
        "oil_data['date'] = pd.to_datetime(oil_data['date'])\n",
        "missing_dates = all_dates.difference(oil_data['date'])\n",
        "full_data = pd.DataFrame({'date': all_dates})\n",
        "oil_data = (pd\n",
        "            .merge(full_data, oil_data, on='date', how='left')\n",
        "            .fillna(method='ffill')\n",
        "            .fillna(method='bfill')\n",
        "            .sort_values(by='date')\n",
        "            .reset_index(drop=True))\n",
        "\n",
        "\n",
        "\n",
        "last_date = oil_data['date'].max()\n",
        "target_date = pd.to_datetime(\"2017-12-26\")\n",
        "extra_dates = pd.date_range(start=last_date + pd.Timedelta(days=1), end=target_date, freq=\"D\")\n",
        "\n",
        "\n",
        "past_dates = extra_dates - pd.DateOffset(years=1)\n",
        "past_data = oil_data[oil_data['date'].isin(past_dates)][['date', 'dcoilwtico']].copy()\n",
        "\n",
        "past_data['date'] = past_data['date'] + pd.DateOffset(years=1)\n",
        "\n",
        "\n",
        "oil_data = pd.concat([oil_data, past_data], ignore_index=True).sort_values('date').reset_index(drop=True)\n",
        "\n",
        "\n",
        "for period in [1, 7, 15, 30, 365]:\n",
        "    oil_data[f'oil_lag_{period}'] = oil_data['dcoilwtico'].shift(period)\n",
        "\n",
        "oil_data['oil_rolling_mean_30'] = oil_data['dcoilwtico'].rolling(window=30, min_periods=1).mean().shift(1)\n",
        "oil_data['oil_rolling_std_30'] = oil_data['dcoilwtico'].rolling(window=30, min_periods=1).std().shift(1)\n",
        "oil_data['oil_rolling_mean_15'] = oil_data['dcoilwtico'].rolling(window=15, min_periods=1).mean().shift(1)\n",
        "oil_data['oil_rolling_std_15'] = oil_data['dcoilwtico'].rolling(window=15, min_periods=1).std().shift(1)\n",
        "oil_data['oil_rolling_median_7'] = oil_data['dcoilwtico'].rolling(window=7, min_periods=1).median().shift(1)\n",
        "oil_data['oil_rolling_mean_7'] = oil_data['dcoilwtico'].rolling(window=7, min_periods=1).mean().shift(1)\n"
      ],
      "metadata": {
        "id": "H8yLW_Whc1zs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03134ff2-4bab-4d91-c942-05bca7a9daac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3826997909.py:7: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  .fillna(method='ffill')\n",
            "/tmp/ipython-input-3826997909.py:8: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  .fillna(method='bfill')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "assert len(extra_dates) == past_data.shape[0], \"The number of dates does not match!\"\n"
      ],
      "metadata": {
        "id": "10r0zY84pWeg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transactions = df['transactions'].copy()\n",
        "transactions['date'] = pd.to_datetime(transactions['date'])\n",
        "\n",
        "full_index = pd.MultiIndex.from_product([all_dates, all_stores], names=['date', 'store_nbr'])\n",
        "full_data = pd.DataFrame(index=full_index).reset_index()\n",
        "transactions = pd.merge(full_data, transactions, on=['date', 'store_nbr'], how='left')\n",
        "\n",
        "transactions['transactions'] = transactions['transactions'].fillna(0)\n",
        "transactions = transactions.sort_values(['date', 'store_nbr']).reset_index(drop=True)\n",
        "\n",
        "last_date = transactions['date'].max()\n",
        "target_date = pd.to_datetime(\"2017-12-26\")\n",
        "extra_dates = pd.date_range(start=last_date + pd.Timedelta(days=1), end=target_date, freq=\"D\")\n",
        "\n",
        "past_dates = extra_dates - pd.DateOffset(years=1)\n",
        "\n",
        "past_data = transactions[transactions['date'].isin(past_dates)][['date', 'store_nbr', 'transactions']].copy()\n",
        "\n",
        "past_data['date'] = past_data['date'] + pd.DateOffset(years=1)\n",
        "\n",
        "transactions = pd.concat([transactions, past_data], ignore_index=True)\n",
        "\n",
        "transactions = transactions.drop_duplicates(subset=['date', 'store_nbr']).sort_values(['date', 'store_nbr']).reset_index(drop=True)\n",
        "\n",
        "assert len(extra_dates) * len(all_stores) == past_data.shape[0], \"The number of dates does not match!\""
      ],
      "metadata": {
        "id": "8GgxQMAvdZeC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_data(_data, train=True):\n",
        "  data = _data.copy()\n",
        "  data['date'] = pd.to_datetime(data['date'])\n",
        "\n",
        "  # --- filling gaps in train ---\n",
        "  if train:\n",
        "    start_date = data['date'].min()\n",
        "    end_date = data['date'].max()\n",
        "    all_dates = pd.date_range(start_date, end_date, freq='D')\n",
        "    missing_dates = all_dates.difference(data['date'])\n",
        "\n",
        "    full_index = pd.MultiIndex.from_product([all_dates, all_stores, all_families], names=['date', 'store_nbr', 'family'])\n",
        "    full_data = pd.DataFrame(index=full_index).reset_index()\n",
        "\n",
        "    data = pd.merge(full_data, data, on=['date', 'store_nbr', 'family'], how='left')\n",
        "    data[['sales', 'onpromotion']] = data[['sales', 'onpromotion']].fillna(0)\n",
        "\n",
        "\n",
        "  # --- date features ---\n",
        "  data['year'] = data['date'].dt.year\n",
        "  data['month'] = data['date'].dt.month\n",
        "  data['day_of_week'] = data['date'].dt.dayofweek\n",
        "  day_of_year = data['date'].dt.dayofyear\n",
        "  for period in [7, 30.5, 365.25]:\n",
        "      data[f'sin_{period}'] = np.sin(2 * np.pi * day_of_year / period)\n",
        "      data[f'cos_{period}'] = np.cos(2 * np.pi * day_of_year / period)\n",
        "  last_day = data['date'].dt.is_month_end\n",
        "  payday_15 = data['date'].dt.day == 15\n",
        "  data['is_payday'] = (last_day | payday_15).astype('int')\n",
        "  data['is_weekend'] = data['day_of_week'].isin([5, 6]).astype('int')\n",
        "  is_dec_25 = ((data['date'].dt.month == 12) & (data['date'].dt.day == 25))\n",
        "  is_jan_1 = ((data['date'].dt.month == 1) & (data['date'].dt.day == 1))\n",
        "  data['is_christmas'] = (is_dec_25 | is_jan_1).astype('int')\n",
        "  earthquake_date = pd.to_datetime('2016-04-16')\n",
        "  days_since_eq = (data['date'] - earthquake_date + pd.Timedelta(days=1)).dt.days\n",
        "  data['earthquake_effect'] = np.where( days_since_eq > 0, np.clip(1 - days_since_eq / 30, 0, 1), 0)\n",
        "\n",
        "\n",
        "  # --- add lags, rolling stats\n",
        "  if train:\n",
        "    grouped_data = data.groupby(['store_nbr', 'family'])\n",
        "    for period in [1, 7, 15, 30, 365]:\n",
        "      data[f'sales_lag_{period}'] = grouped_data['sales'].shift(period)\n",
        "\n",
        "    data[f'sales_rolling_mean_30'] = grouped_data['sales'].transform(lambda x: x.rolling(window=30, min_periods=1).mean().shift(1))\n",
        "    data[f'sales_rolling_std_30'] = grouped_data['sales'].transform(lambda x: x.rolling(window=30, min_periods=1).std().shift(1))\n",
        "    data[f'sales_rolling_mean_15'] = grouped_data['sales'].transform(lambda x: x.rolling(window=15, min_periods=1).mean().shift(1))\n",
        "    data[f'sales_rolling_std_15'] = grouped_data['sales'].transform(lambda x: x.rolling(window=15, min_periods=1).std().shift(1))\n",
        "    data[f'sales_rolling_median_7'] = grouped_data['sales'].transform(lambda x: x.rolling(window=7, min_periods=1).median().shift(1))\n",
        "    data[f'sales_rolling_mean_7'] = grouped_data['sales'].transform(lambda x: x.rolling(window=7, min_periods=1).mean().shift(1))\n",
        "  else:\n",
        "    for period in [1, 7, 15, 30, 365]:\n",
        "      data[f'sales_lag_{period}'] = np.nan\n",
        "\n",
        "    data[f'sales_rolling_mean_30'] = np.nan\n",
        "    data[f'sales_rolling_std_30'] = np.nan\n",
        "    data[f'sales_rolling_mean_15'] = np.nan\n",
        "    data[f'sales_rolling_std_15'] = np.nan\n",
        "    data[f'sales_rolling_median_7'] = np.nan\n",
        "    data[f'sales_rolling_mean_7'] = np.nan\n",
        "\n",
        "\n",
        "  # --- add info about stores ---\n",
        "  data = (pd\n",
        "          .merge(data, df['stores'], on='store_nbr', how='left')\n",
        "          .rename(columns={'city': 'store_city', 'state': 'store_state', 'type': 'store_type', 'cluster': 'store_cluster'}))\n",
        "\n",
        "  # --- add info about holidays ---\n",
        "  data = pd.merge(data, holidays_events, on='date', how='left')\n",
        "  data['locale'] = data['locale'].fillna('NONE')\n",
        "  data['is_holiday'] = (data['type'] == 'Holiday').astype('int')\n",
        "  data['is_event'] = (data['type'] == 'Event').astype('int')\n",
        "\n",
        "  data['store_in_event_area'] = 0\n",
        "  local_holiday_condition = (data['locale'] == 'Local') & (data['store_city'] == data['locale_name'])\n",
        "  data.loc[local_holiday_condition, 'store_in_event_area'] = 1\n",
        "\n",
        "  regional_holiday_condition = (data['locale'] == 'Regional') & (data['store_state'] == data['locale_name'])\n",
        "  data.loc[regional_holiday_condition, 'store_in_event_area'] = 1\n",
        "\n",
        "  national_holiday_condition = (data['locale'] == 'National')\n",
        "  data.loc[national_holiday_condition, 'store_in_event_area'] = 1\n",
        "\n",
        "  data = data.drop(columns=['type', 'locale_name', 'description', 'transferred']).rename(columns={'locale': 'holiday_locale'})\n",
        "\n",
        "\n",
        "  # --- oil ---\n",
        "  data = pd.merge(data, oil_data, on='date', how='left')\n",
        "\n",
        "  # --- transactions ---\n",
        "  data = pd.merge(data, transactions, on=['date', 'store_nbr'], how='left')\n",
        "\n",
        "\n",
        "  # --- sort for time order ---\n",
        "  data = data.sort_values(['date', 'store_nbr', 'family']).reset_index(drop=True)\n",
        "  cat_features = ['year', 'month', 'day_of_week', 'store_city', 'store_state', 'store_type', 'store_cluster', 'holiday_locale']\n",
        "  num_features = [col for col in data.columns if col not in meta_features and col not in cat_features]\n",
        "\n",
        "  data[cat_features] = data[cat_features].astype('category')\n",
        "  if train:\n",
        "    discrete_sales_families = []\n",
        "    continuous_sales_families = []\n",
        "    for family, group in data.groupby('family'):\n",
        "      if np.allclose(group['sales'], group['sales'].astype('int')):\n",
        "        discrete_sales_families.append(family)\n",
        "    else:\n",
        "      continuous_sales_families.append(family)\n",
        "\n",
        "\n",
        "  if train:\n",
        "    return data, cat_features, num_features\n",
        "  return data\n"
      ],
      "metadata": {
        "id": "TEE6UPlzG8LG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_train, cat_features, num_features = make_data(full_train, train=True)\n",
        "train, cat_features, num_features  = make_data(_train, train=True)\n",
        "valid = make_data(_valid, False)"
      ],
      "metadata": {
        "id": "i1ZOXlbfopXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_family_data(data, family):\n",
        "  return data[data['family'] == family]"
      ],
      "metadata": {
        "id": "z7i6ifNktWGF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CatBoostFamily"
      ],
      "metadata": {
        "id": "GYUOvgfPPy-P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "class CatboostModelsSystem:\n",
        "  def __init__(self, cat_features, numeric_features, families, model_path=None):\n",
        "    self.families = families\n",
        "    self.num_features = numeric_features\n",
        "    self.cat_features = cat_features\n",
        "    self.model_path = model_path\n",
        "    self.families_models = {}\n",
        "\n",
        "  def _make_train_pool(self, train, fam):\n",
        "    fam_train = get_family_data(train, fam).copy()\n",
        "    if fam in ['BABY CARE', 'MAGAZINES']:\n",
        "        fam_train = fam_train[fam_train['date'] >= '2016-01-01']\n",
        "\n",
        "    elif fam in ['BOOKS', 'LAWN AND GARDEN']:\n",
        "          fam_train = fam_train[fam_train['date'] >= '2016-10-08']\n",
        "\n",
        "    elif fam in ['CELEBRATION', 'HOME CARE', 'LADIESWEAR', 'PET SUPPLIES', 'PLAYERS AND ELECTRONICS', 'PRODUCE', 'SCHOOL AND OFFICE SUPPLIES']:\n",
        "          fam_train = fam_train[fam_train['date'] >= '2015-06-01']\n",
        "\n",
        "    elif fam in ['HOME AND KITCHEN I', 'HOME AND KITCHEN II']:\n",
        "          fam_train = fam_train[fam_train['date'] >= '2015-01-01']\n",
        "\n",
        "    X = fam_train[self.cat_features + self.num_features]\n",
        "    y = np.log1p(fam_train['sales'])\n",
        "\n",
        "    train_pool = Pool(X, y, self.cat_features)\n",
        "    return train_pool\n",
        "\n",
        "  def fit(self, train, valid, full_train):\n",
        "    end_of_train = train['date'].max()\n",
        "    historic_data = train[(train['date'] <= end_of_train) & (train['date'] >= end_of_train - pd.Timedelta(days=30))]\n",
        "\n",
        "    for fam in self.families:\n",
        "      print('FAMILY:', fam)\n",
        "      train_pool = self._make_train_pool(train, fam)\n",
        "\n",
        "\n",
        "      self.families_models[fam] = {\n",
        "          'historic_data': get_family_data(historic_data, fam),\n",
        "          'is_discrete': fam in discrete_sales_families,\n",
        "      }\n",
        "\n",
        "\n",
        "      model_path = None\n",
        "      if self.model_path is not None:\n",
        "        fam_name = '-'.join(fam.split('/'))\n",
        "        model_path = self.model_path + f'{fam_name}_model.cbm'\n",
        "\n",
        "      if model_path is not None and os.path.exists(model_path):\n",
        "        model = CatBoostRegressor()\n",
        "        model.load_model(model_path)\n",
        "        print('Model is loaded')\n",
        "      else:\n",
        "        pbounds = {\n",
        "          'depth': (4, 10),\n",
        "          'border_count': (32, 255),\n",
        "          'iterations': (100, 1000),\n",
        "\n",
        "          'learning_rate': (0.01, 0.1),\n",
        "          'l2_leaf_reg': (1, 10),\n",
        "          'random_strength': (0.0, 8.0),\n",
        "          'bagging_temperature': (0.0, 1.0),\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "        def objective(depth, border_count, iterations, learning_rate, l2_leaf_reg, random_strength,\n",
        "                      bagging_temperature):\n",
        "\n",
        "\n",
        "          iterations= int(iterations)\n",
        "          depth = int(depth)\n",
        "          border_count = int(border_count)\n",
        "\n",
        "          model = CatBoostRegressor(\n",
        "                depth=depth,\n",
        "                border_count=border_count,\n",
        "                iterations=iterations,\n",
        "\n",
        "                learning_rate=learning_rate,\n",
        "                l2_leaf_reg=l2_leaf_reg,\n",
        "                random_strength=random_strength,\n",
        "                bagging_temperature=bagging_temperature,\n",
        "                loss_function='RMSE',\n",
        "                verbose=0,\n",
        "                task_type='GPU',\n",
        "            )\n",
        "\n",
        "          model.fit(train_pool)\n",
        "          print('\\ntrain score:', model.get_best_score()['learn'])\n",
        "\n",
        "          self.families_models[fam]['model'] = model\n",
        "          pred_frame, common_loss = self.predict_family(valid, fam, True, verbose=False)\n",
        "\n",
        "          return -common_loss #optimize rmsle metric\n",
        "\n",
        "\n",
        "        optimizer = BayesianOptimization(\n",
        "            f=objective,\n",
        "            pbounds=pbounds,\n",
        "            random_state=42,\n",
        "        )\n",
        "\n",
        "\n",
        "        optimizer.maximize(init_points=3, n_iter=7)\n",
        "        best_params = optimizer.max['params']\n",
        "\n",
        "\n",
        "        model = CatBoostRegressor(\n",
        "                depth=int(best_params['depth']),\n",
        "                border_count=int(best_params['border_count']),\n",
        "                iterations=int(best_params['iterations']),\n",
        "\n",
        "                learning_rate=best_params['learning_rate'],\n",
        "                l2_leaf_reg=best_params['l2_leaf_reg'],\n",
        "                random_strength=best_params['random_strength'],\n",
        "                bagging_temperature=best_params['bagging_temperature'],\n",
        "                loss_function='RMSE',\n",
        "                verbose=100,\n",
        "                task_type='GPU',\n",
        "        )\n",
        "\n",
        "        train_pool = self._make_train_pool(full_train, fam)\n",
        "        model.fit(train_pool)\n",
        "        if self.model_path is not None:\n",
        "          model.save_model(model_path)\n",
        "\n",
        "\n",
        "\n",
        "      end_of_train = full_train['date'].max()\n",
        "      historic_data = full_train[(full_train['date'] <= end_of_train) & (full_train['date'] >= end_of_train - pd.Timedelta(days=30))]\n",
        "\n",
        "      # put the model\n",
        "      self.families_models[fam]['historic_data'] = historic_data\n",
        "      self.families_models[fam]['model'] = model\n",
        "\n",
        "\n",
        "  def get_family(self, family):\n",
        "    return self.families_models[family]\n",
        "\n",
        "  def predict_family(self, data, family, for_validation=True, verbose=True):\n",
        "    fam = self.get_family(family)\n",
        "    model = fam['model']\n",
        "    historic_data = fam['historic_data'].copy()\n",
        "    is_discrete = fam['is_discrete']\n",
        "\n",
        "    history = {s: deque(maxlen=30) for s in historic_data['store_nbr'].unique()}\n",
        "\n",
        "    for _, row in historic_data.iterrows():\n",
        "        history[row.store_nbr].append(row.sales)\n",
        "\n",
        "    family_data = data[data['family'] == family].sort_values(\"date\")\n",
        "\n",
        "\n",
        "\n",
        "    pred_frame = pd.DataFrame({\"id\": family_data[\"id\"], \"sales\": np.nan})\n",
        "\n",
        "    common_loss = 0\n",
        "    if verbose:\n",
        "      print(f\"\\nFAMILY {family}\\n\")\n",
        "\n",
        "    for date, daily_data in family_data.groupby(\"date\"):\n",
        "        features = []\n",
        "\n",
        "        for _, row in daily_data.iterrows():\n",
        "            key = row.store_nbr\n",
        "            hist = history[key]\n",
        "            arr = np.array(hist)\n",
        "\n",
        "            feats = {\n",
        "                \"sales_lag_1\": arr[-1] if len(arr) >= 1 else np.nan,\n",
        "                \"sales_lag_7\": arr[-7] if len(arr) >= 7 else np.nan,\n",
        "                \"sales_rolling_mean_30\": arr[-30:].mean() if len(arr) > 0 else np.nan,\n",
        "                \"sales_rolling_std_30\": arr[-30:].std() if len(arr) > 0 else np.nan,\n",
        "                \"sales_rolling_mean_15\": arr[-15:].mean() if len(arr) > 0 else np.nan,\n",
        "                \"sales_rolling_std_15\": arr[-15:].std() if len(arr) > 0 else np.nan,\n",
        "                \"sales_rolling_median_7\": np.median(arr[-7:]) if len(arr) > 0 else np.nan,\n",
        "                \"sales_rolling_mean_7\": arr[-7:].mean() if len(arr) > 0 else np.nan,\n",
        "            }\n",
        "            features.append(feats)\n",
        "\n",
        "        features_df = pd.DataFrame(features, index=daily_data.index)\n",
        "        predict_today = daily_data.copy()\n",
        "        predict_today.update(features_df)\n",
        "\n",
        "\n",
        "        X = predict_today[self.cat_features + self.num_features]\n",
        "\n",
        "\n",
        "        y_pred = np.maximum(0, np.expm1(model.predict(X)))\n",
        "\n",
        "        if is_discrete:\n",
        "          y_pred = np.round(y_pred)\n",
        "\n",
        "        for (_, row), pred in zip(predict_today.iterrows(), y_pred):\n",
        "            history[row.store_nbr].append(pred)\n",
        "\n",
        "\n",
        "\n",
        "        pred_frame.loc[predict_today.index, \"sales\"] = y_pred\n",
        "\n",
        "\n",
        "        if for_validation:\n",
        "          y_true = np.array(predict_today[\"sales\"])\n",
        "\n",
        "          loss = root_mean_squared_log_error(y_true, y_pred)\n",
        "\n",
        "          if verbose:\n",
        "            print(f\"Loss on {date.date()}: {loss}\")\n",
        "          common_loss += loss\n",
        "\n",
        "\n",
        "    if for_validation:\n",
        "      common_loss /= family_data[\"date\"].nunique()\n",
        "      if verbose:\n",
        "        print(f\"\\nCommon Loss: {common_loss}\")\n",
        "\n",
        "      return (pred_frame, common_loss)\n",
        "\n",
        "    return pred_frame\n",
        "\n",
        "\n",
        "  def predict(self, data, for_validation=True, verbose=True):\n",
        "    pred_frames = []\n",
        "    for family in self.families:\n",
        "      pred_frame = self.predict_family(data, family, for_validation, verbose)\n",
        "      pred_frames.append(pred_frame)\n",
        "    return pd.concat(pred_frames).sort_values(by='id').reset_index(drop=True)\n",
        "\n",
        "\n",
        "catboost_models = CatboostModelsSystem(\n",
        "    cat_features,\n",
        "    num_features,\n",
        "    all_families,\n",
        "    './',\n",
        "    )"
      ],
      "metadata": {
        "id": "BfQL8aEeP3du"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "catboost_models.fit(train, valid, full_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QMOU7DRQYJsU",
        "outputId": "b14a50ed-625d-4fe2-f5fb-3c78d4850764"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FAMILY: AUTOMOTIVE\n",
            "Model is loaded\n",
            "FAMILY: BABY CARE\n",
            "Model is loaded\n",
            "FAMILY: BEAUTY\n",
            "Model is loaded\n",
            "FAMILY: BEVERAGES\n",
            "Model is loaded\n",
            "FAMILY: BOOKS\n",
            "Model is loaded\n",
            "FAMILY: BREAD/BAKERY\n",
            "Model is loaded\n",
            "FAMILY: CELEBRATION\n",
            "Model is loaded\n",
            "FAMILY: CLEANING\n",
            "Model is loaded\n",
            "FAMILY: DAIRY\n",
            "Model is loaded\n",
            "FAMILY: DELI\n",
            "Model is loaded\n",
            "FAMILY: EGGS\n",
            "Model is loaded\n",
            "FAMILY: FROZEN FOODS\n",
            "Model is loaded\n",
            "FAMILY: GROCERY I\n",
            "Model is loaded\n",
            "FAMILY: GROCERY II\n",
            "Model is loaded\n",
            "FAMILY: HARDWARE\n",
            "Model is loaded\n",
            "FAMILY: HOME AND KITCHEN I\n",
            "Model is loaded\n",
            "FAMILY: HOME AND KITCHEN II\n",
            "Model is loaded\n",
            "FAMILY: HOME APPLIANCES\n",
            "Model is loaded\n",
            "FAMILY: HOME CARE\n",
            "Model is loaded\n",
            "FAMILY: LADIESWEAR\n",
            "Model is loaded\n",
            "FAMILY: LAWN AND GARDEN\n",
            "Model is loaded\n",
            "FAMILY: LINGERIE\n",
            "Model is loaded\n",
            "FAMILY: LIQUOR,WINE,BEER\n",
            "Model is loaded\n",
            "FAMILY: MAGAZINES\n",
            "Model is loaded\n",
            "FAMILY: MEATS\n",
            "Model is loaded\n",
            "FAMILY: PERSONAL CARE\n",
            "Model is loaded\n",
            "FAMILY: PET SUPPLIES\n",
            "Model is loaded\n",
            "FAMILY: PLAYERS AND ELECTRONICS\n",
            "Model is loaded\n",
            "FAMILY: POULTRY\n",
            "Model is loaded\n",
            "FAMILY: PREPARED FOODS\n",
            "Model is loaded\n",
            "FAMILY: PRODUCE\n",
            "Model is loaded\n",
            "FAMILY: SCHOOL AND OFFICE SUPPLIES\n",
            "Model is loaded\n",
            "FAMILY: SEAFOOD\n",
            "Model is loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "catboost_models.get_family('AUTOMOTIVE')['historic_data'].tail()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "id": "gTMEX8I1h3_G",
        "outputId": "e81e703b-9323-46af-a864-8a6d2c4558ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              date  store_nbr                      family         id  sales  \\\n",
              "3059689 2017-08-15         54                     POULTRY 3000751.00  59.62   \n",
              "3059690 2017-08-15         54              PREPARED FOODS 3000752.00  94.00   \n",
              "3059691 2017-08-15         54                     PRODUCE 3000753.00 915.37   \n",
              "3059692 2017-08-15         54  SCHOOL AND OFFICE SUPPLIES 3000754.00   0.00   \n",
              "3059693 2017-08-15         54                     SEAFOOD 3000755.00   3.00   \n",
              "\n",
              "         onpromotion  year month day_of_week  sin_7  cos_7  sin_30.5  \\\n",
              "3059689         0.00  2017     8           1   0.43  -0.90      0.35   \n",
              "3059690         0.00  2017     8           1   0.43  -0.90      0.35   \n",
              "3059691        76.00  2017     8           1   0.43  -0.90      0.35   \n",
              "3059692         0.00  2017     8           1   0.43  -0.90      0.35   \n",
              "3059693         0.00  2017     8           1   0.43  -0.90      0.35   \n",
              "\n",
              "         cos_30.5  sin_365.25  cos_365.25  is_payday  is_weekend  \\\n",
              "3059689     -0.94       -0.69       -0.72          1           0   \n",
              "3059690     -0.94       -0.69       -0.72          1           0   \n",
              "3059691     -0.94       -0.69       -0.72          1           0   \n",
              "3059692     -0.94       -0.69       -0.72          1           0   \n",
              "3059693     -0.94       -0.69       -0.72          1           0   \n",
              "\n",
              "         is_christmas  earthquake_effect  sales_lag_1  sales_lag_7  \\\n",
              "3059689             0               0.00        56.16        73.29   \n",
              "3059690             0               0.00       147.00        86.00   \n",
              "3059691             0               0.00       585.62       790.01   \n",
              "3059692             0               0.00         0.00         0.00   \n",
              "3059693             0               0.00         0.00        12.00   \n",
              "\n",
              "         sales_lag_15  sales_lag_30  sales_lag_365  sales_rolling_mean_30  \\\n",
              "3059689         96.94        103.90          20.11                  69.01   \n",
              "3059690        100.00         50.00         114.00                  88.77   \n",
              "3059691        633.39        981.71         598.23                 687.86   \n",
              "3059692          0.00          0.00           0.00                   0.00   \n",
              "3059693          4.00          5.00           5.00                   2.97   \n",
              "\n",
              "         sales_rolling_std_30  sales_rolling_mean_15  sales_rolling_std_15  \\\n",
              "3059689                 20.17                  68.62                 17.39   \n",
              "3059690                 29.43                  89.73                 27.99   \n",
              "3059691                161.92                 671.31                174.08   \n",
              "3059692                  0.00                   0.00                  0.00   \n",
              "3059693                  2.61                   2.60                  3.11   \n",
              "\n",
              "         sales_rolling_median_7  sales_rolling_mean_7 store_city store_state  \\\n",
              "3059689                   73.29                 68.80  El Carmen      Manabi   \n",
              "3059690                   86.00                 87.29  El Carmen      Manabi   \n",
              "3059691                  595.18                655.63  El Carmen      Manabi   \n",
              "3059692                    0.00                  0.00  El Carmen      Manabi   \n",
              "3059693                    2.00                  3.00  El Carmen      Manabi   \n",
              "\n",
              "        store_type store_cluster holiday_locale  is_holiday  is_event  \\\n",
              "3059689          C             3          Local           1         0   \n",
              "3059690          C             3          Local           1         0   \n",
              "3059691          C             3          Local           1         0   \n",
              "3059692          C             3          Local           1         0   \n",
              "3059693          C             3          Local           1         0   \n",
              "\n",
              "         store_in_event_area  dcoilwtico  oil_lag_1  oil_lag_7  oil_lag_15  \\\n",
              "3059689                    0       47.57      47.59      49.07       50.21   \n",
              "3059690                    0       47.57      47.59      49.07       50.21   \n",
              "3059691                    0       47.57      47.59      49.07       50.21   \n",
              "3059692                    0       47.57      47.59      49.07       50.21   \n",
              "3059693                    0       47.57      47.59      49.07       50.21   \n",
              "\n",
              "         oil_lag_30  oil_lag_365  oil_rolling_mean_30  oil_rolling_std_30  \\\n",
              "3059689       46.53        45.72                48.27                1.47   \n",
              "3059690       46.53        45.72                48.27                1.47   \n",
              "3059691       46.53        45.72                48.27                1.47   \n",
              "3059692       46.53        45.72                48.27                1.47   \n",
              "3059693       46.53        45.72                48.27                1.47   \n",
              "\n",
              "         oil_rolling_mean_15  oil_rolling_std_15  oil_rolling_median_7  \\\n",
              "3059689                49.16                0.61                 48.81   \n",
              "3059690                49.16                0.61                 48.81   \n",
              "3059691                49.16                0.61                 48.81   \n",
              "3059692                49.16                0.61                 48.81   \n",
              "3059693                49.16                0.61                 48.81   \n",
              "\n",
              "         oil_rolling_mean_7  transactions  \n",
              "3059689               48.75        802.00  \n",
              "3059690               48.75        802.00  \n",
              "3059691               48.75        802.00  \n",
              "3059692               48.75        802.00  \n",
              "3059693               48.75        802.00  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c4aee554-6c90-4795-b138-bfc18418fe28\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>store_nbr</th>\n",
              "      <th>family</th>\n",
              "      <th>id</th>\n",
              "      <th>sales</th>\n",
              "      <th>onpromotion</th>\n",
              "      <th>year</th>\n",
              "      <th>month</th>\n",
              "      <th>day_of_week</th>\n",
              "      <th>sin_7</th>\n",
              "      <th>cos_7</th>\n",
              "      <th>sin_30.5</th>\n",
              "      <th>cos_30.5</th>\n",
              "      <th>sin_365.25</th>\n",
              "      <th>cos_365.25</th>\n",
              "      <th>is_payday</th>\n",
              "      <th>is_weekend</th>\n",
              "      <th>is_christmas</th>\n",
              "      <th>earthquake_effect</th>\n",
              "      <th>sales_lag_1</th>\n",
              "      <th>sales_lag_7</th>\n",
              "      <th>sales_lag_15</th>\n",
              "      <th>sales_lag_30</th>\n",
              "      <th>sales_lag_365</th>\n",
              "      <th>sales_rolling_mean_30</th>\n",
              "      <th>sales_rolling_std_30</th>\n",
              "      <th>sales_rolling_mean_15</th>\n",
              "      <th>sales_rolling_std_15</th>\n",
              "      <th>sales_rolling_median_7</th>\n",
              "      <th>sales_rolling_mean_7</th>\n",
              "      <th>store_city</th>\n",
              "      <th>store_state</th>\n",
              "      <th>store_type</th>\n",
              "      <th>store_cluster</th>\n",
              "      <th>holiday_locale</th>\n",
              "      <th>is_holiday</th>\n",
              "      <th>is_event</th>\n",
              "      <th>store_in_event_area</th>\n",
              "      <th>dcoilwtico</th>\n",
              "      <th>oil_lag_1</th>\n",
              "      <th>oil_lag_7</th>\n",
              "      <th>oil_lag_15</th>\n",
              "      <th>oil_lag_30</th>\n",
              "      <th>oil_lag_365</th>\n",
              "      <th>oil_rolling_mean_30</th>\n",
              "      <th>oil_rolling_std_30</th>\n",
              "      <th>oil_rolling_mean_15</th>\n",
              "      <th>oil_rolling_std_15</th>\n",
              "      <th>oil_rolling_median_7</th>\n",
              "      <th>oil_rolling_mean_7</th>\n",
              "      <th>transactions</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3059689</th>\n",
              "      <td>2017-08-15</td>\n",
              "      <td>54</td>\n",
              "      <td>POULTRY</td>\n",
              "      <td>3000751.00</td>\n",
              "      <td>59.62</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2017</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>0.43</td>\n",
              "      <td>-0.90</td>\n",
              "      <td>0.35</td>\n",
              "      <td>-0.94</td>\n",
              "      <td>-0.69</td>\n",
              "      <td>-0.72</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>56.16</td>\n",
              "      <td>73.29</td>\n",
              "      <td>96.94</td>\n",
              "      <td>103.90</td>\n",
              "      <td>20.11</td>\n",
              "      <td>69.01</td>\n",
              "      <td>20.17</td>\n",
              "      <td>68.62</td>\n",
              "      <td>17.39</td>\n",
              "      <td>73.29</td>\n",
              "      <td>68.80</td>\n",
              "      <td>El Carmen</td>\n",
              "      <td>Manabi</td>\n",
              "      <td>C</td>\n",
              "      <td>3</td>\n",
              "      <td>Local</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>47.57</td>\n",
              "      <td>47.59</td>\n",
              "      <td>49.07</td>\n",
              "      <td>50.21</td>\n",
              "      <td>46.53</td>\n",
              "      <td>45.72</td>\n",
              "      <td>48.27</td>\n",
              "      <td>1.47</td>\n",
              "      <td>49.16</td>\n",
              "      <td>0.61</td>\n",
              "      <td>48.81</td>\n",
              "      <td>48.75</td>\n",
              "      <td>802.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3059690</th>\n",
              "      <td>2017-08-15</td>\n",
              "      <td>54</td>\n",
              "      <td>PREPARED FOODS</td>\n",
              "      <td>3000752.00</td>\n",
              "      <td>94.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2017</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>0.43</td>\n",
              "      <td>-0.90</td>\n",
              "      <td>0.35</td>\n",
              "      <td>-0.94</td>\n",
              "      <td>-0.69</td>\n",
              "      <td>-0.72</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>147.00</td>\n",
              "      <td>86.00</td>\n",
              "      <td>100.00</td>\n",
              "      <td>50.00</td>\n",
              "      <td>114.00</td>\n",
              "      <td>88.77</td>\n",
              "      <td>29.43</td>\n",
              "      <td>89.73</td>\n",
              "      <td>27.99</td>\n",
              "      <td>86.00</td>\n",
              "      <td>87.29</td>\n",
              "      <td>El Carmen</td>\n",
              "      <td>Manabi</td>\n",
              "      <td>C</td>\n",
              "      <td>3</td>\n",
              "      <td>Local</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>47.57</td>\n",
              "      <td>47.59</td>\n",
              "      <td>49.07</td>\n",
              "      <td>50.21</td>\n",
              "      <td>46.53</td>\n",
              "      <td>45.72</td>\n",
              "      <td>48.27</td>\n",
              "      <td>1.47</td>\n",
              "      <td>49.16</td>\n",
              "      <td>0.61</td>\n",
              "      <td>48.81</td>\n",
              "      <td>48.75</td>\n",
              "      <td>802.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3059691</th>\n",
              "      <td>2017-08-15</td>\n",
              "      <td>54</td>\n",
              "      <td>PRODUCE</td>\n",
              "      <td>3000753.00</td>\n",
              "      <td>915.37</td>\n",
              "      <td>76.00</td>\n",
              "      <td>2017</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>0.43</td>\n",
              "      <td>-0.90</td>\n",
              "      <td>0.35</td>\n",
              "      <td>-0.94</td>\n",
              "      <td>-0.69</td>\n",
              "      <td>-0.72</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>585.62</td>\n",
              "      <td>790.01</td>\n",
              "      <td>633.39</td>\n",
              "      <td>981.71</td>\n",
              "      <td>598.23</td>\n",
              "      <td>687.86</td>\n",
              "      <td>161.92</td>\n",
              "      <td>671.31</td>\n",
              "      <td>174.08</td>\n",
              "      <td>595.18</td>\n",
              "      <td>655.63</td>\n",
              "      <td>El Carmen</td>\n",
              "      <td>Manabi</td>\n",
              "      <td>C</td>\n",
              "      <td>3</td>\n",
              "      <td>Local</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>47.57</td>\n",
              "      <td>47.59</td>\n",
              "      <td>49.07</td>\n",
              "      <td>50.21</td>\n",
              "      <td>46.53</td>\n",
              "      <td>45.72</td>\n",
              "      <td>48.27</td>\n",
              "      <td>1.47</td>\n",
              "      <td>49.16</td>\n",
              "      <td>0.61</td>\n",
              "      <td>48.81</td>\n",
              "      <td>48.75</td>\n",
              "      <td>802.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3059692</th>\n",
              "      <td>2017-08-15</td>\n",
              "      <td>54</td>\n",
              "      <td>SCHOOL AND OFFICE SUPPLIES</td>\n",
              "      <td>3000754.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2017</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>0.43</td>\n",
              "      <td>-0.90</td>\n",
              "      <td>0.35</td>\n",
              "      <td>-0.94</td>\n",
              "      <td>-0.69</td>\n",
              "      <td>-0.72</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>El Carmen</td>\n",
              "      <td>Manabi</td>\n",
              "      <td>C</td>\n",
              "      <td>3</td>\n",
              "      <td>Local</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>47.57</td>\n",
              "      <td>47.59</td>\n",
              "      <td>49.07</td>\n",
              "      <td>50.21</td>\n",
              "      <td>46.53</td>\n",
              "      <td>45.72</td>\n",
              "      <td>48.27</td>\n",
              "      <td>1.47</td>\n",
              "      <td>49.16</td>\n",
              "      <td>0.61</td>\n",
              "      <td>48.81</td>\n",
              "      <td>48.75</td>\n",
              "      <td>802.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3059693</th>\n",
              "      <td>2017-08-15</td>\n",
              "      <td>54</td>\n",
              "      <td>SEAFOOD</td>\n",
              "      <td>3000755.00</td>\n",
              "      <td>3.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2017</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>0.43</td>\n",
              "      <td>-0.90</td>\n",
              "      <td>0.35</td>\n",
              "      <td>-0.94</td>\n",
              "      <td>-0.69</td>\n",
              "      <td>-0.72</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>12.00</td>\n",
              "      <td>4.00</td>\n",
              "      <td>5.00</td>\n",
              "      <td>5.00</td>\n",
              "      <td>2.97</td>\n",
              "      <td>2.61</td>\n",
              "      <td>2.60</td>\n",
              "      <td>3.11</td>\n",
              "      <td>2.00</td>\n",
              "      <td>3.00</td>\n",
              "      <td>El Carmen</td>\n",
              "      <td>Manabi</td>\n",
              "      <td>C</td>\n",
              "      <td>3</td>\n",
              "      <td>Local</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>47.57</td>\n",
              "      <td>47.59</td>\n",
              "      <td>49.07</td>\n",
              "      <td>50.21</td>\n",
              "      <td>46.53</td>\n",
              "      <td>45.72</td>\n",
              "      <td>48.27</td>\n",
              "      <td>1.47</td>\n",
              "      <td>49.16</td>\n",
              "      <td>0.61</td>\n",
              "      <td>48.81</td>\n",
              "      <td>48.75</td>\n",
              "      <td>802.00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c4aee554-6c90-4795-b138-bfc18418fe28')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c4aee554-6c90-4795-b138-bfc18418fe28 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c4aee554-6c90-4795-b138-bfc18418fe28');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-3cfe58f9-0892-445a-8bbf-a0a61842d1f5\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3cfe58f9-0892-445a-8bbf-a0a61842d1f5')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-3cfe58f9-0892-445a-8bbf-a0a61842d1f5 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe"
            }
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predict"
      ],
      "metadata": {
        "id": "X-v3ONT6cHQc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test = make_data(df['test'], train=False)"
      ],
      "metadata": {
        "id": "mF0p_nPffpo7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "id": "GFxT-YonpuUm",
        "outputId": "95e6f8f1-d728-4dd0-a168-050584ebf3ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        id       date  store_nbr      family  onpromotion  year month  \\\n",
              "0  3000888 2017-08-16          1  AUTOMOTIVE            0  2017     8   \n",
              "1  3000889 2017-08-16          1   BABY CARE            0  2017     8   \n",
              "2  3000890 2017-08-16          1      BEAUTY            2  2017     8   \n",
              "3  3000891 2017-08-16          1   BEVERAGES           20  2017     8   \n",
              "4  3000892 2017-08-16          1       BOOKS            0  2017     8   \n",
              "\n",
              "  day_of_week  sin_7  cos_7  sin_30.5  cos_30.5  sin_365.25  cos_365.25  \\\n",
              "0           2  -0.43  -0.90      0.15     -0.99       -0.70       -0.71   \n",
              "1           2  -0.43  -0.90      0.15     -0.99       -0.70       -0.71   \n",
              "2           2  -0.43  -0.90      0.15     -0.99       -0.70       -0.71   \n",
              "3           2  -0.43  -0.90      0.15     -0.99       -0.70       -0.71   \n",
              "4           2  -0.43  -0.90      0.15     -0.99       -0.70       -0.71   \n",
              "\n",
              "   is_payday  is_weekend  is_christmas  earthquake_effect  sales_lag_1  \\\n",
              "0          0           0             0               0.00          NaN   \n",
              "1          0           0             0               0.00          NaN   \n",
              "2          0           0             0               0.00          NaN   \n",
              "3          0           0             0               0.00          NaN   \n",
              "4          0           0             0               0.00          NaN   \n",
              "\n",
              "   sales_lag_7  sales_lag_15  sales_lag_30  sales_lag_365  \\\n",
              "0          NaN           NaN           NaN            NaN   \n",
              "1          NaN           NaN           NaN            NaN   \n",
              "2          NaN           NaN           NaN            NaN   \n",
              "3          NaN           NaN           NaN            NaN   \n",
              "4          NaN           NaN           NaN            NaN   \n",
              "\n",
              "   sales_rolling_mean_30  sales_rolling_std_30  sales_rolling_mean_15  \\\n",
              "0                    NaN                   NaN                    NaN   \n",
              "1                    NaN                   NaN                    NaN   \n",
              "2                    NaN                   NaN                    NaN   \n",
              "3                    NaN                   NaN                    NaN   \n",
              "4                    NaN                   NaN                    NaN   \n",
              "\n",
              "   sales_rolling_std_15  sales_rolling_median_7  sales_rolling_mean_7  \\\n",
              "0                   NaN                     NaN                   NaN   \n",
              "1                   NaN                     NaN                   NaN   \n",
              "2                   NaN                     NaN                   NaN   \n",
              "3                   NaN                     NaN                   NaN   \n",
              "4                   NaN                     NaN                   NaN   \n",
              "\n",
              "  store_city store_state store_type store_cluster holiday_locale  is_holiday  \\\n",
              "0      Quito   Pichincha          D            13           NONE           0   \n",
              "1      Quito   Pichincha          D            13           NONE           0   \n",
              "2      Quito   Pichincha          D            13           NONE           0   \n",
              "3      Quito   Pichincha          D            13           NONE           0   \n",
              "4      Quito   Pichincha          D            13           NONE           0   \n",
              "\n",
              "   is_event  store_in_event_area  dcoilwtico  oil_lag_1  oil_lag_7  \\\n",
              "0         0                    0       46.57      47.57      49.59   \n",
              "1         0                    0       46.57      47.57      49.59   \n",
              "2         0                    0       46.57      47.57      49.59   \n",
              "3         0                    0       46.57      47.57      49.59   \n",
              "4         0                    0       46.57      47.57      49.59   \n",
              "\n",
              "   oil_lag_15  oil_lag_30  oil_lag_365  oil_rolling_mean_30  \\\n",
              "0       49.19       46.02        46.57                48.31   \n",
              "1       49.19       46.02        46.57                48.31   \n",
              "2       49.19       46.02        46.57                48.31   \n",
              "3       49.19       46.02        46.57                48.31   \n",
              "4       49.19       46.02        46.57                48.31   \n",
              "\n",
              "   oil_rolling_std_30  oil_rolling_mean_15  oil_rolling_std_15  \\\n",
              "0                1.44                48.98                0.67   \n",
              "1                1.44                48.98                0.67   \n",
              "2                1.44                48.98                0.67   \n",
              "3                1.44                48.98                0.67   \n",
              "4                1.44                48.98                0.67   \n",
              "\n",
              "   oil_rolling_median_7  oil_rolling_mean_7  transactions  \n",
              "0                 48.81               48.53       1648.00  \n",
              "1                 48.81               48.53       1648.00  \n",
              "2                 48.81               48.53       1648.00  \n",
              "3                 48.81               48.53       1648.00  \n",
              "4                 48.81               48.53       1648.00  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2f6cbdaa-21db-4c4e-9fba-2654cb279aca\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>date</th>\n",
              "      <th>store_nbr</th>\n",
              "      <th>family</th>\n",
              "      <th>onpromotion</th>\n",
              "      <th>year</th>\n",
              "      <th>month</th>\n",
              "      <th>day_of_week</th>\n",
              "      <th>sin_7</th>\n",
              "      <th>cos_7</th>\n",
              "      <th>sin_30.5</th>\n",
              "      <th>cos_30.5</th>\n",
              "      <th>sin_365.25</th>\n",
              "      <th>cos_365.25</th>\n",
              "      <th>is_payday</th>\n",
              "      <th>is_weekend</th>\n",
              "      <th>is_christmas</th>\n",
              "      <th>earthquake_effect</th>\n",
              "      <th>sales_lag_1</th>\n",
              "      <th>sales_lag_7</th>\n",
              "      <th>sales_lag_15</th>\n",
              "      <th>sales_lag_30</th>\n",
              "      <th>sales_lag_365</th>\n",
              "      <th>sales_rolling_mean_30</th>\n",
              "      <th>sales_rolling_std_30</th>\n",
              "      <th>sales_rolling_mean_15</th>\n",
              "      <th>sales_rolling_std_15</th>\n",
              "      <th>sales_rolling_median_7</th>\n",
              "      <th>sales_rolling_mean_7</th>\n",
              "      <th>store_city</th>\n",
              "      <th>store_state</th>\n",
              "      <th>store_type</th>\n",
              "      <th>store_cluster</th>\n",
              "      <th>holiday_locale</th>\n",
              "      <th>is_holiday</th>\n",
              "      <th>is_event</th>\n",
              "      <th>store_in_event_area</th>\n",
              "      <th>dcoilwtico</th>\n",
              "      <th>oil_lag_1</th>\n",
              "      <th>oil_lag_7</th>\n",
              "      <th>oil_lag_15</th>\n",
              "      <th>oil_lag_30</th>\n",
              "      <th>oil_lag_365</th>\n",
              "      <th>oil_rolling_mean_30</th>\n",
              "      <th>oil_rolling_std_30</th>\n",
              "      <th>oil_rolling_mean_15</th>\n",
              "      <th>oil_rolling_std_15</th>\n",
              "      <th>oil_rolling_median_7</th>\n",
              "      <th>oil_rolling_mean_7</th>\n",
              "      <th>transactions</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3000888</td>\n",
              "      <td>2017-08-16</td>\n",
              "      <td>1</td>\n",
              "      <td>AUTOMOTIVE</td>\n",
              "      <td>0</td>\n",
              "      <td>2017</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>-0.43</td>\n",
              "      <td>-0.90</td>\n",
              "      <td>0.15</td>\n",
              "      <td>-0.99</td>\n",
              "      <td>-0.70</td>\n",
              "      <td>-0.71</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Quito</td>\n",
              "      <td>Pichincha</td>\n",
              "      <td>D</td>\n",
              "      <td>13</td>\n",
              "      <td>NONE</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>46.57</td>\n",
              "      <td>47.57</td>\n",
              "      <td>49.59</td>\n",
              "      <td>49.19</td>\n",
              "      <td>46.02</td>\n",
              "      <td>46.57</td>\n",
              "      <td>48.31</td>\n",
              "      <td>1.44</td>\n",
              "      <td>48.98</td>\n",
              "      <td>0.67</td>\n",
              "      <td>48.81</td>\n",
              "      <td>48.53</td>\n",
              "      <td>1648.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3000889</td>\n",
              "      <td>2017-08-16</td>\n",
              "      <td>1</td>\n",
              "      <td>BABY CARE</td>\n",
              "      <td>0</td>\n",
              "      <td>2017</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>-0.43</td>\n",
              "      <td>-0.90</td>\n",
              "      <td>0.15</td>\n",
              "      <td>-0.99</td>\n",
              "      <td>-0.70</td>\n",
              "      <td>-0.71</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Quito</td>\n",
              "      <td>Pichincha</td>\n",
              "      <td>D</td>\n",
              "      <td>13</td>\n",
              "      <td>NONE</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>46.57</td>\n",
              "      <td>47.57</td>\n",
              "      <td>49.59</td>\n",
              "      <td>49.19</td>\n",
              "      <td>46.02</td>\n",
              "      <td>46.57</td>\n",
              "      <td>48.31</td>\n",
              "      <td>1.44</td>\n",
              "      <td>48.98</td>\n",
              "      <td>0.67</td>\n",
              "      <td>48.81</td>\n",
              "      <td>48.53</td>\n",
              "      <td>1648.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3000890</td>\n",
              "      <td>2017-08-16</td>\n",
              "      <td>1</td>\n",
              "      <td>BEAUTY</td>\n",
              "      <td>2</td>\n",
              "      <td>2017</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>-0.43</td>\n",
              "      <td>-0.90</td>\n",
              "      <td>0.15</td>\n",
              "      <td>-0.99</td>\n",
              "      <td>-0.70</td>\n",
              "      <td>-0.71</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Quito</td>\n",
              "      <td>Pichincha</td>\n",
              "      <td>D</td>\n",
              "      <td>13</td>\n",
              "      <td>NONE</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>46.57</td>\n",
              "      <td>47.57</td>\n",
              "      <td>49.59</td>\n",
              "      <td>49.19</td>\n",
              "      <td>46.02</td>\n",
              "      <td>46.57</td>\n",
              "      <td>48.31</td>\n",
              "      <td>1.44</td>\n",
              "      <td>48.98</td>\n",
              "      <td>0.67</td>\n",
              "      <td>48.81</td>\n",
              "      <td>48.53</td>\n",
              "      <td>1648.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3000891</td>\n",
              "      <td>2017-08-16</td>\n",
              "      <td>1</td>\n",
              "      <td>BEVERAGES</td>\n",
              "      <td>20</td>\n",
              "      <td>2017</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>-0.43</td>\n",
              "      <td>-0.90</td>\n",
              "      <td>0.15</td>\n",
              "      <td>-0.99</td>\n",
              "      <td>-0.70</td>\n",
              "      <td>-0.71</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Quito</td>\n",
              "      <td>Pichincha</td>\n",
              "      <td>D</td>\n",
              "      <td>13</td>\n",
              "      <td>NONE</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>46.57</td>\n",
              "      <td>47.57</td>\n",
              "      <td>49.59</td>\n",
              "      <td>49.19</td>\n",
              "      <td>46.02</td>\n",
              "      <td>46.57</td>\n",
              "      <td>48.31</td>\n",
              "      <td>1.44</td>\n",
              "      <td>48.98</td>\n",
              "      <td>0.67</td>\n",
              "      <td>48.81</td>\n",
              "      <td>48.53</td>\n",
              "      <td>1648.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3000892</td>\n",
              "      <td>2017-08-16</td>\n",
              "      <td>1</td>\n",
              "      <td>BOOKS</td>\n",
              "      <td>0</td>\n",
              "      <td>2017</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>-0.43</td>\n",
              "      <td>-0.90</td>\n",
              "      <td>0.15</td>\n",
              "      <td>-0.99</td>\n",
              "      <td>-0.70</td>\n",
              "      <td>-0.71</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Quito</td>\n",
              "      <td>Pichincha</td>\n",
              "      <td>D</td>\n",
              "      <td>13</td>\n",
              "      <td>NONE</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>46.57</td>\n",
              "      <td>47.57</td>\n",
              "      <td>49.59</td>\n",
              "      <td>49.19</td>\n",
              "      <td>46.02</td>\n",
              "      <td>46.57</td>\n",
              "      <td>48.31</td>\n",
              "      <td>1.44</td>\n",
              "      <td>48.98</td>\n",
              "      <td>0.67</td>\n",
              "      <td>48.81</td>\n",
              "      <td>48.53</td>\n",
              "      <td>1648.00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2f6cbdaa-21db-4c4e-9fba-2654cb279aca')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-2f6cbdaa-21db-4c4e-9fba-2654cb279aca button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-2f6cbdaa-21db-4c4e-9fba-2654cb279aca');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-861a01ac-894f-46eb-8482-22efe76d682a\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-861a01ac-894f-46eb-8482-22efe76d682a')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-861a01ac-894f-46eb-8482-22efe76d682a button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "test"
            }
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred_frame = catboost_models.predict(test, for_validation=False, verbose=False)"
      ],
      "metadata": {
        "id": "eTA0v3dHgtPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_frame.to_csv('submission.csv', index=False)"
      ],
      "metadata": {
        "id": "ONumVmzor_bL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# some experiments"
      ],
      "metadata": {
        "id": "PcZdEkzIsHhd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FamiliesModelsSystem:\n",
        "  def __init__(self, historic_data, cat_features, numeric_features, families, historic_period=30):\n",
        "    self.historic_period = historic_period\n",
        "    self.families = families\n",
        "    self.numeric_features = numeric_features\n",
        "    self.cat_features = cat_features\n",
        "    self.families_models = {}\n",
        "    self.historic_data = historic_data\n",
        "\n",
        "    for fam in self.families:\n",
        "      self.families_models[fam] = {\n",
        "          'historic_data': self.historic_data[self.historic_data['family'] == fam].copy(),\n",
        "          'model': None,\n",
        "          'input_func': None,\n",
        "          'output_func': None,\n",
        "          'predict_params': {},\n",
        "          'is_discrete': fam in discrete_sales_families,\n",
        "      }\n",
        "\n",
        "\n",
        "  def get_family(self, family):\n",
        "    return self.families_models[family]\n",
        "\n",
        "  def set_family(self, family, model=None, input_func=None, output_func=None, historic_data=None, predict_params={}):\n",
        "    if historic_data is not None:\n",
        "      self.families_models[family]['historic_data'] = historic_data\n",
        "\n",
        "    if model is not None:\n",
        "      self.families_models[family]['model'] = model\n",
        "\n",
        "    if predict_params is not None:\n",
        "      self.families_models[family]['predict_params'] = predict_params\n",
        "\n",
        "    if input_func is not None:\n",
        "      self.families_models[family]['input_func'] = input_func\n",
        "\n",
        "    if output_func is not None:\n",
        "      self.families_models[family]['output_func'] = output_func\n",
        "\n",
        "\n",
        "\n",
        "  def predict_family(self, data, family, for_validation=True, verbose=True):\n",
        "    fam = self.get_family(family)\n",
        "    historic_data = fam['historic_data'].copy()\n",
        "    model = fam['model']\n",
        "    input_func = fam['input_func']\n",
        "    output_func = fam['output_func']\n",
        "    predict_params = fam['predict_params']\n",
        "    is_discrete = fam['is_discrete']\n",
        "\n",
        "    history = {s: deque(maxlen=self.historic_period) for s in historic_data['store_nbr'].unique()}\n",
        "\n",
        "    for _, row in historic_data.iterrows():\n",
        "        history[row.store_nbr].append(row.sales)\n",
        "\n",
        "    family_data = data[data['family'] == family].sort_values(\"date\")\n",
        "    pred_frame = pd.DataFrame({\"id\": family_data[\"id\"], \"sales\": np.nan})\n",
        "\n",
        "    common_loss = 0\n",
        "    if verbose:\n",
        "      print(f\"\\nFAMILY {family}\\n\")\n",
        "\n",
        "    for date, daily_data in family_data.groupby(\"date\"):\n",
        "        features = []\n",
        "\n",
        "        for _, row in daily_data.iterrows():\n",
        "            key = row.store_nbr\n",
        "            hist = history[key]\n",
        "            arr = np.array(hist)\n",
        "\n",
        "            feats = {\n",
        "                \"sales_lag_1\": arr[-1] if len(arr) >= 1 else np.nan,\n",
        "                \"sales_lag_7\": arr[-7] if len(arr) >= 7 else np.nan,\n",
        "                \"sales_rolling_mean_30\": arr[-30:].mean() if len(arr) > 0 else np.nan,\n",
        "                \"sales_rolling_std_30\": arr[-30:].std() if len(arr) > 0 else np.nan,\n",
        "                \"sales_rolling_mean_15\": arr[-15:].mean() if len(arr) > 0 else np.nan,\n",
        "                \"sales_rolling_std_15\": arr[-15:].std() if len(arr) > 0 else np.nan,\n",
        "                \"sales_rolling_median_7\": np.median(arr[-7:]) if len(arr) > 0 else np.nan,\n",
        "                \"sales_rolling_mean_7\": arr[-7:].mean() if len(arr) > 0 else np.nan,\n",
        "            }\n",
        "            features.append(feats)\n",
        "\n",
        "        features_df = pd.DataFrame(features, index=daily_data.index)\n",
        "        predict_today = daily_data.copy()\n",
        "        predict_today.update(features_df)\n",
        "\n",
        "\n",
        "        X = input_func(predict_today)\n",
        "\n",
        "\n",
        "        y_pred = np.maximum(0, output_func(model.predict(X, **predict_params)))\n",
        "\n",
        "        if is_discrete:\n",
        "          y_pred = np.round(y_pred)\n",
        "\n",
        "        for (_, row), pred in zip(predict_today.iterrows(), y_pred):\n",
        "            history[row.store_nbr].append(pred)\n",
        "\n",
        "        if for_validation:\n",
        "          y_true = np.array(predict_today[\"sales\"])\n",
        "\n",
        "          loss = root_mean_squared_log_error(y_true, y_pred)\n",
        "          pred_frame.loc[predict_today.index, \"sales\"] = y_pred\n",
        "\n",
        "          if verbose:\n",
        "            print(f\"Loss on {date.date()}: {loss}\")\n",
        "          common_loss += loss\n",
        "\n",
        "\n",
        "    if for_validation:\n",
        "      common_loss /= family_data[\"date\"].nunique()\n",
        "      if verbose:\n",
        "        print(f\"\\nCommon Loss: {common_loss}\")\n",
        "\n",
        "      return (pred_frame, common_loss)\n",
        "\n",
        "    return pred_frame\n",
        "\n",
        "\n",
        "  def predict(self, data):\n",
        "    pred_frames = []\n",
        "    for family in self.families:\n",
        "      pred_frames.append(self.predict_family(data, family))\n",
        "    return pd.concat(pred_frames).sort_values(by='id').reset_index(drop=True)\n",
        "\n",
        "\n",
        "families_models = FamiliesModelsSystem(\n",
        "    train[(train['date'] >= end_of_train - pd.Timedelta(days=30)) & (train['date'] <= end_of_train)],\n",
        "    cat_features,\n",
        "    num_features,\n",
        "    all_families,\n",
        "    30\n",
        "    )"
      ],
      "metadata": {
        "id": "2lodluPEiie0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## AUTOMOTIVE"
      ],
      "metadata": {
        "id": "4fXXP23UsMMq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main"
      ],
      "metadata": {
        "id": "QZHY3IHKgttQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "automotive_train = get_family_data(train, 'AUTOMOTIVE')\n",
        "automotive_valid = get_family_data(valid, 'AUTOMOTIVE')"
      ],
      "metadata": {
        "id": "Qlg01zw6AHEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_input_func_2(df):\n",
        "  ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False, drop='first')\n",
        "  encoded_features = ohe.fit_transform(df[cat_features])\n",
        "  encoded_feature_names = ohe.get_feature_names_out(cat_features)\n",
        "  X = df[num_features]\n",
        "  cat_df = pd.DataFrame(encoded_features, columns=encoded_feature_names, index=X.index)\n",
        "  X = pd.concat([X, cat_df], axis=1).fillna(0)\n",
        "\n",
        "\n",
        "  corr_matrix = X.corr()\n",
        "  corr_pairs = corr_matrix.unstack()\n",
        "\n",
        "  corr_pairs = corr_pairs.drop_duplicates().dropna()\n",
        "  corr_pairs = corr_pairs[corr_pairs.index.get_level_values(0) != corr_pairs.index.get_level_values(1)]\n",
        "\n",
        "  corr_sorted = corr_pairs.abs().sort_values(ascending=False)\n",
        "\n",
        "  strong_corr = corr_pairs[corr_pairs.abs() > 0.8].sort_values(ascending=False)\n",
        "  corr_pairs = strong_corr.index.tolist()\n",
        "  linear_features = set(X.columns)\n",
        "  variances = X.var()\n",
        "\n",
        "\n",
        "  for feat1, feat2 in corr_pairs:\n",
        "    if feat1 in linear_features and feat2 in linear_features:\n",
        "      if variances[feat1] >= variances[feat2]:\n",
        "        linear_features.remove(feat2)\n",
        "      else:\n",
        "        linear_features.remove(feat1)\n",
        "\n",
        "\n",
        "  linear_features = list(linear_features)\n",
        "  X = X[linear_features]\n",
        "\n",
        "\n",
        "  linear_features = X.columns\n",
        "  linear_num_features = list(set(linear_features) - set(encoded_feature_names))\n",
        "\n",
        "  def input_func_2(df):\n",
        "    encoded_features = ohe.fit_transform(df[cat_features])\n",
        "    encoded_feature_names = ohe.get_feature_names_out(cat_features)\n",
        "    _df = df[linear_num_features]\n",
        "    cat_df = pd.DataFrame(encoded_features, columns=encoded_feature_names, index=df.index)\n",
        "    _df = pd.concat([_df, cat_df], axis=1).fillna(0)\n",
        "\n",
        "\n",
        "\n",
        "    X = pd.DataFrame()\n",
        "    for feat in linear_features:\n",
        "      if feat in _df.columns:\n",
        "        X[feat] = _df[feat]\n",
        "      else:\n",
        "        X[feat] = pd.Series(0, index=_df.index)\n",
        "\n",
        "    return X\n",
        "\n",
        "  return (input_func_2, linear_num_features, encoded_feature_names)"
      ],
      "metadata": {
        "id": "C2R5Mfl-_X5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def prepare_input_func_3(df):\n",
        "  num_df = df[num_features]\n",
        "  cat_df = df[cat_features]\n",
        "\n",
        "\n",
        "  corr_matrix = num_df.corr()\n",
        "  corr_pairs = corr_matrix.unstack()\n",
        "\n",
        "  corr_pairs = corr_pairs.drop_duplicates().dropna()\n",
        "  corr_pairs = corr_pairs[corr_pairs.index.get_level_values(0) != corr_pairs.index.get_level_values(1)]\n",
        "\n",
        "  corr_sorted = corr_pairs.abs().sort_values(ascending=False)\n",
        "\n",
        "  strong_corr = corr_pairs[corr_pairs.abs() > 0.8].sort_values(ascending=False)\n",
        "  corr_pairs = strong_corr.index.tolist()\n",
        "  linear_features = set(num_df.columns)\n",
        "  variances = num_df.var()\n",
        "\n",
        "\n",
        "  for feat1, feat2 in corr_pairs:\n",
        "    if feat1 in linear_features and feat2 in linear_features:\n",
        "      if variances[feat1] >= variances[feat2]:\n",
        "        linear_features.remove(feat2)\n",
        "      else:\n",
        "        linear_features.remove(feat1)\n",
        "\n",
        "\n",
        "  linear_features = list(linear_features)\n",
        "\n",
        "  def input_func_3(df):\n",
        "    return df[linear_features + cat_features]\n",
        "\n",
        "  return input_func_3\n"
      ],
      "metadata": {
        "id": "inJVGxYdFucK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_input_func_4(df):\n",
        "  ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False, drop='first')\n",
        "  encoded_features = ohe.fit_transform(df[cat_features])\n",
        "  encoded_feature_names = ohe.get_feature_names_out(cat_features)\n",
        "  X = df[num_features]\n",
        "  cat_df = pd.DataFrame(encoded_features, columns=encoded_feature_names, index=X.index)\n",
        "  X = pd.concat([X, cat_df], axis=1)\n",
        "  features = X.columns\n",
        "\n",
        "\n",
        "  def input_func_4(df):\n",
        "    encoded_features = ohe.fit_transform(df[cat_features])\n",
        "    encoded_feature_names = ohe.get_feature_names_out(cat_features)\n",
        "    _df = df[num_features].copy()\n",
        "    cat_df = pd.DataFrame(encoded_features, columns=encoded_feature_names, index=df.index)\n",
        "    _df = pd.concat([_df, cat_df], axis=1)\n",
        "\n",
        "\n",
        "\n",
        "    list_of_dfs = []\n",
        "    for feat in features:\n",
        "      if feat not in _df.columns:\n",
        "        _df[feat] = 0\n",
        "      list_of_dfs.append(_df[[feat]])\n",
        "    return pd.concat(list_of_dfs, axis=1)\n",
        "\n",
        "\n",
        "  return input_func_4"
      ],
      "metadata": {
        "id": "XRoPdmUGU408"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def input_func_1(df):\n",
        "  return df[num_features + cat_features] # simple preprocess (fits for catboost/lightgbm for example)\n",
        "input_func_2, linear_num_features, encoded_feature_names = prepare_input_func_2(automotive_train) # ohe + remove multicollinearity\n",
        "input_func_3 = prepare_input_func_3(automotive_train) # remove multicollinearity\n",
        "input_func_4 = prepare_input_func_4(automotive_train) # ohe\n",
        "\n",
        "def input_func_5(df):\n",
        "  return  torch.tensor(input_func_2(df).to_numpy(), dtype=torch.float32).to(device)"
      ],
      "metadata": {
        "id": "if13n7GK-Ou3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def output_func_1(y, df=None): # for working with y_log\n",
        "  return np.expm1(np.clip(y, -np.inf, 709))\n",
        "def output_func_2(y, df=None): # for gamma\n",
        "  return np.array(y) - 1\n",
        "\n",
        "def output_func_3(y, df=None): #just target\n",
        "  return np.array(y)"
      ],
      "metadata": {
        "id": "2fd34mzO-5qB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = np.array(automotive_train['sales'])\n",
        "y_log = np.log1p(automotive_train['sales'])\n",
        "y_tensor = torch.tensor(np.array(y),  dtype=torch.float32).to(device)\n",
        "X_1 = input_func_1(automotive_train)\n",
        "X_2 = input_func_2(automotive_train)\n",
        "X_3 = input_func_3(automotive_train)\n",
        "X_4 = input_func_4(automotive_train)\n",
        "X_5 = input_func_5(automotive_train)"
      ],
      "metadata": {
        "id": "UtPxfwhywMmX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(y)\n",
        "plt.show()\n",
        "plt.hist(y_log)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 843
        },
        "id": "sD9dx1Pr1DZz",
        "outputId": "a80ac804-3e1c-46ff-ddf0-444543a1d9b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gamma Regressor"
      ],
      "metadata": {
        "id": "jb1dvyrsBrWm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import GammaRegressor\n",
        "model = GammaRegressor(alpha=2.0, fit_intercept=True, max_iter =2000, verbose=500)\n",
        "model.fit(X_2, y + 1)\n",
        "families_models.set_family('AUTOMOTIVE', model=model, input_func=input_func_2, output_func=output_func_2)\n",
        "_ = families_models.predict_family(valid, 'AUTOMOTIVE', True, True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0XhScv0EBtPS",
        "outputId": "52670e85-c677-43fa-a0f7-073f1db12e42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "FAMILY AUTOMOTIVE\n",
            "\n",
            "Loss on 2017-08-01: 0.6204911596668764\n",
            "Loss on 2017-08-02: 0.5991919770224481\n",
            "Loss on 2017-08-03: 0.5580627611667507\n",
            "Loss on 2017-08-04: 0.6019643693988888\n",
            "Loss on 2017-08-05: 0.5843635895979518\n",
            "Loss on 2017-08-06: 0.6057707603502303\n",
            "Loss on 2017-08-07: 0.6034971411184565\n",
            "Loss on 2017-08-08: 0.5821938905476199\n",
            "Loss on 2017-08-09: 0.5887554829606841\n",
            "Loss on 2017-08-10: 0.5413358480219108\n",
            "Loss on 2017-08-11: 0.6247682705474564\n",
            "Loss on 2017-08-12: 0.6342916033866437\n",
            "Loss on 2017-08-13: 0.7671376172707637\n",
            "Loss on 2017-08-14: 0.6767066637029295\n",
            "Loss on 2017-08-15: 0.79192196601114\n",
            "\n",
            "Common Loss: 0.6253635400513834\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Catboost"
      ],
      "metadata": {
        "id": "zVroTSdTx1C6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RMSE"
      ],
      "metadata": {
        "id": "T_eobxVpEgcU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pbounds = {\n",
        "    'depth': (4, 10),\n",
        "    'border_count': (32, 255),\n",
        "    'iterations': (100, 1000),\n",
        "\n",
        "    'learning_rate': (0.01, 0.1),\n",
        "    'l2_leaf_reg': (1, 10),\n",
        "    'random_strength': (0.0, 8.0),\n",
        "    'bagging_temperature': (0.0, 1.0),\n",
        "}\n",
        "\n",
        "\n",
        "def objective(depth, border_count, iterations, learning_rate, l2_leaf_reg, random_strength,\n",
        "              bagging_temperature):\n",
        "\n",
        "\n",
        "  iterations= int(iterations)\n",
        "  depth = int(depth)\n",
        "  border_count = int(border_count)\n",
        "\n",
        "  model = CatBoostRegressor(\n",
        "        depth=depth,\n",
        "        border_count=border_count,\n",
        "        iterations=iterations,\n",
        "\n",
        "        learning_rate=learning_rate,\n",
        "        l2_leaf_reg=l2_leaf_reg,\n",
        "        random_strength=random_strength,\n",
        "        bagging_temperature=bagging_temperature,\n",
        "        loss_function='RMSE',\n",
        "        verbose=0,\n",
        "        task_type='GPU',\n",
        "    )\n",
        "\n",
        "  model.fit(train_pool)\n",
        "  print('\\ntrain score:', model.get_best_score()['learn'])\n",
        "\n",
        "  families_models.set_family('AUTOMOTIVE', model=model, input_func=input_func_1, output_func=output_func_1)\n",
        "  pred_frame, common_loss = families_models.predict_family(valid, 'AUTOMOTIVE', True, verbose=False)\n",
        "\n",
        "  return -common_loss #optimize rmsle metric\n",
        "\n",
        "\n",
        "optimizer = BayesianOptimization(\n",
        "    f=objective,\n",
        "    pbounds=pbounds,\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "\n",
        "optimizer.maximize(init_points=5, n_iter=20)\n",
        "\n",
        "best_params = optimizer.max['params']\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "076a5eed-6866-47af-be4a-73f802b0c971",
        "id": "riTJ0jt_AgDS"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|   iter    |  target   |   depth   | border... | iterat... | learni... | l2_lea... | random... | baggin... |\n",
            "-------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "train score: {'RMSE': 0.49048649602617406}\n",
            "| \u001b[39m1        \u001b[39m | \u001b[39m-0.510971\u001b[39m | \u001b[39m6.2472407\u001b[39m | \u001b[39m244.00929\u001b[39m | \u001b[39m758.79454\u001b[39m | \u001b[39m0.0638792\u001b[39m | \u001b[39m2.4041677\u001b[39m | \u001b[39m1.2479561\u001b[39m | \u001b[39m0.0580836\u001b[39m |\n",
            "\n",
            "train score: {'RMSE': 0.5083128141935075}\n",
            "| \u001b[39m2        \u001b[39m | \u001b[39m-0.517176\u001b[39m | \u001b[39m9.1970568\u001b[39m | \u001b[39m166.04864\u001b[39m | \u001b[39m737.26532\u001b[39m | \u001b[39m0.0118526\u001b[39m | \u001b[39m9.7291886\u001b[39m | \u001b[39m6.6595411\u001b[39m | \u001b[39m0.2123391\u001b[39m |\n",
            "\n",
            "train score: {'RMSE': 0.5077139570181322}\n",
            "| \u001b[39m3        \u001b[39m | \u001b[39m-0.512480\u001b[39m | \u001b[39m5.0909498\u001b[39m | \u001b[39m72.899205\u001b[39m | \u001b[39m373.81801\u001b[39m | \u001b[39m0.0572280\u001b[39m | \u001b[39m4.8875051\u001b[39m | \u001b[39m2.3298331\u001b[39m | \u001b[39m0.6118528\u001b[39m |\n",
            "\n",
            "train score: {'RMSE': 0.5101637168959184}\n",
            "| \u001b[39m4        \u001b[39m | \u001b[39m-0.517401\u001b[39m | \u001b[39m4.8369631\u001b[39m | \u001b[39m97.148256\u001b[39m | \u001b[39m429.72565\u001b[39m | \u001b[39m0.0510462\u001b[39m | \u001b[39m8.0665836\u001b[39m | \u001b[39m1.5973902\u001b[39m | \u001b[39m0.5142344\u001b[39m |\n",
            "\n",
            "train score: {'RMSE': 0.5056911712567718}\n",
            "| \u001b[39m5        \u001b[39m | \u001b[39m-0.519506\u001b[39m | \u001b[39m7.5544874\u001b[39m | \u001b[39m42.358442\u001b[39m | \u001b[39m646.79036\u001b[39m | \u001b[39m0.0253471\u001b[39m | \u001b[39m1.5854643\u001b[39m | \u001b[39m7.5910842\u001b[39m | \u001b[39m0.9656320\u001b[39m |\n",
            "\n",
            "train score: {'RMSE': 0.5123597076378779}\n",
            "| \u001b[39m6        \u001b[39m | \u001b[39m-0.518611\u001b[39m | \u001b[39m5.2137119\u001b[39m | \u001b[39m73.304199\u001b[39m | \u001b[39m373.93056\u001b[39m | \u001b[39m0.0301868\u001b[39m | \u001b[39m5.0240674\u001b[39m | \u001b[39m1.0697812\u001b[39m | \u001b[39m0.4098479\u001b[39m |\n",
            "\n",
            "train score: {'RMSE': 0.5005839304569792}\n",
            "| \u001b[39m7        \u001b[39m | \u001b[39m-0.512772\u001b[39m | \u001b[39m6.7164321\u001b[39m | \u001b[39m205.76170\u001b[39m | \u001b[39m755.05905\u001b[39m | \u001b[39m0.0445504\u001b[39m | \u001b[39m4.1170128\u001b[39m | \u001b[39m4.6918416\u001b[39m | \u001b[39m0.9808633\u001b[39m |\n",
            "\n",
            "train score: {'RMSE': 0.5009852821160704}\n",
            "| \u001b[35m8        \u001b[39m | \u001b[35m-0.508388\u001b[39m | \u001b[35m4.5345368\u001b[39m | \u001b[35m163.26095\u001b[39m | \u001b[35m853.08720\u001b[39m | \u001b[35m0.0997253\u001b[39m | \u001b[35m4.0160199\u001b[39m | \u001b[35m2.7685559\u001b[39m | \u001b[35m0.9490182\u001b[39m |\n",
            "\n",
            "train score: {'RMSE': 0.5090547384518951}\n",
            "| \u001b[39m9        \u001b[39m | \u001b[39m-0.516013\u001b[39m | \u001b[39m4.9947722\u001b[39m | \u001b[39m77.481696\u001b[39m | \u001b[39m817.87742\u001b[39m | \u001b[39m0.0329387\u001b[39m | \u001b[39m9.6946510\u001b[39m | \u001b[39m0.6936871\u001b[39m | \u001b[39m0.6762526\u001b[39m |\n",
            "\n",
            "train score: {'RMSE': 0.501134274932976}\n",
            "| \u001b[39m10       \u001b[39m | \u001b[39m-0.510139\u001b[39m | \u001b[39m4.5620768\u001b[39m | \u001b[39m248.39507\u001b[39m | \u001b[39m934.99781\u001b[39m | \u001b[39m0.0927606\u001b[39m | \u001b[39m6.9868763\u001b[39m | \u001b[39m5.5960033\u001b[39m | \u001b[39m0.8885432\u001b[39m |\n",
            "\n",
            "train score: {'RMSE': 0.47761255195942176}\n",
            "| \u001b[39m11       \u001b[39m | \u001b[39m-0.510222\u001b[39m | \u001b[39m7.2513442\u001b[39m | \u001b[39m109.62503\u001b[39m | \u001b[39m561.55171\u001b[39m | \u001b[39m0.0978254\u001b[39m | \u001b[39m1.5210555\u001b[39m | \u001b[39m2.5004192\u001b[39m | \u001b[39m0.5843103\u001b[39m |\n",
            "\n",
            "train score: {'RMSE': 0.4738632581452467}\n",
            "| \u001b[39m12       \u001b[39m | \u001b[39m-0.512750\u001b[39m | \u001b[39m8.0713333\u001b[39m | \u001b[39m168.05403\u001b[39m | \u001b[39m826.85314\u001b[39m | \u001b[39m0.0534927\u001b[39m | \u001b[39m3.7676547\u001b[39m | \u001b[39m7.9564239\u001b[39m | \u001b[39m0.3134158\u001b[39m |\n",
            "\n",
            "train score: {'RMSE': 0.46764085440559866}\n",
            "| \u001b[39m13       \u001b[39m | \u001b[39m-0.509545\u001b[39m | \u001b[39m9.4927359\u001b[39m | \u001b[39m60.848066\u001b[39m | \u001b[39m979.08399\u001b[39m | \u001b[39m0.0379925\u001b[39m | \u001b[39m4.9440100\u001b[39m | \u001b[39m4.0886791\u001b[39m | \u001b[39m0.2392471\u001b[39m |\n",
            "\n",
            "train score: {'RMSE': 0.46401265231143435}\n",
            "| \u001b[35m14       \u001b[39m | \u001b[35m-0.507519\u001b[39m | \u001b[35m9.9380892\u001b[39m | \u001b[35m227.39709\u001b[39m | \u001b[35m996.04215\u001b[39m | \u001b[35m0.0429257\u001b[39m | \u001b[35m8.3054692\u001b[39m | \u001b[35m1.2160092\u001b[39m | \u001b[35m0.9713499\u001b[39m |\n",
            "\n",
            "train score: {'RMSE': 0.49711823774950437}\n",
            "| \u001b[39m15       \u001b[39m | \u001b[39m-0.511870\u001b[39m | \u001b[39m7.5245980\u001b[39m | \u001b[39m121.60065\u001b[39m | \u001b[39m708.48934\u001b[39m | \u001b[39m0.0341482\u001b[39m | \u001b[39m3.2979447\u001b[39m | \u001b[39m3.1799639\u001b[39m | \u001b[39m0.3486651\u001b[39m |\n",
            "\n",
            "train score: {'RMSE': 0.476075259006216}\n",
            "| \u001b[39m16       \u001b[39m | \u001b[39m-0.508425\u001b[39m | \u001b[39m9.7413955\u001b[39m | \u001b[39m125.99341\u001b[39m | \u001b[39m549.16096\u001b[39m | \u001b[39m0.0623701\u001b[39m | \u001b[39m9.8020184\u001b[39m | \u001b[39m6.3308291\u001b[39m | \u001b[39m0.5551452\u001b[39m |\n",
            "\n",
            "train score: {'RMSE': 0.5068778229546232}\n",
            "| \u001b[39m17       \u001b[39m | \u001b[39m-0.522761\u001b[39m | \u001b[39m9.8659102\u001b[39m | \u001b[39m249.82953\u001b[39m | \u001b[39m317.51636\u001b[39m | \u001b[39m0.0319078\u001b[39m | \u001b[39m9.9718369\u001b[39m | \u001b[39m6.2727282\u001b[39m | \u001b[39m0.1132726\u001b[39m |\n",
            "\n",
            "train score: {'RMSE': 0.4571394422670154}\n",
            "| \u001b[39m18       \u001b[39m | \u001b[39m-0.517603\u001b[39m | \u001b[39m8.4567273\u001b[39m | \u001b[39m191.40048\u001b[39m | \u001b[39m669.15258\u001b[39m | \u001b[39m0.0978971\u001b[39m | \u001b[39m4.7462319\u001b[39m | \u001b[39m1.1737298\u001b[39m | \u001b[39m0.5888336\u001b[39m |\n",
            "\n",
            "train score: {'RMSE': 0.48597368477161085}\n",
            "| \u001b[39m19       \u001b[39m | \u001b[39m-0.508834\u001b[39m | \u001b[39m6.8002328\u001b[39m | \u001b[39m208.80362\u001b[39m | \u001b[39m856.53242\u001b[39m | \u001b[39m0.0846279\u001b[39m | \u001b[39m8.0907464\u001b[39m | \u001b[39m3.7308564\u001b[39m | \u001b[39m0.1620674\u001b[39m |\n",
            "\n",
            "train score: {'RMSE': 0.418372700278043}\n",
            "| \u001b[39m20       \u001b[39m | \u001b[39m-0.511026\u001b[39m | \u001b[39m9.0138614\u001b[39m | \u001b[39m44.650360\u001b[39m | \u001b[39m842.09340\u001b[39m | \u001b[39m0.0876681\u001b[39m | \u001b[39m1.9745626\u001b[39m | \u001b[39m7.3276652\u001b[39m | \u001b[39m0.0092846\u001b[39m |\n",
            "\n",
            "train score: {'RMSE': 0.5228856329049534}\n",
            "| \u001b[39m21       \u001b[39m | \u001b[39m-0.526838\u001b[39m | \u001b[39m7.5126907\u001b[39m | \u001b[39m110.53015\u001b[39m | \u001b[39m165.08330\u001b[39m | \u001b[39m0.0170102\u001b[39m | \u001b[39m4.5191930\u001b[39m | \u001b[39m1.3537148\u001b[39m | \u001b[39m0.1943711\u001b[39m |\n",
            "\n",
            "train score: {'RMSE': 0.4878708350734877}\n",
            "| \u001b[39m22       \u001b[39m | \u001b[39m-0.508117\u001b[39m | \u001b[39m7.7916104\u001b[39m | \u001b[39m228.71650\u001b[39m | \u001b[39m728.83031\u001b[39m | \u001b[39m0.0530393\u001b[39m | \u001b[39m4.8715510\u001b[39m | \u001b[39m0.2840120\u001b[39m | \u001b[39m0.1092624\u001b[39m |\n",
            "\n",
            "train score: {'RMSE': 0.49782013660299956}\n",
            "| \u001b[39m23       \u001b[39m | \u001b[39m-0.510854\u001b[39m | \u001b[39m5.6800627\u001b[39m | \u001b[39m215.89345\u001b[39m | \u001b[39m640.84029\u001b[39m | \u001b[39m0.0777957\u001b[39m | \u001b[39m3.0358242\u001b[39m | \u001b[39m5.0284314\u001b[39m | \u001b[39m0.1494642\u001b[39m |\n",
            "\n",
            "train score: {'RMSE': 0.49385047551303124}\n",
            "| \u001b[39m24       \u001b[39m | \u001b[39m-0.508784\u001b[39m | \u001b[39m7.3824453\u001b[39m | \u001b[39m124.38673\u001b[39m | \u001b[39m661.02899\u001b[39m | \u001b[39m0.0433901\u001b[39m | \u001b[39m1.3988657\u001b[39m | \u001b[39m2.2056653\u001b[39m | \u001b[39m0.7866360\u001b[39m |\n",
            "\n",
            "train score: {'RMSE': 0.5047487584757631}\n",
            "| \u001b[39m25       \u001b[39m | \u001b[39m-0.509781\u001b[39m | \u001b[39m5.5167961\u001b[39m | \u001b[39m107.65836\u001b[39m | \u001b[39m419.07387\u001b[39m | \u001b[39m0.0706188\u001b[39m | \u001b[39m5.8552362\u001b[39m | \u001b[39m0.9783119\u001b[39m | \u001b[39m0.6980347\u001b[39m |\n",
            "=============================================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Huber"
      ],
      "metadata": {
        "id": "HQdU8UlnEOlf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_pool = Pool(X_1, y_log, cat_features)"
      ],
      "metadata": {
        "id": "cWevjctyAji3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mad = np.median(np.abs(y - np.median(y)))\n",
        "sigma_hat = 1.4826 * mad"
      ],
      "metadata": {
        "id": "bFvZmpwj3Jhm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CatBoost with Huber loss is prone to overfitting"
      ],
      "metadata": {
        "id": "AfhJf3F15yGb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pbounds = {\n",
        "    'depth': (2, 4),\n",
        "    'border_count': (32, 64),\n",
        "    'iterations': (100, 500),\n",
        "\n",
        "\n",
        "    'lr_log': (-2.0, -1.0),\n",
        "    'l2_leaf_reg': (1000, 600),\n",
        "    'bagging_temperature': (0.0, 5.0),\n",
        "    'random_strength': (2.0, 5.0),\n",
        "\n",
        "\n",
        "    'alpha': (1, 2),\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "def objective(depth, border_count, iterations, lr_log, l2_leaf_reg,\n",
        "              bagging_temperature, random_strength, alpha):\n",
        "\n",
        "\n",
        "  depth = int(depth)\n",
        "  border_count = int(border_count)\n",
        "  iterations = int(iterations)\n",
        "  learning_rate = 10 ** lr_log\n",
        "\n",
        "  model = CatBoostRegressor(\n",
        "        depth=depth,\n",
        "        iterations=iterations,\n",
        "        learning_rate=learning_rate,\n",
        "        l2_leaf_reg=l2_leaf_reg,\n",
        "        bagging_temperature=bagging_temperature,\n",
        "        border_count=border_count,\n",
        "        random_strength=random_strength,\n",
        "        loss_function=f'Huber:delta={alpha * sigma_hat}',\n",
        "        verbose=0,\n",
        "        task_type='GPU',\n",
        "    )\n",
        "\n",
        "  model.fit(train_pool)\n",
        "  print(\"\\nBest train score:\", model.get_best_score()['learn'])\n",
        "  families_models.set_family('AUTOMOTIVE', model=model, input_func=input_func_1, output_func=output_func_1, predict_params={})\n",
        "  _, common_loss = families_models.predict_family(valid, 'AUTOMOTIVE', True, verbose=False)\n",
        "  # best_valid_score = float('inf')\n",
        "  # best_iter = None\n",
        "\n",
        "  # for i in range(50, model.tree_count_, 50):\n",
        "  #   families_models.set_family('AUTOMOTIVE', model=model, input_func=input_func_1, output_func=output_func_1, predict_params={'ntree_end': i})\n",
        "  #   pred_frame, common_loss = families_models.predict_family(valid, 'AUTOMOTIVE', True, verbose=False)\n",
        "  #   if common_loss < best_valid_score:\n",
        "  #     best_valid_score = common_loss\n",
        "  #     best_iter = i\n",
        "\n",
        "  # print('Best iter', best_iter)\n",
        "  return -common_loss #optimize rmsle metric\n",
        "\n",
        "\n",
        "optimizer = BayesianOptimization(\n",
        "    f=objective,\n",
        "    pbounds=pbounds,\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "\n",
        "optimizer.maximize(init_points=5, n_iter=20)\n",
        "\n",
        "best_params = optimizer.max['params']\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 807
        },
        "id": "07peA-JSohIR",
        "outputId": "b6a39926-a324-4785-fe62-9a617d6eb510"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|   iter    |  target   |   depth   | border... | iterat... | learni... | l2_lea... | baggin... | random... |   alpha   |\n",
            "-------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Best train score: {'Huber:delta=20.453334783533272': 0.2750424082370414}\n",
            "| \u001b[39m1        \u001b[39m | \u001b[39m-0.888799\u001b[39m | \u001b[39m2.3745401\u001b[39m | \u001b[39m62.422857\u001b[39m | \u001b[39m39.279757\u001b[39m | \u001b[39m0.0339463\u001b[39m | \u001b[39m120.20838\u001b[39m | \u001b[39m2.4679835\u001b[39m | \u001b[39m2.1742508\u001b[39m | \u001b[39m4.5985284\u001b[39m |\n",
            "\n",
            "Best train score: {'Huber:delta=11.34283973677831': 0.7476955556099578}\n",
            "| \u001b[39m2        \u001b[39m | \u001b[39m-1.402403\u001b[39m | \u001b[39m2.6011150\u001b[39m | \u001b[39m54.658322\u001b[39m | \u001b[39m10.823379\u001b[39m | \u001b[39m0.0487963\u001b[39m | \u001b[39m424.59918\u001b[39m | \u001b[39m2.6370173\u001b[39m | \u001b[39m2.5454749\u001b[39m | \u001b[39m2.5502135\u001b[39m |\n",
            "\n",
            "Best train score: {'Huber:delta=13.784112619805045': 0.6541253379738434}\n",
            "| \u001b[39m3        \u001b[39m | \u001b[39m-1.356872\u001b[39m | \u001b[39m2.3042422\u001b[39m | \u001b[39m48.792205\u001b[39m | \u001b[39m27.277800\u001b[39m | \u001b[39m0.0216491\u001b[39m | \u001b[39m325.33380\u001b[39m | \u001b[39m2.4184815\u001b[39m | \u001b[39m2.8764339\u001b[39m | \u001b[39m3.0990855\u001b[39m |\n",
            "\n",
            "Best train score: {'Huber:delta=11.170971592009007': 0.7191743373808591}\n",
            "| \u001b[39m4        \u001b[39m | \u001b[39m-1.375798\u001b[39m | \u001b[39m2.4560699\u001b[39m | \u001b[39m57.125630\u001b[39m | \u001b[39m17.986951\u001b[39m | \u001b[39m0.0305693\u001b[39m | \u001b[39m316.58655\u001b[39m | \u001b[39m2.1393512\u001b[39m | \u001b[39m3.8226345\u001b[39m | \u001b[39m2.5115723\u001b[39m |\n",
            "\n",
            "Best train score: {'Huber:delta=14.768730784964996': 0.1853474993335727}\n",
            "| \u001b[35m5        \u001b[39m | \u001b[35m-0.706955\u001b[39m | \u001b[35m2.0650515\u001b[39m | \u001b[35m62.364337\u001b[39m | \u001b[35m48.625281\u001b[39m | \u001b[35m0.0423358\u001b[39m | \u001b[35m187.07619\u001b[39m | \u001b[35m2.2930163\u001b[39m | \u001b[35m4.0526990\u001b[39m | \u001b[35m3.3204574\u001b[39m |\n",
            "\n",
            "Best train score: {'Huber:delta=15.75703808213413': 0.18402155127274014}\n",
            "| \u001b[35m6        \u001b[39m | \u001b[35m-0.699709\u001b[39m | \u001b[35m2.2772760\u001b[39m | \u001b[35m61.927678\u001b[39m | \u001b[35m47.824897\u001b[39m | \u001b[35m0.0433370\u001b[39m | \u001b[35m187.33377\u001b[39m | \u001b[35m2.5559914\u001b[39m | \u001b[35m4.0984199\u001b[39m | \u001b[35m3.5426588\u001b[39m |\n",
            "\n",
            "Best train score: {'Huber:delta=8.8956': 1.4139060935935936}\n",
            "| \u001b[39m7        \u001b[39m | \u001b[39m-2.027387\u001b[39m | \u001b[39m2.0      \u001b[39m | \u001b[39m32.0     \u001b[39m | \u001b[39m10.0     \u001b[39m | \u001b[39m0.01     \u001b[39m | \u001b[39m175.36308\u001b[39m | \u001b[39m2.0      \u001b[39m | \u001b[39m2.0      \u001b[39m | \u001b[39m2.0      \u001b[39m |\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/bayes_opt/bayesian_optimization.py\u001b[0m in \u001b[0;36mmaximize\u001b[0;34m(self, init_points, n_iter)\u001b[0m\n\u001b[1;32m    317\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m                 \u001b[0mx_probe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopleft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: pop from an empty deque",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-183861612.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_points\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0mbest_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/bayes_opt/bayesian_optimization.py\u001b[0m in \u001b[0;36mmaximize\u001b[0;34m(self, init_points, n_iter)\u001b[0m\n\u001b[1;32m    318\u001b[0m                 \u001b[0mx_probe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopleft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                 \u001b[0mx_probe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuggest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m                 \u001b[0miteration\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_probe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlazy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/bayes_opt/bayesian_optimization.py\u001b[0m in \u001b[0;36msuggest\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0;31m# Finding argmax of the acquisition function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m         suggestion = self._acquisition_function.suggest(\n\u001b[0m\u001b[1;32m    269\u001b[0m             \u001b[0mgp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_space\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_gp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_random_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/bayes_opt/acquisition.py\u001b[0m in \u001b[0;36msuggest\u001b[0;34m(self, gp, target_space, n_random, n_smart, fit_gp, random_state)\u001b[0m\n\u001b[1;32m    528\u001b[0m             )\n\u001b[1;32m    529\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mConstraintNotSupportedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m         x_max = super().suggest(\n\u001b[0m\u001b[1;32m    531\u001b[0m             \u001b[0mgp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m             \u001b[0mtarget_space\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_space\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/bayes_opt/acquisition.py\u001b[0m in \u001b[0;36msuggest\u001b[0;34m(self, gp, target_space, n_random, n_smart, fit_gp, random_state)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0macq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_acq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_acq_min\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_random\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_random\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_smart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_smart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     def _get_acq(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/bayes_opt/acquisition.py\u001b[0m in \u001b[0;36m_acq_min\u001b[0;34m(self, acq, space, random_state, n_random, n_smart)\u001b[0m\n\u001b[1;32m    266\u001b[0m         )\n\u001b[1;32m    267\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_smart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m             \u001b[0mx_min_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_acq_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_smart_minimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_seeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_seeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m             \u001b[0;31m# Either n_random or n_smart is not 0 => at least one of x_min_r and x_min_s is not None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmin_acq_r\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmin_acq_s\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/bayes_opt/acquisition.py\u001b[0m in \u001b[0;36m_smart_minimize\u001b[0;34m(self, acq, space, x_seeds, random_state)\u001b[0m\n\u001b[1;32m    364\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontinuous_dimensions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mx_try\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx_seeds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 366\u001b[0;31m                 \u001b[0mres\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptimizeResult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_try\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontinuous_bounds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"L-BFGS-B\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    367\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/optimize/_minimize.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    782\u001b[0m                                  **options)\n\u001b[1;32m    783\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'l-bfgs-b'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 784\u001b[0;31m         res = _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[0m\u001b[1;32m    785\u001b[0m                                callback=callback, **options)\n\u001b[1;32m    786\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tnc'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/optimize/_lbfgsb_py.py\u001b[0m in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, workers, **unknown_options)\u001b[0m\n\u001b[1;32m    467\u001b[0m             \u001b[0;31m# until the completion of the current minimization iteration.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m             \u001b[0;31m# Overwrite f and g:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    470\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m             \u001b[0;31m# new iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36mfun_and_grad\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_x\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 404\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    405\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36m_update_grad\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    364\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_orig_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mFD_METHODS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 366\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    367\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg_updated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x, f0, **kwds)\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matleast_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mFD_METHODS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             g, dct = approx_derivative(\n\u001b[0m\u001b[1;32m     42\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                 \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/optimize/_numdiff.py\u001b[0m in \u001b[0;36mapprox_derivative\u001b[0;34m(fun, x0, method, rel_step, abs_step, f0, bounds, sparsity, as_linear_operator, args, kwargs, full_output, workers)\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mMapWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msparsity\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m                 J, _nfev = _dense_difference(fun_wrapped, x0, f0, h,\n\u001b[0m\u001b[1;32m    594\u001b[0m                                          \u001b[0muse_one_sided\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m                                          mf)\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/optimize/_numdiff.py\u001b[0m in \u001b[0;36m_dense_difference\u001b[0;34m(fun, x0, f0, h, use_one_sided, method, workers)\u001b[0m\n\u001b[1;32m    684\u001b[0m         \u001b[0mf_evals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_generator2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m         \u001b[0mdx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mf_eval\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mf0\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf_eval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf_evals\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m         \u001b[0mdf_dx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdelf\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mdelx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdelf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m         \u001b[0mnfev\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_dx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/optimize/_numdiff.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    877\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 879\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matleast_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    880\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m             raise RuntimeError(\"`fun` return value has \"\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/_lib/_util.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    588\u001b[0m         \u001b[0;31m# Send a copy because the user may overwrite it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m         \u001b[0;31m# The user of this class might want `x` to remain unchanged.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m         \u001b[0mfx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnfev\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/bayes_opt/acquisition.py\u001b[0m in \u001b[0;36macq\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     \u001b[0mmean\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mNDArray\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mFloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                     \u001b[0mstd\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mNDArray\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mFloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_std\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_acq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/gaussian_process/_gpr.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X, return_std, return_cov)\u001b[0m\n\u001b[1;32m    438\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Predict based on GP posterior\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m             \u001b[0;31m# Alg 2.1, page 19, line 4 -> f*_bar = K(X_test, X_train) . alpha\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m             \u001b[0mK_trans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_train_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m             \u001b[0my_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK_trans\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pbounds = {\n",
        "    'depth': (4, 10),\n",
        "    'border_count': (32, 255),\n",
        "\n",
        "\n",
        "    'learning_rate': (0.03, 0.1),\n",
        "    'l2_leaf_reg': (1, 10),\n",
        "    'bagging_temperature': (0.0, 1.0),\n",
        "\n",
        "\n",
        "    'alpha': (0.5, 2.0),\n",
        "}\n",
        "\n",
        "\n",
        "mad = np.median(np.abs(y - np.median(y)))\n",
        "sigma_hat = 1.4826 * mad\n",
        "\n",
        "\n",
        "def objective(depth, border_count, learning_rate, l2_leaf_reg,\n",
        "              bagging_temperature, alpha):\n",
        "\n",
        "\n",
        "  depth = int(depth)\n",
        "  border_count = int(border_count)\n",
        "\n",
        "  model = CatBoostRegressor(\n",
        "        depth=depth,\n",
        "        iterations=2000,\n",
        "        learning_rate=learning_rate,\n",
        "        l2_leaf_reg=l2_leaf_reg,\n",
        "        bagging_temperature=bagging_temperature,\n",
        "        border_count=border_count,\n",
        "        loss_function=f'Huber:delta={alpha * sigma_hat}',\n",
        "        verbose=0,\n",
        "        task_type='GPU',\n",
        "    )\n",
        "\n",
        "  model.fit(train_pool, early_stopping_rounds=50)\n",
        "\n",
        "  families_models.set_family('AUTOMOTIVE', model=model)\n",
        "  # print('\\nPARAMS:')\n",
        "  # print(families_models.get_family('AUTOMOTIVE')['model'].get_params())\n",
        "  pred_frame, common_loss = families_models.predict_family(valid, 'AUTOMOTIVE', True, verbose=False)\n",
        "\n",
        "  return -common_loss #optimize rmsle metric\n",
        "\n",
        "\n",
        "optimizer = BayesianOptimization(\n",
        "    f=objective,\n",
        "    pbounds=pbounds,\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "\n",
        "optimizer.maximize(init_points=5, n_iter=20)\n",
        "\n",
        "best_params = optimizer.max['params']\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fee281b-e1a5-4741-9847-009c016c7614",
        "id": "OY1YaGvIDP82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|   iter    |  target   |   depth   | border... | learni... | l2_lea... | baggin... |   alpha   |\n",
            "-------------------------------------------------------------------------------------------------\n",
            "| \u001b[39m1        \u001b[39m | \u001b[39m-0.508439\u001b[39m | \u001b[39m6.2472407\u001b[39m | \u001b[39m244.00929\u001b[39m | \u001b[39m0.0812395\u001b[39m | \u001b[39m6.3879263\u001b[39m | \u001b[39m0.1560186\u001b[39m | \u001b[39m0.7339917\u001b[39m |\n",
            "| \u001b[39m2        \u001b[39m | \u001b[39m-0.514879\u001b[39m | \u001b[39m4.3485016\u001b[39m | \u001b[39m225.15728\u001b[39m | \u001b[39m0.0720780\u001b[39m | \u001b[39m7.3726532\u001b[39m | \u001b[39m0.0205844\u001b[39m | \u001b[39m1.9548647\u001b[39m |\n",
            "| \u001b[39m3        \u001b[39m | \u001b[39m-0.508864\u001b[39m | \u001b[39m8.9946558\u001b[39m | \u001b[39m79.351621\u001b[39m | \u001b[39m0.0427277\u001b[39m | \u001b[39m2.6506405\u001b[39m | \u001b[39m0.3042422\u001b[39m | \u001b[39m1.2871346\u001b[39m |\n",
            "| \u001b[35m4        \u001b[39m | \u001b[35m-0.507627\u001b[39m | \u001b[35m6.5916701\u001b[39m | \u001b[35m96.944098\u001b[39m | \u001b[35m0.0728297\u001b[39m | \u001b[35m2.2554447\u001b[39m | \u001b[35m0.2921446\u001b[39m | \u001b[35m1.0495427\u001b[39m |\n",
            "| \u001b[39m5        \u001b[39m | \u001b[39m-0.515701\u001b[39m | \u001b[39m6.7364199\u001b[39m | \u001b[39m207.09423\u001b[39m | \u001b[39m0.0439771\u001b[39m | \u001b[39m5.6281099\u001b[39m | \u001b[39m0.5924145\u001b[39m | \u001b[39m0.5696756\u001b[39m |\n",
            "| \u001b[39m6        \u001b[39m | \u001b[39m-0.520521\u001b[39m | \u001b[39m9.1130890\u001b[39m | \u001b[39m121.62341\u001b[39m | \u001b[39m0.0637458\u001b[39m | \u001b[39m9.9336292\u001b[39m | \u001b[39m0.4956881\u001b[39m | \u001b[39m1.4157312\u001b[39m |\n",
            "| \u001b[39m7        \u001b[39m | \u001b[39m-0.509282\u001b[39m | \u001b[39m4.0      \u001b[39m | \u001b[39m88.969716\u001b[39m | \u001b[39m0.1      \u001b[39m | \u001b[39m10.0     \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m2.0      \u001b[39m |\n",
            "| \u001b[35m8        \u001b[39m | \u001b[35m-0.506165\u001b[39m | \u001b[35m5.0983817\u001b[39m | \u001b[35m97.085529\u001b[39m | \u001b[35m0.0781746\u001b[39m | \u001b[35m3.5559340\u001b[39m | \u001b[35m0.6756173\u001b[39m | \u001b[35m0.9930018\u001b[39m |\n",
            "| \u001b[35m9        \u001b[39m | \u001b[35m-0.505803\u001b[39m | \u001b[35m4.0      \u001b[39m | \u001b[35m101.86873\u001b[39m | \u001b[35m0.1      \u001b[39m | \u001b[35m10.0     \u001b[39m | \u001b[35m1.0      \u001b[39m | \u001b[35m0.5      \u001b[39m |\n",
            "| \u001b[39m10       \u001b[39m | \u001b[39m-0.516893\u001b[39m | \u001b[39m9.4116089\u001b[39m | \u001b[39m60.695668\u001b[39m | \u001b[39m0.0577070\u001b[39m | \u001b[39m1.8788162\u001b[39m | \u001b[39m0.2321115\u001b[39m | \u001b[39m0.7443636\u001b[39m |\n",
            "| \u001b[39m11       \u001b[39m | \u001b[39m-0.513797\u001b[39m | \u001b[39m10.0     \u001b[39m | \u001b[39m255.0    \u001b[39m | \u001b[39m0.03     \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m2.0      \u001b[39m |\n",
            "| \u001b[35m12       \u001b[39m | \u001b[35m-0.505216\u001b[39m | \u001b[35m4.0      \u001b[39m | \u001b[35m105.37284\u001b[39m | \u001b[35m0.0490671\u001b[39m | \u001b[35m1.0      \u001b[39m | \u001b[35m1.0      \u001b[39m | \u001b[35m0.5      \u001b[39m |\n",
            "| \u001b[39m13       \u001b[39m | \u001b[39m-0.511871\u001b[39m | \u001b[39m4.5687302\u001b[39m | \u001b[39m167.01604\u001b[39m | \u001b[39m0.0792067\u001b[39m | \u001b[39m1.6222694\u001b[39m | \u001b[39m0.1231127\u001b[39m | \u001b[39m1.4587353\u001b[39m |\n",
            "| \u001b[39m14       \u001b[39m | \u001b[39m-0.508983\u001b[39m | \u001b[39m7.0622111\u001b[39m | \u001b[39m32.088025\u001b[39m | \u001b[39m0.0688927\u001b[39m | \u001b[39m9.8005736\u001b[39m | \u001b[39m0.4818493\u001b[39m | \u001b[39m1.7873619\u001b[39m |\n",
            "| \u001b[39m15       \u001b[39m | \u001b[39m-0.517398\u001b[39m | \u001b[39m9.6379671\u001b[39m | \u001b[39m150.82690\u001b[39m | \u001b[39m0.0765510\u001b[39m | \u001b[39m9.1622554\u001b[39m | \u001b[39m0.2228705\u001b[39m | \u001b[39m1.9034811\u001b[39m |\n",
            "| \u001b[39m16       \u001b[39m | \u001b[39m-0.515615\u001b[39m | \u001b[39m10.0     \u001b[39m | \u001b[39m182.67315\u001b[39m | \u001b[39m0.0935134\u001b[39m | \u001b[39m10.0     \u001b[39m | \u001b[39m0.8387525\u001b[39m | \u001b[39m1.4474207\u001b[39m |\n",
            "| \u001b[39m17       \u001b[39m | \u001b[39m-0.509528\u001b[39m | \u001b[39m4.0      \u001b[39m | \u001b[39m38.489921\u001b[39m | \u001b[39m0.03     \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m0.5      \u001b[39m |\n",
            "| \u001b[39m18       \u001b[39m | \u001b[39m-0.517172\u001b[39m | \u001b[39m10.0     \u001b[39m | \u001b[39m104.06951\u001b[39m | \u001b[39m0.03     \u001b[39m | \u001b[39m6.1186779\u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m2.0      \u001b[39m |\n",
            "| \u001b[35m19       \u001b[39m | \u001b[35m-0.504799\u001b[39m | \u001b[35m4.0      \u001b[39m | \u001b[35m96.668152\u001b[39m | \u001b[35m0.1      \u001b[39m | \u001b[35m9.6344805\u001b[39m | \u001b[35m0.0      \u001b[39m | \u001b[35m0.5      \u001b[39m |\n",
            "| \u001b[39m20       \u001b[39m | \u001b[39m-0.509144\u001b[39m | \u001b[39m4.0      \u001b[39m | \u001b[39m89.196844\u001b[39m | \u001b[39m0.03     \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m0.0      \u001b[39m | \u001b[39m0.5      \u001b[39m |\n",
            "| \u001b[39m21       \u001b[39m | \u001b[39m-0.518770\u001b[39m | \u001b[39m10.0     \u001b[39m | \u001b[39m42.438150\u001b[39m | \u001b[39m0.03     \u001b[39m | \u001b[39m10.0     \u001b[39m | \u001b[39m0.0      \u001b[39m | \u001b[39m2.0      \u001b[39m |\n",
            "| \u001b[39m22       \u001b[39m | \u001b[39m-0.527592\u001b[39m | \u001b[39m10.0     \u001b[39m | \u001b[39m238.26159\u001b[39m | \u001b[39m0.03     \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m0.5      \u001b[39m |\n",
            "| \u001b[39m23       \u001b[39m | \u001b[39m-0.519106\u001b[39m | \u001b[39m9.1266147\u001b[39m | \u001b[39m93.343269\u001b[39m | \u001b[39m0.1      \u001b[39m | \u001b[39m8.0907890\u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m0.5      \u001b[39m |\n",
            "| \u001b[39m24       \u001b[39m | \u001b[39m-0.504889\u001b[39m | \u001b[39m4.0      \u001b[39m | \u001b[39m100.36340\u001b[39m | \u001b[39m0.1      \u001b[39m | \u001b[39m6.0849814\u001b[39m | \u001b[39m0.0      \u001b[39m | \u001b[39m0.5      \u001b[39m |\n",
            "| \u001b[39m25       \u001b[39m | \u001b[39m-0.519270\u001b[39m | \u001b[39m4.0      \u001b[39m | \u001b[39m100.88901\u001b[39m | \u001b[39m0.1      \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m0.0      \u001b[39m | \u001b[39m2.0      \u001b[39m |\n",
            "=================================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_1 = CatBoostRegressor(\n",
        "    depth=1,\n",
        "    iterations=2000,\n",
        "    learning_rate=0.01,\n",
        "    l2_leaf_reg=50,\n",
        "    bagging_temperature=0,\n",
        "    border_count=96,\n",
        "    loss_function=f'Huber:delta={0.5 * sigma_hat}',\n",
        "    verbose=100,\n",
        "    task_type='GPU',\n",
        ")\n",
        "\n",
        "\n",
        "model_1.fit(train_pool)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSUfy4G013x0",
        "outputId": "6fb56c1f-3262-450f-ead1-ae04410d23d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0:\tlearn: 1.6076632\ttotal: 13.5ms\tremaining: 27s\n",
            "100:\tlearn: 0.3792730\ttotal: 885ms\tremaining: 16.6s\n",
            "200:\tlearn: 0.2067158\ttotal: 1.49s\tremaining: 13.3s\n",
            "300:\tlearn: 0.1693709\ttotal: 1.76s\tremaining: 9.96s\n",
            "400:\tlearn: 0.1560493\ttotal: 2s\tremaining: 8s\n",
            "500:\tlearn: 0.1496215\ttotal: 2.27s\tremaining: 6.8s\n",
            "600:\tlearn: 0.1461245\ttotal: 2.49s\tremaining: 5.79s\n",
            "700:\tlearn: 0.1439572\ttotal: 2.71s\tremaining: 5.01s\n",
            "800:\tlearn: 0.1425057\ttotal: 2.91s\tremaining: 4.36s\n",
            "900:\tlearn: 0.1414943\ttotal: 3.15s\tremaining: 3.84s\n",
            "1000:\tlearn: 0.1407485\ttotal: 3.36s\tremaining: 3.35s\n",
            "1100:\tlearn: 0.1401990\ttotal: 3.57s\tremaining: 2.91s\n",
            "1200:\tlearn: 0.1397756\ttotal: 3.77s\tremaining: 2.51s\n",
            "1300:\tlearn: 0.1394364\ttotal: 3.99s\tremaining: 2.14s\n",
            "1400:\tlearn: 0.1391551\ttotal: 4.22s\tremaining: 1.8s\n",
            "1500:\tlearn: 0.1389133\ttotal: 4.43s\tremaining: 1.47s\n",
            "1600:\tlearn: 0.1387006\ttotal: 4.64s\tremaining: 1.16s\n",
            "1700:\tlearn: 0.1385114\ttotal: 4.84s\tremaining: 852ms\n",
            "1800:\tlearn: 0.1383422\ttotal: 5.06s\tremaining: 559ms\n",
            "1900:\tlearn: 0.1381936\ttotal: 5.29s\tremaining: 276ms\n",
            "1999:\tlearn: 0.1380606\ttotal: 5.5s\tremaining: 0us\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<catboost.core.CatBoostRegressor at 0x7adb4ea7e420>"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "families_models.set_family('AUTOMOTIVE', model=model_1, input_func=input_func_1, output_func=output_func_1)\n",
        "pred_frame, common_loss = families_models.predict_family(valid, 'AUTOMOTIVE', True, verbose=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6CIqoCV3WCW",
        "outputId": "412a8ed7-1e7c-4963-b72e-92c68dea599d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "FAMILY AUTOMOTIVE\n",
            "\n",
            "Loss on 2017-08-01: 0.5496756500556205\n",
            "Loss on 2017-08-02: 0.5565513347003225\n",
            "Loss on 2017-08-03: 0.5390836947969766\n",
            "Loss on 2017-08-04: 0.6199470109944155\n",
            "Loss on 2017-08-05: 0.4563432476559968\n",
            "Loss on 2017-08-06: 0.4700667445994126\n",
            "Loss on 2017-08-07: 0.49894585951996806\n",
            "Loss on 2017-08-08: 0.5493815754188087\n",
            "Loss on 2017-08-09: 0.4553798781715263\n",
            "Loss on 2017-08-10: 0.4947861674112539\n",
            "Loss on 2017-08-11: 0.6494766038066936\n",
            "Loss on 2017-08-12: 0.47714196917184043\n",
            "Loss on 2017-08-13: 0.6083143304731341\n",
            "Loss on 2017-08-14: 0.49075998882709765\n",
            "Loss on 2017-08-15: 0.5383207437548301\n",
            "\n",
            "Common Loss: 0.5302783199571931\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The distribution of the logarithmic target looks like normal, but Huber Loss does not help, you can optimize RMSE directly.\n",
        "I tried to make the model simpler, but even at 100 iterations with trees of depth 2 and l2_leaf_reg 500, the model already had a learn score of 0.13"
      ],
      "metadata": {
        "id": "2IS9IWc9DTap"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tweedie (gamma)"
      ],
      "metadata": {
        "id": "qKRBIIg2EUMY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_pool = Pool(X_1, y + 1, cat_features)"
      ],
      "metadata": {
        "id": "pC_tlzk3E_1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pbounds = {\n",
        "    'depth': (3, 5),\n",
        "    'border_count': (64, 128),\n",
        "    'iterations': (1000, 3000),\n",
        "    'learning_rate': (0.01, 0.03),\n",
        "    'l2_leaf_reg': (50, 200),\n",
        "    'random_strength': (1.0, 5.0),\n",
        "    'bagging_temperature': (1.0, 2.0),\n",
        "    'variance_power': (1.5, 1.7)\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "def objective(depth, border_count, iterations, learning_rate, l2_leaf_reg, random_strength,\n",
        "              bagging_temperature, variance_power):\n",
        "\n",
        "    iterations = int(iterations)\n",
        "    depth = int(depth)\n",
        "    border_count = int(border_count)\n",
        "\n",
        "    model = CatBoostRegressor(\n",
        "        depth=depth,\n",
        "        border_count=border_count,\n",
        "        iterations=iterations,\n",
        "        learning_rate=learning_rate,\n",
        "        l2_leaf_reg=l2_leaf_reg,\n",
        "        random_strength=random_strength,\n",
        "        bagging_temperature=bagging_temperature,\n",
        "        loss_function=f'Tweedie:variance_power={variance_power}',  # Gamma\n",
        "        verbose=0,\n",
        "        task_type='GPU',\n",
        "    )\n",
        "\n",
        "    model.fit(train_pool)\n",
        "    print('\\ntrain score:', model.get_best_score()['learn'])\n",
        "\n",
        "    families_models.set_family('AUTOMOTIVE', model=model, input_func=input_func_1, output_func=output_func_2)\n",
        "    pred_frame, common_loss = families_models.predict_family(valid, 'AUTOMOTIVE', True, verbose=False)\n",
        "\n",
        "    return -common_loss\n",
        "\n",
        "\n",
        "optimizer = BayesianOptimization(\n",
        "    f=objective,\n",
        "    pbounds=pbounds,\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "optimizer.maximize(init_points=5, n_iter=20)\n",
        "\n",
        "best_params = optimizer.max['params']\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 668
        },
        "id": "cZLX0vyFLrdu",
        "outputId": "0e1278d0-95c6-4234-bd24-80606662f516"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|   iter    |  target   |   depth   | border... | iterat... | learni... | l2_lea... | random... | baggin... | varian... |\n",
            "-------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "train score: {'Tweedie:variance_power=1.673235229154987': 13.592502828915872}\n",
            "| \u001b[39m1        \u001b[39m | \u001b[39m-1.974962\u001b[39m | \u001b[39m3.7490802\u001b[39m | \u001b[39m124.84571\u001b[39m | \u001b[39m2463.9878\u001b[39m | \u001b[39m0.0219731\u001b[39m | \u001b[39m73.402796\u001b[39m | \u001b[39m1.6239780\u001b[39m | \u001b[39m1.0580836\u001b[39m | \u001b[39m1.6732352\u001b[39m |\n",
            "\n",
            "train score: {'Tweedie:variance_power=1.5366809019706869': 15.350347630238934}\n",
            "| \u001b[35m2        \u001b[39m | \u001b[35m-1.961437\u001b[39m | \u001b[35m4.2022300\u001b[39m | \u001b[35m109.31664\u001b[39m | \u001b[35m1041.1689\u001b[39m | \u001b[35m0.0293981\u001b[39m | \u001b[35m174.86639\u001b[39m | \u001b[35m1.8493564\u001b[39m | \u001b[35m1.1818249\u001b[39m | \u001b[35m1.5366809\u001b[39m |\n",
            "\n",
            "train score: {'Tweedie:variance_power=1.5732723686587384': 14.71188579884232}\n",
            "| \u001b[39m3        \u001b[39m | \u001b[39m-1.970208\u001b[39m | \u001b[39m3.6084844\u001b[39m | \u001b[39m97.584411\u001b[39m | \u001b[39m1863.8900\u001b[39m | \u001b[39m0.0158245\u001b[39m | \u001b[39m141.77793\u001b[39m | \u001b[39m1.5579754\u001b[39m | \u001b[39m1.2921446\u001b[39m | \u001b[39m1.5732723\u001b[39m |\n",
            "\n",
            "train score: {'Tweedie:variance_power=1.5341048247374582': 15.420750913957436}\n",
            "| \u001b[39m4        \u001b[39m | \u001b[39m-1.988952\u001b[39m | \u001b[39m3.9121399\u001b[39m | \u001b[39m114.25126\u001b[39m | \u001b[39m1399.3475\u001b[39m | \u001b[39m0.0202846\u001b[39m | \u001b[39m138.86218\u001b[39m | \u001b[39m1.1858016\u001b[39m | \u001b[39m1.6075448\u001b[39m | \u001b[39m1.5341048\u001b[39m |\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1479175877.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m )\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_points\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0mbest_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/bayes_opt/bayesian_optimization.py\u001b[0m in \u001b[0;36mmaximize\u001b[0;34m(self, init_points, n_iter)\u001b[0m\n\u001b[1;32m    320\u001b[0m                 \u001b[0mx_probe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuggest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 \u001b[0miteration\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_probe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlazy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bounds_transformer\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/bayes_opt/bayesian_optimization.py\u001b[0m in \u001b[0;36mprobe\u001b[0;34m(self, params, lazy)\u001b[0m\n\u001b[1;32m    237\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m             self.logger.log_optimization_step(\n\u001b[1;32m    241\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/bayes_opt/target_space.py\u001b[0m in \u001b[0;36mprobe\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    553\u001b[0m             \u001b[0merror_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"No target function has been provided.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mdict_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constraint\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1479175877.py\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(depth, border_count, iterations, learning_rate, l2_leaf_reg, random_strength, bagging_temperature, variance_power)\u001b[0m\n\u001b[1;32m     32\u001b[0m     )\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_pool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\ntrain score:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_best_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'learn'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/catboost/core.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, cat_features, text_features, embedding_features, graph, sample_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[1;32m   5871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'loss_function'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5872\u001b[0m             \u001b[0mCatBoostRegressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_is_compatible_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss_function'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5873\u001b[0;31m         return self._fit(X, y, cat_features, text_features, embedding_features, None, graph, sample_weight, None, None, None, None, baseline,\n\u001b[0m\u001b[1;32m   5874\u001b[0m                          \u001b[0muse_best_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogging_level\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn_description\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5875\u001b[0m                          \u001b[0mverbose_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_period\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msilent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/catboost/core.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, cat_features, text_features, embedding_features, pairs, graph, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[1;32m   2408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2409\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mplot_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Training plots'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_get_train_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2410\u001b[0;31m                 self._train(\n\u001b[0m\u001b[1;32m   2411\u001b[0m                     \u001b[0mtrain_pool\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2412\u001b[0m                     \u001b[0mtrain_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"eval_sets\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/catboost/core.py\u001b[0m in \u001b[0;36m_train\u001b[0;34m(self, train_pool, test_pool, params, allow_clear_pool, init_model)\u001b[0m\n\u001b[1;32m   1788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1789\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_clear_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1790\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_clear_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_object\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minit_model\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1791\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_trained_model_attributes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1792\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._CatBoost._train\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._CatBoost._train\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### models"
      ],
      "metadata": {
        "id": "GHQsT2f4MDgq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_pool = Pool(X_1, y_log, cat_features)\n",
        "catboost_model = CatBoostRegressor(\n",
        "    iterations=996,\n",
        "    border_count=227,\n",
        "    depth=9,\n",
        "    learning_rate=0.0429257,\n",
        "    l2_leaf_reg=8.3054692,\n",
        "    random_strength=1.2160092,\n",
        "    bagging_temperature=0.9713499,\n",
        "    # task_type='GPU',\n",
        "    loss_function='RMSE',\n",
        "    )\n",
        "\n",
        "catboost_model.fit(train_pool, verbose=500)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vaANWy9SL_oB",
        "outputId": "c9c67e5d-fe35-43ef-9b62-6a1d29d654ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0:\tlearn: 0.8629125\ttotal: 697ms\tremaining: 11m 33s\n",
            "500:\tlearn: 0.4932873\ttotal: 3m 25s\tremaining: 3m 22s\n",
            "995:\tlearn: 0.4759069\ttotal: 6m 19s\tremaining: 0us\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<catboost.core.CatBoostRegressor at 0x7cfb45b6e240>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## XGBoost"
      ],
      "metadata": {
        "id": "tSisDqlU1cxn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBRegressor"
      ],
      "metadata": {
        "id": "fvu6KCS-EVkp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RMSLE"
      ],
      "metadata": {
        "id": "NfLNVwe2V1Gz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pbounds = {\n",
        "    'learning_rate': (0.01, 0.2),\n",
        "    'max_depth': (4, 12),\n",
        "    'min_child_weight': (1, 10),\n",
        "    'gamma': (0, 5),\n",
        "    'subsample': (0.5, 1.0),\n",
        "    'colsample_bytree': (0.5, 1.0),\n",
        "    'reg_alpha': (0, 10),\n",
        "    'reg_lambda': (0, 10),\n",
        "    'max_bin': (64, 512),\n",
        "    'scale_pos_weight': (1, 5),\n",
        "    'n_estimators': (500, 5000)\n",
        "}\n",
        "\n",
        "\n",
        "def objective(learning_rate, max_depth, min_child_weight, gamma,\n",
        "              subsample, colsample_bytree, reg_alpha, reg_lambda, max_bin,\n",
        "              scale_pos_weight, n_estimators):\n",
        "\n",
        "  model = XGBRegressor(\n",
        "    objective='reg:squaredlogerror',\n",
        "    n_estimators=int(n_estimators),\n",
        "    learning_rate=learning_rate,\n",
        "    tree_method='gpu_hist',\n",
        "    enable_categorical=True,\n",
        "    max_depth=int(max_depth),\n",
        "    min_child_weight=min_child_weight,\n",
        "    gamma=gamma,\n",
        "    subsample=subsample,\n",
        "    colsample_bytree=colsample_bytree,\n",
        "    reg_alpha=reg_alpha,\n",
        "    reg_lambda=reg_lambda,\n",
        "    max_bin=int(max_bin),\n",
        "    scale_pos_weight=scale_pos_weight,\n",
        "    verbosity=0,\n",
        "  )\n",
        "\n",
        "  model.fit(X_1, y, verbose=False)\n",
        "\n",
        "  families_models.set_family('AUTOMOTIVE', model=model, input_func=input_func_1, output_func=output_func_3)\n",
        "  _, common_loss = families_models.predict_family(valid, 'AUTOMOTIVE', True, verbose=False)\n",
        "\n",
        "  return -common_loss #optimize rmsle metric\n",
        "\n",
        "\n",
        "optimizer = BayesianOptimization(\n",
        "    f=objective,\n",
        "    pbounds=pbounds,\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "\n",
        "optimizer.maximize(init_points=5, n_iter=20)\n",
        "\n",
        "best_params = optimizer.max['params']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1edNxqRQgRO",
        "outputId": "7b09b122-b2b6-4cc6-9e78-48b25740b4de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|   iter    |  target   | learni... | max_depth | min_ch... |   gamma   | subsample | colsam... | reg_alpha | reg_la... |  max_bin  | scale_... | n_esti... |\n",
            "-------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "| \u001b[39m1        \u001b[39m | \u001b[39m-0.525749\u001b[39m | \u001b[39m0.0811626\u001b[39m | \u001b[39m11.605714\u001b[39m | \u001b[39m7.5879454\u001b[39m | \u001b[39m2.9932924\u001b[39m | \u001b[39m0.5780093\u001b[39m | \u001b[39m0.5779972\u001b[39m | \u001b[39m0.5808361\u001b[39m | \u001b[39m8.6617614\u001b[39m | \u001b[39m333.29952\u001b[39m | \u001b[39m3.8322903\u001b[39m | \u001b[39m592.63022\u001b[39m |\n",
            "| \u001b[35m2        \u001b[39m | \u001b[35m-0.520139\u001b[39m | \u001b[35m0.1942828\u001b[39m | \u001b[35m10.659541\u001b[39m | \u001b[35m2.9110519\u001b[39m | \u001b[35m0.9091248\u001b[39m | \u001b[35m0.5917022\u001b[39m | \u001b[35m0.6521211\u001b[39m | \u001b[35m5.2475643\u001b[39m | \u001b[35m4.3194501\u001b[39m | \u001b[35m194.47065\u001b[39m | \u001b[35m3.4474115\u001b[39m | \u001b[35m1127.7223\u001b[39m |\n",
            "| \u001b[39m3        \u001b[39m | \u001b[39m-0.527881\u001b[39m | \u001b[39m0.0655074\u001b[39m | \u001b[39m6.9308947\u001b[39m | \u001b[39m5.1046298\u001b[39m | \u001b[39m3.9258798\u001b[39m | \u001b[39m0.5998368\u001b[39m | \u001b[39m0.7571172\u001b[39m | \u001b[39m5.9241456\u001b[39m | \u001b[39m0.4645041\u001b[39m | \u001b[39m336.18009\u001b[39m | \u001b[39m1.6820964\u001b[39m | \u001b[39m792.73216\u001b[39m |\n",
            "| \u001b[39m4        \u001b[39m | \u001b[39m-0.521551\u001b[39m | \u001b[39m0.1902882\u001b[39m | \u001b[39m11.725056\u001b[39m | \u001b[39m8.2755761\u001b[39m | \u001b[39m1.5230688\u001b[39m | \u001b[39m0.5488360\u001b[39m | \u001b[39m0.8421165\u001b[39m | \u001b[39m4.4015249\u001b[39m | \u001b[39m1.2203823\u001b[39m | \u001b[39m285.83925\u001b[39m | \u001b[39m1.1375540\u001b[39m | \u001b[39m4591.9418\u001b[39m |\n",
            "| \u001b[39m5        \u001b[39m | \u001b[39m-0.526100\u001b[39m | \u001b[39m0.0591681\u001b[39m | \u001b[39m9.3001782\u001b[39m | \u001b[39m3.8053996\u001b[39m | \u001b[39m2.6003401\u001b[39m | \u001b[39m0.7733551\u001b[39m | \u001b[39m0.5924272\u001b[39m | \u001b[39m9.6958462\u001b[39m | \u001b[39m7.7513282\u001b[39m | \u001b[39m484.89552\u001b[39m | \u001b[39m4.5793094\u001b[39m | \u001b[39m3190.5499\u001b[39m |\n",
            "| \u001b[35m6        \u001b[39m | \u001b[35m-0.518866\u001b[39m | \u001b[35m0.1708453\u001b[39m | \u001b[35m9.0083557\u001b[39m | \u001b[35m2.2164742\u001b[39m | \u001b[35m2.8139061\u001b[39m | \u001b[35m0.7846351\u001b[39m | \u001b[35m0.8559835\u001b[39m | \u001b[35m3.3895951\u001b[39m | \u001b[35m3.8427798\u001b[39m | \u001b[35m286.68418\u001b[39m | \u001b[35m2.2148555\u001b[39m | \u001b[35m4588.5059\u001b[39m |\n",
            "| \u001b[39m7        \u001b[39m | \u001b[39m-0.527295\u001b[39m | \u001b[39m0.1291626\u001b[39m | \u001b[39m8.3005791\u001b[39m | \u001b[39m2.0805602\u001b[39m | \u001b[39m3.4538125\u001b[39m | \u001b[39m0.7022937\u001b[39m | \u001b[39m0.8067063\u001b[39m | \u001b[39m5.5112929\u001b[39m | \u001b[39m7.0751527\u001b[39m | \u001b[39m286.61065\u001b[39m | \u001b[39m4.2031794\u001b[39m | \u001b[39m4588.7743\u001b[39m |\n",
            "| \u001b[39m8        \u001b[39m | \u001b[39m-0.528213\u001b[39m | \u001b[39m0.0489743\u001b[39m | \u001b[39m4.7621509\u001b[39m | \u001b[39m2.9607236\u001b[39m | \u001b[39m3.3389077\u001b[39m | \u001b[39m0.8125367\u001b[39m | \u001b[39m0.8985650\u001b[39m | \u001b[39m2.9048639\u001b[39m | \u001b[39m2.5217709\u001b[39m | \u001b[39m429.83561\u001b[39m | \u001b[39m3.4034912\u001b[39m | \u001b[39m1997.8435\u001b[39m |\n",
            "| \u001b[39m9        \u001b[39m | \u001b[39m-0.521909\u001b[39m | \u001b[39m0.1332743\u001b[39m | \u001b[39m8.8781476\u001b[39m | \u001b[39m1.1261810\u001b[39m | \u001b[39m2.5019813\u001b[39m | \u001b[39m0.5992040\u001b[39m | \u001b[39m0.6252350\u001b[39m | \u001b[39m2.1387100\u001b[39m | \u001b[39m5.9788213\u001b[39m | \u001b[39m293.56813\u001b[39m | \u001b[39m3.9462102\u001b[39m | \u001b[39m2522.7698\u001b[39m |\n",
            "| \u001b[39m10       \u001b[39m | \u001b[39m-0.526289\u001b[39m | \u001b[39m0.1118314\u001b[39m | \u001b[39m8.8724266\u001b[39m | \u001b[39m4.3778918\u001b[39m | \u001b[39m3.4419168\u001b[39m | \u001b[39m0.5236680\u001b[39m | \u001b[39m0.6588583\u001b[39m | \u001b[39m4.7596768\u001b[39m | \u001b[39m8.0997313\u001b[39m | \u001b[39m102.21632\u001b[39m | \u001b[39m2.9002289\u001b[39m | \u001b[39m4908.2447\u001b[39m |\n",
            "| \u001b[39m11       \u001b[39m | \u001b[39m-0.527321\u001b[39m | \u001b[39m0.0555009\u001b[39m | \u001b[39m6.6830484\u001b[39m | \u001b[39m4.7510531\u001b[39m | \u001b[39m4.4602819\u001b[39m | \u001b[39m0.5166051\u001b[39m | \u001b[39m0.5781779\u001b[39m | \u001b[39m3.1384440\u001b[39m | \u001b[39m7.7233054\u001b[39m | \u001b[39m327.16998\u001b[39m | \u001b[39m2.6071867\u001b[39m | \u001b[39m3542.4467\u001b[39m |\n",
            "| \u001b[39m12       \u001b[39m | \u001b[39m-0.527204\u001b[39m | \u001b[39m0.0156766\u001b[39m | \u001b[39m8.1464652\u001b[39m | \u001b[39m1.4813105\u001b[39m | \u001b[39m0.3986832\u001b[39m | \u001b[39m0.9784496\u001b[39m | \u001b[39m0.7107475\u001b[39m | \u001b[39m4.9906773\u001b[39m | \u001b[39m5.8189032\u001b[39m | \u001b[39m502.14491\u001b[39m | \u001b[39m4.1654145\u001b[39m | \u001b[39m2998.1534\u001b[39m |\n",
            "| \u001b[39m13       \u001b[39m | \u001b[39m-0.527869\u001b[39m | \u001b[39m0.1957538\u001b[39m | \u001b[39m11.814512\u001b[39m | \u001b[39m3.1751636\u001b[39m | \u001b[39m1.2171006\u001b[39m | \u001b[39m0.9984353\u001b[39m | \u001b[39m0.8920455\u001b[39m | \u001b[39m1.1327266\u001b[39m | \u001b[39m7.9724231\u001b[39m | \u001b[39m161.62827\u001b[39m | \u001b[39m3.7686851\u001b[39m | \u001b[39m946.05391\u001b[39m |\n",
            "| \u001b[39m14       \u001b[39m | \u001b[39m-0.526279\u001b[39m | \u001b[39m0.0890871\u001b[39m | \u001b[39m5.1737298\u001b[39m | \u001b[39m6.2995027\u001b[39m | \u001b[39m4.6921785\u001b[39m | \u001b[39m0.9135916\u001b[39m | \u001b[39m0.8108740\u001b[39m | \u001b[39m9.1683862\u001b[39m | \u001b[39m5.0933098\u001b[39m | \u001b[39m349.43309\u001b[39m | \u001b[39m4.4185323\u001b[39m | \u001b[39m3974.9034\u001b[39m |\n",
            "| \u001b[39m15       \u001b[39m | \u001b[39m-0.526751\u001b[39m | \u001b[39m0.1307735\u001b[39m | \u001b[39m7.2402659\u001b[39m | \u001b[39m9.8056359\u001b[39m | \u001b[39m1.2435016\u001b[39m | \u001b[39m0.9359783\u001b[39m | \u001b[39m0.7873791\u001b[39m | \u001b[39m3.4591932\u001b[39m | \u001b[39m4.9845073\u001b[39m | \u001b[39m397.97000\u001b[39m | \u001b[39m3.0227839\u001b[39m | \u001b[39m3231.9622\u001b[39m |\n",
            "| \u001b[39m16       \u001b[39m | \u001b[39m-0.526557\u001b[39m | \u001b[39m0.0310271\u001b[39m | \u001b[39m7.8539178\u001b[39m | \u001b[39m1.2308336\u001b[39m | \u001b[39m4.1267846\u001b[39m | \u001b[39m0.9725276\u001b[39m | \u001b[39m0.9407531\u001b[39m | \u001b[39m2.1865631\u001b[39m | \u001b[39m4.1249650\u001b[39m | \u001b[39m425.59751\u001b[39m | \u001b[39m2.8026923\u001b[39m | \u001b[39m1650.1479\u001b[39m |\n",
            "| \u001b[39m17       \u001b[39m | \u001b[39m-0.521545\u001b[39m | \u001b[39m0.0261177\u001b[39m | \u001b[39m6.3738352\u001b[39m | \u001b[39m2.1486990\u001b[39m | \u001b[39m0.5025755\u001b[39m | \u001b[39m0.6397369\u001b[39m | \u001b[39m0.9200191\u001b[39m | \u001b[39m4.5190556\u001b[39m | \u001b[39m7.1015915\u001b[39m | \u001b[39m304.41501\u001b[39m | \u001b[39m3.8715154\u001b[39m | \u001b[39m637.89677\u001b[39m |\n",
            "| \u001b[39m18       \u001b[39m | \u001b[39m-0.525199\u001b[39m | \u001b[39m0.0227552\u001b[39m | \u001b[39m5.5423967\u001b[39m | \u001b[39m7.7666220\u001b[39m | \u001b[39m0.0965608\u001b[39m | \u001b[39m0.7823399\u001b[39m | \u001b[39m0.5792964\u001b[39m | \u001b[39m2.0829753\u001b[39m | \u001b[39m2.4781889\u001b[39m | \u001b[39m423.69464\u001b[39m | \u001b[39m2.5789456\u001b[39m | \u001b[39m2777.4690\u001b[39m |\n",
            "| \u001b[39m19       \u001b[39m | \u001b[39m-0.520730\u001b[39m | \u001b[39m0.1329820\u001b[39m | \u001b[39m8.2048294\u001b[39m | \u001b[39m8.0695757\u001b[39m | \u001b[39m3.0694605\u001b[39m | \u001b[39m0.8788809\u001b[39m | \u001b[39m0.6522801\u001b[39m | \u001b[39m9.1840992\u001b[39m | \u001b[39m8.7076505\u001b[39m | \u001b[39m90.801544\u001b[39m | \u001b[39m1.0009788\u001b[39m | \u001b[39m4101.3197\u001b[39m |\n",
            "| \u001b[39m20       \u001b[39m | \u001b[39m-0.530664\u001b[39m | \u001b[39m0.0162507\u001b[39m | \u001b[39m5.6253948\u001b[39m | \u001b[39m2.2495031\u001b[39m | \u001b[39m2.3132734\u001b[39m | \u001b[39m0.7263213\u001b[39m | \u001b[39m0.5162749\u001b[39m | \u001b[39m2.9305058\u001b[39m | \u001b[39m6.3049386\u001b[39m | \u001b[39m67.301088\u001b[39m | \u001b[39m1.1761432\u001b[39m | \u001b[39m540.20506\u001b[39m |\n",
            "| \u001b[39m21       \u001b[39m | \u001b[39m-0.534556\u001b[39m | \u001b[39m0.0349240\u001b[39m | \u001b[39m9.9574660\u001b[39m | \u001b[39m2.5418794\u001b[39m | \u001b[39m0.2522339\u001b[39m | \u001b[39m0.5200748\u001b[39m | \u001b[39m0.8618464\u001b[39m | \u001b[39m8.0890805\u001b[39m | \u001b[39m2.7599049\u001b[39m | \u001b[39m146.37498\u001b[39m | \u001b[39m4.4992689\u001b[39m | \u001b[39m3688.4837\u001b[39m |\n",
            "| \u001b[39m22       \u001b[39m | \u001b[39m-0.525775\u001b[39m | \u001b[39m0.1946642\u001b[39m | \u001b[39m8.4826244\u001b[39m | \u001b[39m7.6348993\u001b[39m | \u001b[39m3.6602501\u001b[39m | \u001b[39m0.6719552\u001b[39m | \u001b[39m0.7881055\u001b[39m | \u001b[39m9.3177911\u001b[39m | \u001b[39m8.7634836\u001b[39m | \u001b[39m263.39949\u001b[39m | \u001b[39m4.7795849\u001b[39m | \u001b[39m2004.3922\u001b[39m |\n",
            "| \u001b[39m23       \u001b[39m | \u001b[39m-0.525256\u001b[39m | \u001b[39m0.1088971\u001b[39m | \u001b[39m6.1539163\u001b[39m | \u001b[39m2.5284119\u001b[39m | \u001b[39m3.9024212\u001b[39m | \u001b[39m0.7528368\u001b[39m | \u001b[39m0.9931766\u001b[39m | \u001b[39m7.9388915\u001b[39m | \u001b[39m1.5305864\u001b[39m | \u001b[39m138.37521\u001b[39m | \u001b[39m3.2541993\u001b[39m | \u001b[39m4592.2479\u001b[39m |\n",
            "| \u001b[39m24       \u001b[39m | \u001b[39m-0.522512\u001b[39m | \u001b[39m0.0802388\u001b[39m | \u001b[39m11.205135\u001b[39m | \u001b[39m2.0243332\u001b[39m | \u001b[39m0.7357258\u001b[39m | \u001b[39m0.9525221\u001b[39m | \u001b[39m0.7792586\u001b[39m | \u001b[39m6.3088513\u001b[39m | \u001b[39m7.9389158\u001b[39m | \u001b[39m374.44335\u001b[39m | \u001b[39m3.2319040\u001b[39m | \u001b[39m1607.3102\u001b[39m |\n",
            "| \u001b[39m25       \u001b[39m | \u001b[39m-0.524050\u001b[39m | \u001b[39m0.1305061\u001b[39m | \u001b[39m11.503069\u001b[39m | \u001b[39m6.0212672\u001b[39m | \u001b[39m3.8604468\u001b[39m | \u001b[39m0.5467943\u001b[39m | \u001b[39m0.5828339\u001b[39m | \u001b[39m9.0725240\u001b[39m | \u001b[39m3.0475883\u001b[39m | \u001b[39m64.653505\u001b[39m | \u001b[39m1.8276999\u001b[39m | \u001b[39m2766.9367\u001b[39m |\n",
            "=============================================================================================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BjaS-7Y8TTF5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gamma"
      ],
      "metadata": {
        "id": "n1cZXQagV3_Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pbounds = {\n",
        "    'learning_rate': (0.01, 0.2),\n",
        "    'max_depth': (4, 12),\n",
        "    'min_child_weight': (1, 10),\n",
        "    'gamma': (0, 5),\n",
        "    'subsample': (0.5, 1.0),\n",
        "    'colsample_bytree': (0.5, 1.0),\n",
        "    'reg_alpha': (0, 10),\n",
        "    'reg_lambda': (0, 10),\n",
        "    'max_bin': (64, 512),\n",
        "    'scale_pos_weight': (1, 5),\n",
        "    'n_estimators': (500, 5000)\n",
        "}\n",
        "\n",
        "def objective(learning_rate, max_depth, min_child_weight, gamma,\n",
        "              subsample, colsample_bytree, reg_alpha, reg_lambda, max_bin,\n",
        "              scale_pos_weight, n_estimators):\n",
        "\n",
        "    model = XGBRegressor( # without enable_cat cause of ohe\n",
        "        objective='reg:gamma',\n",
        "        n_estimators=int(n_estimators),\n",
        "        learning_rate=learning_rate,\n",
        "        tree_method='gpu_hist',\n",
        "        max_depth=int(max_depth),\n",
        "        min_child_weight=min_child_weight,\n",
        "        gamma=gamma,\n",
        "        subsample=subsample,\n",
        "        colsample_bytree=colsample_bytree,\n",
        "        reg_alpha=reg_alpha,\n",
        "        reg_lambda=reg_lambda,\n",
        "        max_bin=int(max_bin),\n",
        "        scale_pos_weight=scale_pos_weight,\n",
        "        verbosity=0,\n",
        "        predictor='gpu_predictor'\n",
        "    )\n",
        "\n",
        "    model.fit(\n",
        "        X_4, y+1,\n",
        "        verbose=False,\n",
        "    )\n",
        "\n",
        "    families_models.set_family('AUTOMOTIVE', model=model, input_func=input_func_4, output_func=output_func_2)\n",
        "    _, common_loss = families_models.predict_family(valid, 'AUTOMOTIVE', True, verbose=False)\n",
        "\n",
        "    return -common_loss\n",
        "\n",
        "# Bayesian Optimization\n",
        "optimizer = BayesianOptimization(\n",
        "    f=objective,\n",
        "    pbounds=pbounds,\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "optimizer.maximize(init_points=5, n_iter=20)\n",
        "\n",
        "best_params = optimizer.max['params']\n",
        "print(best_params)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bo9aQLLnV6Na",
        "outputId": "6cefc860-1dd5-4fe9-c621-055c8f04eede"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|   iter    |  target   | learni... | max_depth | min_ch... |   gamma   | subsample | colsam... | reg_alpha | reg_la... |  max_bin  | scale_... | n_esti... |\n",
            "-------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "| \u001b[39m1        \u001b[39m | \u001b[39m-0.522582\u001b[39m | \u001b[39m0.0811626\u001b[39m | \u001b[39m11.605714\u001b[39m | \u001b[39m7.5879454\u001b[39m | \u001b[39m2.9932924\u001b[39m | \u001b[39m0.5780093\u001b[39m | \u001b[39m0.5779972\u001b[39m | \u001b[39m0.5808361\u001b[39m | \u001b[39m8.6617614\u001b[39m | \u001b[39m333.29952\u001b[39m | \u001b[39m3.8322903\u001b[39m | \u001b[39m592.63022\u001b[39m |\n",
            "| \u001b[39m2        \u001b[39m | \u001b[39m-0.539151\u001b[39m | \u001b[39m0.1942828\u001b[39m | \u001b[39m10.659541\u001b[39m | \u001b[39m2.9110519\u001b[39m | \u001b[39m0.9091248\u001b[39m | \u001b[39m0.5917022\u001b[39m | \u001b[39m0.6521211\u001b[39m | \u001b[39m5.2475643\u001b[39m | \u001b[39m4.3194501\u001b[39m | \u001b[39m194.47065\u001b[39m | \u001b[39m3.4474115\u001b[39m | \u001b[39m1127.7223\u001b[39m |\n",
            "| \u001b[35m3        \u001b[39m | \u001b[35m-0.510634\u001b[39m | \u001b[35m0.0655074\u001b[39m | \u001b[35m6.9308947\u001b[39m | \u001b[35m5.1046298\u001b[39m | \u001b[35m3.9258798\u001b[39m | \u001b[35m0.5998368\u001b[39m | \u001b[35m0.7571172\u001b[39m | \u001b[35m5.9241456\u001b[39m | \u001b[35m0.4645041\u001b[39m | \u001b[35m336.18009\u001b[39m | \u001b[35m1.6820964\u001b[39m | \u001b[35m792.73216\u001b[39m |\n",
            "| \u001b[39m4        \u001b[39m | \u001b[39m-0.514569\u001b[39m | \u001b[39m0.1902882\u001b[39m | \u001b[39m11.725056\u001b[39m | \u001b[39m8.2755761\u001b[39m | \u001b[39m1.5230688\u001b[39m | \u001b[39m0.5488360\u001b[39m | \u001b[39m0.8421165\u001b[39m | \u001b[39m4.4015249\u001b[39m | \u001b[39m1.2203823\u001b[39m | \u001b[39m285.83925\u001b[39m | \u001b[39m1.1375540\u001b[39m | \u001b[39m4591.9418\u001b[39m |\n",
            "| \u001b[39m5        \u001b[39m | \u001b[39m-0.515659\u001b[39m | \u001b[39m0.0591681\u001b[39m | \u001b[39m9.3001782\u001b[39m | \u001b[39m3.8053996\u001b[39m | \u001b[39m2.6003401\u001b[39m | \u001b[39m0.7733551\u001b[39m | \u001b[39m0.5924272\u001b[39m | \u001b[39m9.6958462\u001b[39m | \u001b[39m7.7513282\u001b[39m | \u001b[39m484.89552\u001b[39m | \u001b[39m4.5793094\u001b[39m | \u001b[39m3190.5499\u001b[39m |\n",
            "| \u001b[39m6        \u001b[39m | \u001b[39m-0.519815\u001b[39m | \u001b[39m0.0544709\u001b[39m | \u001b[39m11.916696\u001b[39m | \u001b[39m2.2958648\u001b[39m | \u001b[39m1.8380233\u001b[39m | \u001b[39m0.5618298\u001b[39m | \u001b[39m0.7050335\u001b[39m | \u001b[39m8.4659825\u001b[39m | \u001b[39m7.8152769\u001b[39m | \u001b[39m486.49627\u001b[39m | \u001b[39m4.6084675\u001b[39m | \u001b[39m3185.2981\u001b[39m |\n",
            "| \u001b[39m7        \u001b[39m | \u001b[39m-0.519596\u001b[39m | \u001b[39m0.1180672\u001b[39m | \u001b[39m7.1422753\u001b[39m | \u001b[39m4.7559368\u001b[39m | \u001b[39m2.0019309\u001b[39m | \u001b[39m0.7257704\u001b[39m | \u001b[39m0.5348390\u001b[39m | \u001b[39m5.6155230\u001b[39m | \u001b[39m0.7219083\u001b[39m | \u001b[39m332.27495\u001b[39m | \u001b[39m3.6888145\u001b[39m | \u001b[39m793.56754\u001b[39m |\n",
            "| \u001b[39m8        \u001b[39m | \u001b[39m-0.514230\u001b[39m | \u001b[39m0.0140513\u001b[39m | \u001b[39m6.9195499\u001b[39m | \u001b[39m5.9478975\u001b[39m | \u001b[39m5.0      \u001b[39m | \u001b[39m0.5132123\u001b[39m | \u001b[39m0.9580380\u001b[39m | \u001b[39m5.6380221\u001b[39m | \u001b[39m0.8919208\u001b[39m | \u001b[39m341.63128\u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m791.46511\u001b[39m |\n",
            "| \u001b[39m9        \u001b[39m | \u001b[39m-0.514932\u001b[39m | \u001b[39m0.01     \u001b[39m | \u001b[39m4.0      \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m5.0      \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m10.0     \u001b[39m | \u001b[39m0.0      \u001b[39m | \u001b[39m338.58807\u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m795.07505\u001b[39m |\n",
            "| \u001b[39m10       \u001b[39m | \u001b[39m-0.516445\u001b[39m | \u001b[39m0.1442513\u001b[39m | \u001b[39m11.170852\u001b[39m | \u001b[39m9.0145999\u001b[39m | \u001b[39m1.0121755\u001b[39m | \u001b[39m0.7774125\u001b[39m | \u001b[39m0.8148604\u001b[39m | \u001b[39m6.1194172\u001b[39m | \u001b[39m4.3329348\u001b[39m | \u001b[39m285.14385\u001b[39m | \u001b[39m2.2455710\u001b[39m | \u001b[39m4602.1444\u001b[39m |\n",
            "| \u001b[35m11       \u001b[39m | \u001b[35m-0.509633\u001b[39m | \u001b[35m0.0555009\u001b[39m | \u001b[35m6.6830484\u001b[39m | \u001b[35m4.7510531\u001b[39m | \u001b[35m4.4602819\u001b[39m | \u001b[35m0.5166051\u001b[39m | \u001b[35m0.5781779\u001b[39m | \u001b[35m3.1384440\u001b[39m | \u001b[35m7.7233054\u001b[39m | \u001b[35m327.16998\u001b[39m | \u001b[35m2.6071867\u001b[39m | \u001b[35m3542.4467\u001b[39m |\n",
            "| \u001b[39m12       \u001b[39m | \u001b[39m-0.515484\u001b[39m | \u001b[39m0.0156766\u001b[39m | \u001b[39m8.1464652\u001b[39m | \u001b[39m1.4813105\u001b[39m | \u001b[39m0.3986832\u001b[39m | \u001b[39m0.9784496\u001b[39m | \u001b[39m0.7107475\u001b[39m | \u001b[39m4.9906773\u001b[39m | \u001b[39m5.8189032\u001b[39m | \u001b[39m502.14491\u001b[39m | \u001b[39m4.1654145\u001b[39m | \u001b[39m2998.1534\u001b[39m |\n",
            "| \u001b[39m13       \u001b[39m | \u001b[39m-0.512333\u001b[39m | \u001b[39m0.1176688\u001b[39m | \u001b[39m5.3129885\u001b[39m | \u001b[39m3.6907678\u001b[39m | \u001b[39m2.1182244\u001b[39m | \u001b[39m0.5754201\u001b[39m | \u001b[39m0.5690157\u001b[39m | \u001b[39m2.5651784\u001b[39m | \u001b[39m9.0759521\u001b[39m | \u001b[39m331.61167\u001b[39m | \u001b[39m2.1578457\u001b[39m | \u001b[39m3546.8026\u001b[39m |\n",
            "| \u001b[35m14       \u001b[39m | \u001b[35m-0.509183\u001b[39m | \u001b[35m0.0606537\u001b[39m | \u001b[35m8.5085808\u001b[39m | \u001b[35m5.1085874\u001b[39m | \u001b[35m3.8861886\u001b[39m | \u001b[35m0.5745213\u001b[39m | \u001b[35m0.7286591\u001b[39m | \u001b[35m5.3854684\u001b[39m | \u001b[35m7.5315923\u001b[39m | \u001b[35m333.28799\u001b[39m | \u001b[35m2.4328256\u001b[39m | \u001b[35m3538.4297\u001b[39m |\n",
            "| \u001b[39m15       \u001b[39m | \u001b[39m-0.509779\u001b[39m | \u001b[39m0.1307735\u001b[39m | \u001b[39m7.2402659\u001b[39m | \u001b[39m9.8056359\u001b[39m | \u001b[39m1.2435016\u001b[39m | \u001b[39m0.9359783\u001b[39m | \u001b[39m0.7873791\u001b[39m | \u001b[39m3.4591932\u001b[39m | \u001b[39m4.9845073\u001b[39m | \u001b[39m397.97000\u001b[39m | \u001b[39m3.0227839\u001b[39m | \u001b[39m3231.9622\u001b[39m |\n",
            "| \u001b[39m16       \u001b[39m | \u001b[39m-0.511238\u001b[39m | \u001b[39m0.0310271\u001b[39m | \u001b[39m7.8539178\u001b[39m | \u001b[39m1.2308336\u001b[39m | \u001b[39m4.1267846\u001b[39m | \u001b[39m0.9725276\u001b[39m | \u001b[39m0.9407531\u001b[39m | \u001b[39m2.1865631\u001b[39m | \u001b[39m4.1249650\u001b[39m | \u001b[39m425.59751\u001b[39m | \u001b[39m2.8026923\u001b[39m | \u001b[39m1650.1479\u001b[39m |\n",
            "| \u001b[39m17       \u001b[39m | \u001b[39m-0.511508\u001b[39m | \u001b[39m0.0261177\u001b[39m | \u001b[39m6.3738352\u001b[39m | \u001b[39m2.1486990\u001b[39m | \u001b[39m0.5025755\u001b[39m | \u001b[39m0.6397369\u001b[39m | \u001b[39m0.9200191\u001b[39m | \u001b[39m4.5190556\u001b[39m | \u001b[39m7.1015915\u001b[39m | \u001b[39m304.41501\u001b[39m | \u001b[39m3.8715154\u001b[39m | \u001b[39m637.89677\u001b[39m |\n",
            "| \u001b[39m18       \u001b[39m | \u001b[39m-0.514329\u001b[39m | \u001b[39m0.0227552\u001b[39m | \u001b[39m5.5423967\u001b[39m | \u001b[39m7.7666220\u001b[39m | \u001b[39m0.0965608\u001b[39m | \u001b[39m0.7823399\u001b[39m | \u001b[39m0.5792964\u001b[39m | \u001b[39m2.0829753\u001b[39m | \u001b[39m2.4781889\u001b[39m | \u001b[39m423.69464\u001b[39m | \u001b[39m2.5789456\u001b[39m | \u001b[39m2777.4690\u001b[39m |\n",
            "| \u001b[39m19       \u001b[39m | \u001b[39m-0.514055\u001b[39m | \u001b[39m0.0960042\u001b[39m | \u001b[39m4.4221737\u001b[39m | \u001b[39m6.3368010\u001b[39m | \u001b[39m0.0571020\u001b[39m | \u001b[39m0.5678846\u001b[39m | \u001b[39m0.5175370\u001b[39m | \u001b[39m1.6559542\u001b[39m | \u001b[39m5.2088896\u001b[39m | \u001b[39m330.14973\u001b[39m | \u001b[39m1.7515074\u001b[39m | \u001b[39m3537.4851\u001b[39m |\n",
            "| \u001b[39m20       \u001b[39m | \u001b[39m-0.511454\u001b[39m | \u001b[39m0.0162507\u001b[39m | \u001b[39m5.6253948\u001b[39m | \u001b[39m2.2495031\u001b[39m | \u001b[39m2.3132734\u001b[39m | \u001b[39m0.7263213\u001b[39m | \u001b[39m0.5162749\u001b[39m | \u001b[39m2.9305058\u001b[39m | \u001b[39m6.3049386\u001b[39m | \u001b[39m67.301088\u001b[39m | \u001b[39m1.1761432\u001b[39m | \u001b[39m540.20506\u001b[39m |\n",
            "| \u001b[39m21       \u001b[39m | \u001b[39m-0.536804\u001b[39m | \u001b[39m0.0349240\u001b[39m | \u001b[39m9.9574660\u001b[39m | \u001b[39m2.5418794\u001b[39m | \u001b[39m0.2522339\u001b[39m | \u001b[39m0.5200748\u001b[39m | \u001b[39m0.8618464\u001b[39m | \u001b[39m8.0890805\u001b[39m | \u001b[39m2.7599049\u001b[39m | \u001b[39m146.37498\u001b[39m | \u001b[39m4.4992689\u001b[39m | \u001b[39m3688.4837\u001b[39m |\n",
            "| \u001b[39m22       \u001b[39m | \u001b[39m-0.530363\u001b[39m | \u001b[39m0.1946642\u001b[39m | \u001b[39m8.4826244\u001b[39m | \u001b[39m7.6348993\u001b[39m | \u001b[39m3.6602501\u001b[39m | \u001b[39m0.6719552\u001b[39m | \u001b[39m0.7881055\u001b[39m | \u001b[39m9.3177911\u001b[39m | \u001b[39m8.7634836\u001b[39m | \u001b[39m263.39949\u001b[39m | \u001b[39m4.7795849\u001b[39m | \u001b[39m2004.3922\u001b[39m |\n",
            "| \u001b[39m23       \u001b[39m | \u001b[39m-0.511648\u001b[39m | \u001b[39m0.1088971\u001b[39m | \u001b[39m6.1539163\u001b[39m | \u001b[39m2.5284119\u001b[39m | \u001b[39m3.9024212\u001b[39m | \u001b[39m0.7528368\u001b[39m | \u001b[39m0.9931766\u001b[39m | \u001b[39m7.9388915\u001b[39m | \u001b[39m1.5305864\u001b[39m | \u001b[39m138.37521\u001b[39m | \u001b[39m3.2541993\u001b[39m | \u001b[39m4592.2479\u001b[39m |\n",
            "| \u001b[39m24       \u001b[39m | \u001b[39m-0.518452\u001b[39m | \u001b[39m0.0802388\u001b[39m | \u001b[39m11.205135\u001b[39m | \u001b[39m2.0243332\u001b[39m | \u001b[39m0.7357258\u001b[39m | \u001b[39m0.9525221\u001b[39m | \u001b[39m0.7792586\u001b[39m | \u001b[39m6.3088513\u001b[39m | \u001b[39m7.9389158\u001b[39m | \u001b[39m374.44335\u001b[39m | \u001b[39m3.2319040\u001b[39m | \u001b[39m1607.3102\u001b[39m |\n",
            "| \u001b[39m25       \u001b[39m | \u001b[39m-0.515793\u001b[39m | \u001b[39m0.1243871\u001b[39m | \u001b[39m11.808420\u001b[39m | \u001b[39m1.5399758\u001b[39m | \u001b[39m5.0      \u001b[39m | \u001b[39m0.8833167\u001b[39m | \u001b[39m0.9056283\u001b[39m | \u001b[39m9.3265485\u001b[39m | \u001b[39m2.5859738\u001b[39m | \u001b[39m329.62109\u001b[39m | \u001b[39m4.8713147\u001b[39m | \u001b[39m3543.0784\u001b[39m |\n",
            "=============================================================================================================================================================\n",
            "{'learning_rate': np.float64(0.06065371547389371), 'max_depth': np.float64(8.508580827035098), 'min_child_weight': np.float64(5.108587499603239), 'gamma': np.float64(3.886188696580525), 'subsample': np.float64(0.574521354998989), 'colsample_bytree': np.float64(0.7286591581741475), 'reg_alpha': np.float64(5.385468496364977), 'reg_lambda': np.float64(7.5315923258875435), 'max_bin': np.float64(333.28799580746954), 'scale_pos_weight': np.float64(2.432825625613177), 'n_estimators': np.float64(3538.4297378898004)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### models"
      ],
      "metadata": {
        "id": "K970gbjDQcfo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "xgb.set_config(verbosity=0)"
      ],
      "metadata": {
        "id": "kzA-A5-4U--k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xgboost_model = XGBRegressor(\n",
        "        objective='reg:gamma',\n",
        "        n_estimators=3538,\n",
        "        learning_rate=0.0606537,\n",
        "        tree_method='hist',\n",
        "        max_depth=8,\n",
        "        min_child_weight=5.1085874,\n",
        "        gamma=3.8861886,\n",
        "        subsample=0.5745213,\n",
        "        colsample_bytree=0.7286591,\n",
        "        reg_alpha=5.3854684,\n",
        "        reg_lambda=7.5315923,\n",
        "        max_bin=333,\n",
        "        scale_pos_weight=2.4328256 ,\n",
        "        device=\"cuda\",\n",
        ")\n",
        "\n",
        "xgboost_model.fit(X=X_4, y=y+1)\n",
        "\n",
        "families_models.set_family('AUTOMOTIVE', model=xgboost_model, input_func=input_func_4, output_func=output_func_2)\n",
        "xgboost_pred_frame, _ = families_models.predict_family(valid, 'AUTOMOTIVE', True, verbose=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZ6GQt-qQQCY",
        "outputId": "1aaeef10-ed7d-4988-c5fb-0582d8d7e6ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "FAMILY AUTOMOTIVE\n",
            "\n",
            "Loss on 2017-08-01: 0.548135025992128\n",
            "Loss on 2017-08-02: 0.5322290996728101\n",
            "Loss on 2017-08-03: 0.532047322141564\n",
            "Loss on 2017-08-04: 0.5626479193680096\n",
            "Loss on 2017-08-05: 0.4314828740464766\n",
            "Loss on 2017-08-06: 0.41098183587445253\n",
            "Loss on 2017-08-07: 0.48576675596400837\n",
            "Loss on 2017-08-08: 0.5415334805459774\n",
            "Loss on 2017-08-09: 0.44543473658931176\n",
            "Loss on 2017-08-10: 0.47538016918746506\n",
            "Loss on 2017-08-11: 0.5327368341377836\n",
            "Loss on 2017-08-12: 0.5227738255906444\n",
            "Loss on 2017-08-13: 0.5778687182768394\n",
            "Loss on 2017-08-14: 0.5378133567048226\n",
            "Loss on 2017-08-15: 0.5009245509337512\n",
            "\n",
            "Common Loss: 0.5091837670017364\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LightGBM"
      ],
      "metadata": {
        "id": "pF0bJmu0WHVH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import lightgbm as lgb"
      ],
      "metadata": {
        "id": "HfCpfKAxWKVH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RMSE"
      ],
      "metadata": {
        "id": "fj3RBXmKX5um"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pbounds_lgb = {\n",
        "    'learning_rate': (0.01, 0.2),\n",
        "    'max_depth': (4, 12),\n",
        "    'num_leaves': (16, 256),\n",
        "    'min_child_samples': (5, 50),\n",
        "    'min_child_weight': (1e-3, 10.0),\n",
        "    'subsample': (0.5, 1.0),\n",
        "    'colsample_bytree': (0.5, 1.0),\n",
        "    'reg_alpha': (0, 10),\n",
        "    'reg_lambda': (0, 10),\n",
        "    'max_bin': (64, 255),\n",
        "    'n_estimators': (500, 2000)\n",
        "}\n",
        "\n",
        "def objective(learning_rate, max_depth, num_leaves, min_child_samples,\n",
        "              min_child_weight, subsample, colsample_bytree,\n",
        "              reg_alpha, reg_lambda, max_bin, n_estimators):\n",
        "\n",
        "    train_pool = lgb.Dataset(X_1, label=y_log, categorical_feature=cat_features)\n",
        "\n",
        "    max_depth = int(max_depth)\n",
        "    num_leaves = int(num_leaves)\n",
        "    min_child_samples = int(min_child_samples)\n",
        "    max_bin = int(max_bin)\n",
        "    n_estimators = int(n_estimators)\n",
        "\n",
        "\n",
        "    params = {\n",
        "        'objective': 'regression',\n",
        "        'boosting_type': 'gbdt',\n",
        "        'learning_rate': learning_rate,\n",
        "        'num_leaves': num_leaves,\n",
        "        'max_depth': max_depth,\n",
        "        'min_child_samples': min_child_samples,\n",
        "        'min_child_weight': min_child_weight,\n",
        "        'subsample': subsample,\n",
        "        'colsample_bytree': colsample_bytree,\n",
        "        'reg_alpha': reg_alpha,\n",
        "        'reg_lambda': reg_lambda,\n",
        "        'max_bin': max_bin,\n",
        "        'metric': 'rmse',\n",
        "        'device': 'gpu',\n",
        "        'verbose': -1\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    callbacks = [lgb.log_evaluation(period=n_estimators)]\n",
        "\n",
        "    model = lgb.train(\n",
        "        params,\n",
        "        train_pool,\n",
        "        num_boost_round=n_estimators,\n",
        "        valid_sets=[train_pool],\n",
        "        callbacks=callbacks\n",
        "    )\n",
        "\n",
        "\n",
        "    families_models.set_family('AUTOMOTIVE', model, input_func_1, output_func_1)\n",
        "    pred_frame, common_loss = families_models.predict_family(valid, 'AUTOMOTIVE', True, verbose=False)\n",
        "\n",
        "    return -common_loss\n",
        "\n",
        "optimizer = BayesianOptimization(\n",
        "    f=objective,\n",
        "    pbounds=pbounds_lgb,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "optimizer.maximize(init_points=5, n_iter=20)\n",
        "\n",
        "best_params = optimizer.max['params']\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2H6sSrVDX6yK",
        "outputId": "17dd0910-a522-4780-ec3e-f7cb947fe0cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|   iter    |  target   | learni... | max_depth | num_le... | min_ch... | min_ch... | subsample | colsam... | reg_alpha | reg_la... |  max_bin  | n_esti... |\n",
            "-------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "[530]\ttraining's rmse: 0.360314\n",
            "| \u001b[39m1        \u001b[39m | \u001b[39m-0.523502\u001b[39m | \u001b[39m0.0811626\u001b[39m | \u001b[39m11.605714\u001b[39m | \u001b[39m191.67854\u001b[39m | \u001b[39m31.939631\u001b[39m | \u001b[39m1.5610303\u001b[39m | \u001b[39m0.5779972\u001b[39m | \u001b[39m0.5290418\u001b[39m | \u001b[39m8.6617614\u001b[39m | \u001b[39m6.0111501\u001b[39m | \u001b[39m199.24186\u001b[39m | \u001b[39m530.87674\u001b[39m |\n",
            "[709]\ttraining's rmse: 0.318923\n",
            "| \u001b[39m2        \u001b[39m | \u001b[39m-0.531266\u001b[39m | \u001b[39m0.1942828\u001b[39m | \u001b[39m10.659541\u001b[39m | \u001b[39m66.961386\u001b[39m | \u001b[39m13.182123\u001b[39m | \u001b[39m1.8348616\u001b[39m | \u001b[39m0.6521211\u001b[39m | \u001b[39m0.7623782\u001b[39m | \u001b[39m4.3194501\u001b[39m | \u001b[39m2.9122914\u001b[39m | \u001b[39m180.86390\u001b[39m | \u001b[39m709.24079\u001b[39m |\n",
            "[597]\ttraining's rmse: 0.45297\n",
            "| \u001b[35m3        \u001b[39m | \u001b[35m-0.515164\u001b[39m | \u001b[35m0.0655074\u001b[39m | \u001b[35m6.9308947\u001b[39m | \u001b[35m125.45679\u001b[39m | \u001b[35m40.332918\u001b[39m | \u001b[35m1.9975381\u001b[39m | \u001b[35m0.7571172\u001b[39m | \u001b[35m0.7962072\u001b[39m | \u001b[35m0.4645041\u001b[39m | \u001b[35m6.0754485\u001b[39m | \u001b[35m96.570107\u001b[39m | \u001b[35m597.57738\u001b[39m |\n",
            "[1863]\ttraining's rmse: 0.043395\n",
            "| \u001b[39m4        \u001b[39m | \u001b[39m-0.531844\u001b[39m | \u001b[39m0.1902882\u001b[39m | \u001b[39m11.725056\u001b[39m | \u001b[39m210.01536\u001b[39m | \u001b[39m18.707619\u001b[39m | \u001b[39m0.9776234\u001b[39m | \u001b[39m0.8421165\u001b[39m | \u001b[39m0.7200762\u001b[39m | \u001b[39m1.2203823\u001b[39m | \u001b[39m4.9517691\u001b[39m | \u001b[39m70.568207\u001b[39m | \u001b[39m1863.9806\u001b[39m |\n",
            "[1396]\ttraining's rmse: 0.352448\n",
            "| \u001b[39m5        \u001b[39m | \u001b[39m-0.517743\u001b[39m | \u001b[39m0.0591681\u001b[39m | \u001b[39m9.3001782\u001b[39m | \u001b[39m90.810658\u001b[39m | \u001b[39m28.403060\u001b[39m | \u001b[39m5.4675560\u001b[39m | \u001b[39m0.5924272\u001b[39m | \u001b[39m0.9847923\u001b[39m | \u001b[39m7.7513282\u001b[39m | \u001b[39m9.3949894\u001b[39m | \u001b[39m234.91202\u001b[39m | \u001b[39m1396.8499\u001b[39m |\n",
            "[1398]\ttraining's rmse: 0.43157\n",
            "| \u001b[35m6        \u001b[39m | \u001b[35m-0.513391\u001b[39m | \u001b[35m0.0227966\u001b[39m | \u001b[35m8.8609886\u001b[39m | \u001b[35m90.527096\u001b[39m | \u001b[35m22.085474\u001b[39m | \u001b[35m1.3861815\u001b[39m | \u001b[35m0.9386620\u001b[39m | \u001b[35m0.6042688\u001b[39m | \u001b[35m5.2240700\u001b[39m | \u001b[35m5.8220352\u001b[39m | \u001b[35m241.20895\u001b[39m | \u001b[35m1398.9884\u001b[39m |\n",
            "[1397]\ttraining's rmse: 0.292392\n",
            "| \u001b[39m7        \u001b[39m | \u001b[39m-0.526604\u001b[39m | \u001b[39m0.1077474\u001b[39m | \u001b[39m8.2118999\u001b[39m | \u001b[39m83.143495\u001b[39m | \u001b[39m17.728674\u001b[39m | \u001b[39m1.8044317\u001b[39m | \u001b[39m0.6707325\u001b[39m | \u001b[39m0.5716150\u001b[39m | \u001b[39m4.3644472\u001b[39m | \u001b[39m1.3348970\u001b[39m | \u001b[39m247.87360\u001b[39m | \u001b[39m1397.6658\u001b[39m |\n",
            "[1399]\ttraining's rmse: 0.467666\n",
            "| \u001b[35m8        \u001b[39m | \u001b[35m-0.512247\u001b[39m | \u001b[35m0.01     \u001b[39m | \u001b[35m8.8911580\u001b[39m | \u001b[35m95.972426\u001b[39m | \u001b[35m24.230966\u001b[39m | \u001b[35m0.9799491\u001b[39m | \u001b[35m1.0      \u001b[39m | \u001b[35m0.5405808\u001b[39m | \u001b[35m5.5100850\u001b[39m | \u001b[35m8.0235913\u001b[39m | \u001b[35m238.99241\u001b[39m | \u001b[35m1399.7856\u001b[39m |\n",
            "[1407]\ttraining's rmse: 0.45938\n",
            "| \u001b[35m9        \u001b[39m | \u001b[35m-0.511220\u001b[39m | \u001b[35m0.01     \u001b[39m | \u001b[35m11.292387\u001b[39m | \u001b[35m93.764575\u001b[39m | \u001b[35m18.795627\u001b[39m | \u001b[35m0.001    \u001b[39m | \u001b[35m1.0      \u001b[39m | \u001b[35m0.5      \u001b[39m | \u001b[35m2.3003900\u001b[39m | \u001b[35m6.2844358\u001b[39m | \u001b[35m233.38369\u001b[39m | \u001b[35m1407.6911\u001b[39m |\n",
            "[1397]\ttraining's rmse: 0.460145\n",
            "| \u001b[39m10       \u001b[39m | \u001b[39m-0.512665\u001b[39m | \u001b[39m0.01     \u001b[39m | \u001b[39m8.7461133\u001b[39m | \u001b[39m102.22492\u001b[39m | \u001b[39m11.914430\u001b[39m | \u001b[39m0.001    \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m1.9159498\u001b[39m | \u001b[39m3.3291964\u001b[39m | \u001b[39m231.88905\u001b[39m | \u001b[39m1397.6756\u001b[39m |\n",
            "[1412]\ttraining's rmse: 0.504339\n",
            "| \u001b[39m11       \u001b[39m | \u001b[39m-0.513097\u001b[39m | \u001b[39m0.01     \u001b[39m | \u001b[39m4.0      \u001b[39m | \u001b[39m105.81652\u001b[39m | \u001b[39m16.905090\u001b[39m | \u001b[39m0.001    \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m0.5153858\u001b[39m | \u001b[39m6.6676282\u001b[39m | \u001b[39m2.0066939\u001b[39m | \u001b[39m239.14921\u001b[39m | \u001b[39m1412.6404\u001b[39m |\n",
            "[1412]\ttraining's rmse: 0.347089\n",
            "| \u001b[39m12       \u001b[39m | \u001b[39m-0.542942\u001b[39m | \u001b[39m0.2      \u001b[39m | \u001b[39m5.7272369\u001b[39m | \u001b[39m104.51368\u001b[39m | \u001b[39m19.234097\u001b[39m | \u001b[39m0.001    \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m0.6382217\u001b[39m | \u001b[39m1.4105569\u001b[39m | \u001b[39m0.0      \u001b[39m | \u001b[39m218.85245\u001b[39m | \u001b[39m1412.2588\u001b[39m |\n",
            "[1405]\ttraining's rmse: 0.460413\n",
            "| \u001b[39m13       \u001b[39m | \u001b[39m-0.514548\u001b[39m | \u001b[39m0.01     \u001b[39m | \u001b[39m12.0     \u001b[39m | \u001b[39m100.30128\u001b[39m | \u001b[39m11.319827\u001b[39m | \u001b[39m0.001    \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m5.1405668\u001b[39m | \u001b[39m10.0     \u001b[39m | \u001b[39m242.39784\u001b[39m | \u001b[39m1405.7541\u001b[39m |\n",
            "[1415]\ttraining's rmse: 0.45911\n",
            "| \u001b[39m14       \u001b[39m | \u001b[39m-0.513447\u001b[39m | \u001b[39m0.01     \u001b[39m | \u001b[39m11.639192\u001b[39m | \u001b[39m95.754174\u001b[39m | \u001b[39m25.547456\u001b[39m | \u001b[39m0.001    \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m6.0409797\u001b[39m | \u001b[39m6.1131136\u001b[39m | \u001b[39m244.55150\u001b[39m | \u001b[39m1415.2104\u001b[39m |\n",
            "[1403]\ttraining's rmse: 0.447252\n",
            "| \u001b[39m15       \u001b[39m | \u001b[39m-0.514735\u001b[39m | \u001b[39m0.01     \u001b[39m | \u001b[39m12.0     \u001b[39m | \u001b[39m104.52347\u001b[39m | \u001b[39m20.817198\u001b[39m | \u001b[39m6.1967706\u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m0.5826563\u001b[39m | \u001b[39m0.0      \u001b[39m | \u001b[39m0.0      \u001b[39m | \u001b[39m244.39800\u001b[39m | \u001b[39m1403.7209\u001b[39m |\n",
            "[1414]\ttraining's rmse: 0.279861\n",
            "| \u001b[39m16       \u001b[39m | \u001b[39m-0.534827\u001b[39m | \u001b[39m0.1063292\u001b[39m | \u001b[39m9.7608587\u001b[39m | \u001b[39m98.998802\u001b[39m | \u001b[39m36.220693\u001b[39m | \u001b[39m5.4491237\u001b[39m | \u001b[39m0.7785671\u001b[39m | \u001b[39m0.6114200\u001b[39m | \u001b[39m6.2527645\u001b[39m | \u001b[39m3.9195768\u001b[39m | \u001b[39m245.39755\u001b[39m | \u001b[39m1414.3529\u001b[39m |\n",
            "[1406]\ttraining's rmse: 0.461521\n",
            "| \u001b[39m17       \u001b[39m | \u001b[39m-0.514606\u001b[39m | \u001b[39m0.01     \u001b[39m | \u001b[39m12.0     \u001b[39m | \u001b[39m97.027014\u001b[39m | \u001b[39m17.299084\u001b[39m | \u001b[39m0.001    \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m10.0     \u001b[39m | \u001b[39m0.0      \u001b[39m | \u001b[39m239.55720\u001b[39m | \u001b[39m1406.6683\u001b[39m |\n",
            "[1395]\ttraining's rmse: 0.295208\n",
            "| \u001b[39m18       \u001b[39m | \u001b[39m-0.532670\u001b[39m | \u001b[39m0.1978348\u001b[39m | \u001b[39m6.3807494\u001b[39m | \u001b[39m106.13676\u001b[39m | \u001b[39m18.187251\u001b[39m | \u001b[39m7.2144626\u001b[39m | \u001b[39m0.8579989\u001b[39m | \u001b[39m0.7288169\u001b[39m | \u001b[39m7.2080747\u001b[39m | \u001b[39m4.3540218\u001b[39m | \u001b[39m228.64076\u001b[39m | \u001b[39m1395.0750\u001b[39m |\n",
            "[1408]\ttraining's rmse: 0.502701\n",
            "| \u001b[39m19       \u001b[39m | \u001b[39m-0.514470\u001b[39m | \u001b[39m0.01     \u001b[39m | \u001b[39m4.0      \u001b[39m | \u001b[39m96.434095\u001b[39m | \u001b[39m17.885604\u001b[39m | \u001b[39m0.001    \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m0.0      \u001b[39m | \u001b[39m6.0601935\u001b[39m | \u001b[39m242.40125\u001b[39m | \u001b[39m1408.7438\u001b[39m |\n",
            "[600]\ttraining's rmse: 0.28305\n",
            "| \u001b[39m20       \u001b[39m | \u001b[39m-0.547480\u001b[39m | \u001b[39m0.2      \u001b[39m | \u001b[39m9.1682337\u001b[39m | \u001b[39m129.86713\u001b[39m | \u001b[39m40.705314\u001b[39m | \u001b[39m3.9498599\u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m10.0     \u001b[39m | \u001b[39m0.0      \u001b[39m | \u001b[39m85.913760\u001b[39m | \u001b[39m600.52178\u001b[39m |\n",
            "[1420]\ttraining's rmse: 0.45848\n",
            "| \u001b[35m21       \u001b[39m | \u001b[35m-0.509898\u001b[39m | \u001b[35m0.01     \u001b[39m | \u001b[35m12.0     \u001b[39m | \u001b[35m97.126280\u001b[39m | \u001b[35m13.566206\u001b[39m | \u001b[35m0.001    \u001b[39m | \u001b[35m1.0      \u001b[39m | \u001b[35m0.5701678\u001b[39m | \u001b[35m4.5168817\u001b[39m | \u001b[35m5.2173880\u001b[39m | \u001b[35m242.17865\u001b[39m | \u001b[35m1420.4862\u001b[39m |\n",
            "[1386]\ttraining's rmse: 0.257288\n",
            "| \u001b[39m22       \u001b[39m | \u001b[39m-0.516407\u001b[39m | \u001b[39m0.0885820\u001b[39m | \u001b[39m11.455418\u001b[39m | \u001b[39m94.842958\u001b[39m | \u001b[39m29.150585\u001b[39m | \u001b[39m7.7274850\u001b[39m | \u001b[39m0.5609460\u001b[39m | \u001b[39m0.8715845\u001b[39m | \u001b[39m0.4456480\u001b[39m | \u001b[39m6.9674159\u001b[39m | \u001b[39m244.51196\u001b[39m | \u001b[39m1386.2673\u001b[39m |\n",
            "[1406]\ttraining's rmse: 0.313282\n",
            "| \u001b[39m23       \u001b[39m | \u001b[39m-0.563760\u001b[39m | \u001b[39m0.1289654\u001b[39m | \u001b[39m6.6966021\u001b[39m | \u001b[39m93.153939\u001b[39m | \u001b[39m17.435896\u001b[39m | \u001b[39m8.3706527\u001b[39m | \u001b[39m0.5206639\u001b[39m | \u001b[39m0.9325242\u001b[39m | \u001b[39m1.0342299\u001b[39m | \u001b[39m9.6657793\u001b[39m | \u001b[39m231.21439\u001b[39m | \u001b[39m1406.3810\u001b[39m |\n",
            "[1401]\ttraining's rmse: 0.200983\n",
            "| \u001b[39m24       \u001b[39m | \u001b[39m-0.540111\u001b[39m | \u001b[39m0.1800528\u001b[39m | \u001b[39m8.3533262\u001b[39m | \u001b[39m97.553879\u001b[39m | \u001b[39m16.454558\u001b[39m | \u001b[39m6.9549037\u001b[39m | \u001b[39m0.7628457\u001b[39m | \u001b[39m0.7622691\u001b[39m | \u001b[39m4.7036779\u001b[39m | \u001b[39m6.7904537\u001b[39m | \u001b[39m243.73043\u001b[39m | \u001b[39m1401.7401\u001b[39m |\n",
            "[1398]\ttraining's rmse: 0.465172\n",
            "| \u001b[35m25       \u001b[39m | \u001b[35m-0.507918\u001b[39m | \u001b[35m0.01     \u001b[39m | \u001b[35m9.4713823\u001b[39m | \u001b[35m91.716595\u001b[39m | \u001b[35m25.227834\u001b[39m | \u001b[35m1.7019235\u001b[39m | \u001b[35m0.8819046\u001b[39m | \u001b[35m0.6867278\u001b[39m | \u001b[35m6.2317801\u001b[39m | \u001b[35m7.4000712\u001b[39m | \u001b[35m237.90513\u001b[39m | \u001b[35m1398.7770\u001b[39m |\n",
            "=============================================================================================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gamma"
      ],
      "metadata": {
        "id": "NczsSPPefH8Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pbounds_lgb = {\n",
        "    'learning_rate': (0.01, 0.2),\n",
        "    'max_depth': (4, 12),\n",
        "    'num_leaves': (16, 256),\n",
        "    'min_child_samples': (5, 50),\n",
        "    'min_child_weight': (1e-3, 10.0),\n",
        "    'subsample': (0.5, 1.0),\n",
        "    'colsample_bytree': (0.5, 1.0),\n",
        "    'reg_alpha': (0, 10),\n",
        "    'reg_lambda': (0, 10),\n",
        "    'max_bin': (64, 255),\n",
        "    'n_estimators': (500, 2000)\n",
        "}\n",
        "\n",
        "def objective(learning_rate, max_depth, num_leaves, min_child_samples,\n",
        "              min_child_weight, subsample, colsample_bytree,\n",
        "              reg_alpha, reg_lambda, max_bin, n_estimators):\n",
        "\n",
        "    train_pool = lgb.Dataset(X_1, label=y + 1, categorical_feature=cat_features)\n",
        "\n",
        "    max_depth = int(max_depth)\n",
        "    num_leaves = int(num_leaves)\n",
        "    min_child_samples = int(min_child_samples)\n",
        "    max_bin = int(max_bin)\n",
        "    n_estimators = int(n_estimators)\n",
        "\n",
        "    params = {\n",
        "        'objective': 'gamma',\n",
        "        'boosting_type': 'gbdt',\n",
        "        'learning_rate': learning_rate,\n",
        "        'num_leaves': num_leaves,\n",
        "        'max_depth': max_depth,\n",
        "        'min_child_samples': min_child_samples,\n",
        "        'min_child_weight': min_child_weight,\n",
        "        'subsample': subsample,\n",
        "        'colsample_bytree': colsample_bytree,\n",
        "        'reg_alpha': reg_alpha,\n",
        "        'reg_lambda': reg_lambda,\n",
        "        'max_bin': max_bin,\n",
        "        'metric': 'gamma',\n",
        "        'device': 'gpu',\n",
        "        'verbose': -1\n",
        "    }\n",
        "\n",
        "    callbacks = [lgb.log_evaluation(period=n_estimators)]\n",
        "\n",
        "    model = lgb.train(\n",
        "        params,\n",
        "        train_pool,\n",
        "        num_boost_round=n_estimators,\n",
        "        valid_sets=[train_pool],\n",
        "        callbacks=callbacks\n",
        "    )\n",
        "\n",
        "    families_models.set_family('AUTOMOTIVE', model, input_func_1, output_func_2)\n",
        "    pred_frame, common_loss = families_models.predict_family(valid, 'AUTOMOTIVE', True, verbose=False)\n",
        "\n",
        "    return -common_loss\n",
        "\n",
        "optimizer = BayesianOptimization(\n",
        "    f=objective,\n",
        "    pbounds=pbounds_lgb,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "optimizer.maximize(init_points=5, n_iter=20)\n",
        "\n",
        "best_params = optimizer.max['params']\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gu0_45D1lJrm",
        "outputId": "9897d4dc-775d-4048-d9f4-dda8fdfb15b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|   iter    |  target   | learni... | max_depth | num_le... | min_ch... | min_ch... | subsample | colsam... | reg_alpha | reg_la... |  max_bin  | n_esti... |\n",
            "-------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "[530]\ttraining's gamma: 2.66957\n",
            "| \u001b[39m1        \u001b[39m | \u001b[39m-0.508181\u001b[39m | \u001b[39m0.0811626\u001b[39m | \u001b[39m11.605714\u001b[39m | \u001b[39m191.67854\u001b[39m | \u001b[39m31.939631\u001b[39m | \u001b[39m1.5610303\u001b[39m | \u001b[39m0.5779972\u001b[39m | \u001b[39m0.5290418\u001b[39m | \u001b[39m8.6617614\u001b[39m | \u001b[39m6.0111501\u001b[39m | \u001b[39m199.24186\u001b[39m | \u001b[39m530.87674\u001b[39m |\n",
            "[709]\ttraining's gamma: 2.65686\n",
            "| \u001b[39m2        \u001b[39m | \u001b[39m-0.524492\u001b[39m | \u001b[39m0.1942828\u001b[39m | \u001b[39m10.659541\u001b[39m | \u001b[39m66.961386\u001b[39m | \u001b[39m13.182123\u001b[39m | \u001b[39m1.8348616\u001b[39m | \u001b[39m0.6521211\u001b[39m | \u001b[39m0.7623782\u001b[39m | \u001b[39m4.3194501\u001b[39m | \u001b[39m2.9122914\u001b[39m | \u001b[39m180.86390\u001b[39m | \u001b[39m709.24079\u001b[39m |\n",
            "[597]\ttraining's gamma: 2.70215\n",
            "| \u001b[39m3        \u001b[39m | \u001b[39m-0.510532\u001b[39m | \u001b[39m0.0655074\u001b[39m | \u001b[39m6.9308947\u001b[39m | \u001b[39m125.45679\u001b[39m | \u001b[39m40.332918\u001b[39m | \u001b[39m1.9975381\u001b[39m | \u001b[39m0.7571172\u001b[39m | \u001b[39m0.7962072\u001b[39m | \u001b[39m0.4645041\u001b[39m | \u001b[39m6.0754485\u001b[39m | \u001b[39m96.570107\u001b[39m | \u001b[39m597.57738\u001b[39m |\n",
            "[1863]\ttraining's gamma: 2.60672\n",
            "| \u001b[39m4        \u001b[39m | \u001b[39m-0.537804\u001b[39m | \u001b[39m0.1902882\u001b[39m | \u001b[39m11.725056\u001b[39m | \u001b[39m210.01536\u001b[39m | \u001b[39m18.707619\u001b[39m | \u001b[39m0.9776234\u001b[39m | \u001b[39m0.8421165\u001b[39m | \u001b[39m0.7200762\u001b[39m | \u001b[39m1.2203823\u001b[39m | \u001b[39m4.9517691\u001b[39m | \u001b[39m70.568207\u001b[39m | \u001b[39m1863.9806\u001b[39m |\n",
            "[1396]\ttraining's gamma: 2.66702\n",
            "| \u001b[35m5        \u001b[39m | \u001b[35m-0.507353\u001b[39m | \u001b[35m0.0591681\u001b[39m | \u001b[35m9.3001782\u001b[39m | \u001b[35m90.810658\u001b[39m | \u001b[35m28.403060\u001b[39m | \u001b[35m5.4675560\u001b[39m | \u001b[35m0.5924272\u001b[39m | \u001b[35m0.9847923\u001b[39m | \u001b[35m7.7513282\u001b[39m | \u001b[35m9.3949894\u001b[39m | \u001b[35m234.91202\u001b[39m | \u001b[35m1396.8499\u001b[39m |\n",
            "[1398]\ttraining's gamma: 2.69393\n",
            "| \u001b[35m6        \u001b[39m | \u001b[35m-0.506637\u001b[39m | \u001b[35m0.0227966\u001b[39m | \u001b[35m8.8609886\u001b[39m | \u001b[35m90.527096\u001b[39m | \u001b[35m22.085474\u001b[39m | \u001b[35m1.3861815\u001b[39m | \u001b[35m0.9386620\u001b[39m | \u001b[35m0.6042688\u001b[39m | \u001b[35m5.2240700\u001b[39m | \u001b[35m5.8220352\u001b[39m | \u001b[35m241.20895\u001b[39m | \u001b[35m1398.9884\u001b[39m |\n",
            "[1238]\ttraining's gamma: 2.70508\n",
            "| \u001b[39m7        \u001b[39m | \u001b[39m-0.510914\u001b[39m | \u001b[39m0.0538862\u001b[39m | \u001b[39m5.2278187\u001b[39m | \u001b[39m247.76362\u001b[39m | \u001b[39m17.704016\u001b[39m | \u001b[39m1.5751873\u001b[39m | \u001b[39m0.8810567\u001b[39m | \u001b[39m0.5095684\u001b[39m | \u001b[39m3.1484241\u001b[39m | \u001b[39m9.0843732\u001b[39m | \u001b[39m250.39710\u001b[39m | \u001b[39m1238.4669\u001b[39m |\n",
            "[1161]\ttraining's gamma: 2.72357\n",
            "| \u001b[39m8        \u001b[39m | \u001b[39m-0.512944\u001b[39m | \u001b[39m0.0156450\u001b[39m | \u001b[39m4.4081529\u001b[39m | \u001b[39m29.326241\u001b[39m | \u001b[39m9.2967055\u001b[39m | \u001b[39m3.4928786\u001b[39m | \u001b[39m0.6618913\u001b[39m | \u001b[39m0.8277501\u001b[39m | \u001b[39m7.5240130\u001b[39m | \u001b[39m4.1188917\u001b[39m | \u001b[39m251.11331\u001b[39m | \u001b[39m1161.1025\u001b[39m |\n",
            "[500]\ttraining's gamma: 2.72907\n",
            "| \u001b[39m9        \u001b[39m | \u001b[39m-0.512784\u001b[39m | \u001b[39m0.01     \u001b[39m | \u001b[39m4.2327796\u001b[39m | \u001b[39m256.0    \u001b[39m | \u001b[39m5.0      \u001b[39m | \u001b[39m6.5874305\u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m2.4070564\u001b[39m | \u001b[39m0.0      \u001b[39m | \u001b[39m64.0     \u001b[39m | \u001b[39m500.0    \u001b[39m |\n",
            "[1485]\ttraining's gamma: 2.63781\n",
            "| \u001b[39m10       \u001b[39m | \u001b[39m-0.515143\u001b[39m | \u001b[39m0.1122666\u001b[39m | \u001b[39m7.3763864\u001b[39m | \u001b[39m254.56410\u001b[39m | \u001b[39m13.555667\u001b[39m | \u001b[39m1.2647590\u001b[39m | \u001b[39m0.5267668\u001b[39m | \u001b[39m0.8989953\u001b[39m | \u001b[39m3.1707887\u001b[39m | \u001b[39m4.5908235\u001b[39m | \u001b[39m236.85777\u001b[39m | \u001b[39m1485.1753\u001b[39m |\n",
            "[1399]\ttraining's gamma: 2.65633\n",
            "| \u001b[39m11       \u001b[39m | \u001b[39m-0.510825\u001b[39m | \u001b[39m0.0864399\u001b[39m | \u001b[39m11.738219\u001b[39m | \u001b[39m91.521118\u001b[39m | \u001b[39m33.191385\u001b[39m | \u001b[39m0.0450871\u001b[39m | \u001b[39m0.7695034\u001b[39m | \u001b[39m0.6537122\u001b[39m | \u001b[39m7.8647923\u001b[39m | \u001b[39m2.4236921\u001b[39m | \u001b[39m228.95829\u001b[39m | \u001b[39m1399.1802\u001b[39m |\n",
            "[1343]\ttraining's gamma: 2.6897\n",
            "| \u001b[39m12       \u001b[39m | \u001b[39m-0.519142\u001b[39m | \u001b[39m0.2      \u001b[39m | \u001b[39m4.0      \u001b[39m | \u001b[39m119.85007\u001b[39m | \u001b[39m5.0      \u001b[39m | \u001b[39m10.0     \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m0.9999150\u001b[39m | \u001b[39m10.0     \u001b[39m | \u001b[39m255.0    \u001b[39m | \u001b[39m1343.4287\u001b[39m |\n",
            "[1403]\ttraining's gamma: 2.72498\n",
            "| \u001b[39m13       \u001b[39m | \u001b[39m-0.514313\u001b[39m | \u001b[39m0.01     \u001b[39m | \u001b[39m4.0      \u001b[39m | \u001b[39m42.885521\u001b[39m | \u001b[39m10.564279\u001b[39m | \u001b[39m10.0     \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m10.0     \u001b[39m | \u001b[39m10.0     \u001b[39m | \u001b[39m255.0    \u001b[39m | \u001b[39m1403.6813\u001b[39m |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_pool = lgb.Dataset(X_1, label=y + 1, categorical_feature=cat_features)\n",
        "params = {\n",
        "        'objective': 'gamma',\n",
        "        'boosting_type': 'gbdt',\n",
        "        'learning_rate': 0.0750216,\n",
        "        'num_leaves': 17,\n",
        "        'max_depth':  7,\n",
        "        'min_child_samples': 44,\n",
        "        'min_child_weight': 7.1179489,\n",
        "        'subsample': 0.8976433 ,\n",
        "        'colsample_bytree': 0.9186422 ,\n",
        "        'reg_alpha': 8.0243414 ,\n",
        "        'reg_lambda': 7.9912505 ,\n",
        "        'max_bin': 67,\n",
        "        'metric': 'gamma',\n",
        "        # 'device': 'gpu',\n",
        "        'verbose': -1\n",
        "    }\n",
        "\n",
        "callbacks = [lgb.log_evaluation(period=1446)]\n",
        "lgb_model = lgb.train(\n",
        "        params,\n",
        "        train_pool,\n",
        "        num_boost_round=1446,\n",
        "        valid_sets=[train_pool],\n",
        "        callbacks=callbacks\n",
        ")"
      ],
      "metadata": {
        "id": "7VuQcpICS2bq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aceb9769-0bb5-444e-d8b1-451c424e218c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1446]\ttraining's gamma: 2.70423\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gamma + RMSLE"
      ],
      "metadata": {
        "id": "xpKEeJRno3TD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from lightgbm import LGBMRegressor"
      ],
      "metadata": {
        "id": "XW1uDzZ4ygy0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gamma_rsmle_lgb_loss(y_true, y_pred, alpha=0.5):\n",
        "  y_pred += 1\n",
        "  gamma_grad = (y_pred - y_true) / y_pred ** 2\n",
        "\n",
        "  gamma_hess = y_true / y_pred ** 2 # inaccurate formula - due to the lgb requirement for a positive hessian\n",
        "\n",
        "  log_pred  = np.log(y_pred)\n",
        "  log_true = np.log(y_true)\n",
        "  diff= log_pred - log_true\n",
        "\n",
        "  rmsle_grad =  diff / y_pred\n",
        "  rmsle_hess= -1 * diff / (y_pred ** 2)\n",
        "\n",
        "  # print(np.abs(gamma_grad).sum(), np.abs(rmsle_grad).sum())\n",
        "\n",
        "  grad = alpha * gamma_grad + rmsle_grad\n",
        "  hess = alpha * gamma_hess + rmsle_hess\n",
        "\n",
        "\n",
        "  return grad, hess\n",
        "\n",
        "pbounds_lgb = {\n",
        "    'learning_rate': (0.01, 0.2),\n",
        "    'max_depth': (4, 12),\n",
        "    'num_leaves': (16, 256),\n",
        "    'min_child_samples': (5, 50),\n",
        "    'min_child_weight': (1e-3, 10.0),\n",
        "    'subsample': (0.5, 1.0),\n",
        "    'colsample_bytree': (0.5, 1.0),\n",
        "    'reg_alpha': (0, 10),\n",
        "    'reg_lambda': (0, 10),\n",
        "    'max_bin': (64, 255),\n",
        "    'n_estimators': (500, 2000),\n",
        "    'alpha': (0.0, 1.0),\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "def create_custom_objective(alpha):\n",
        "  def custom_objective(y_true, y_pred):\n",
        "      return gamma_rsmle_lgb_loss(y_true, y_pred, alpha)\n",
        "  return custom_objective\n",
        "\n",
        "\n",
        "def objective(learning_rate, max_depth, num_leaves, min_child_samples,\n",
        "              min_child_weight, subsample, colsample_bytree,\n",
        "              reg_alpha, reg_lambda, max_bin, n_estimators, alpha):\n",
        "\n",
        "\n",
        "    max_depth = int(max_depth)\n",
        "    num_leaves = int(num_leaves)\n",
        "    min_child_samples = int(min_child_samples)\n",
        "    max_bin = int(max_bin)\n",
        "    n_estimators = int(n_estimators)\n",
        "\n",
        "\n",
        "    model = LGBMRegressor(\n",
        "            boosting_type='gbdt',\n",
        "            objective=create_custom_objective(0),\n",
        "            learning_rate=learning_rate,\n",
        "            num_leaves=num_leaves,\n",
        "            max_depth=max_depth,\n",
        "            min_child_samples=min_child_samples,\n",
        "            min_child_weight=min_child_weight,\n",
        "            subsample=subsample,\n",
        "            colsample_bytree=colsample_bytree,\n",
        "            reg_alpha=reg_alpha,\n",
        "            reg_lambda=reg_lambda,\n",
        "            max_bin=max_bin,\n",
        "            n_estimators=n_estimators,\n",
        "            device='gpu',\n",
        "            verbose=-1,\n",
        "            random_state=42\n",
        "        )\n",
        "    model.fit(X_1, y + 1, categorical_feature=cat_features)\n",
        "\n",
        "\n",
        "    families_models.set_family('AUTOMOTIVE', model, input_func_1, output_func_2)\n",
        "    pred_frame, common_loss = families_models.predict_family(valid, 'AUTOMOTIVE', True, verbose=False)\n",
        "\n",
        "    return -common_loss\n",
        "\n",
        "optimizer = BayesianOptimization(\n",
        "    f=objective,\n",
        "    pbounds=pbounds_lgb,\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "optimizer.maximize(init_points=5, n_iter=20)\n",
        "\n",
        "best_params = optimizer.max['params']\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gDhNKqI2qpJh",
        "outputId": "59bcd50b-0468-43c7-87ee-706c66436d23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|   iter    |  target   | learni... | max_depth | num_le... | min_ch... | min_ch... | subsample | colsam... | reg_alpha | reg_la... |  max_bin  | n_esti... |   alpha   |\n",
            "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "| \u001b[39m1        \u001b[39m | \u001b[39m-0.561956\u001b[39m | \u001b[39m0.0811626\u001b[39m | \u001b[39m11.605714\u001b[39m | \u001b[39m191.67854\u001b[39m | \u001b[39m31.939631\u001b[39m | \u001b[39m1.5610303\u001b[39m | \u001b[39m0.5779972\u001b[39m | \u001b[39m0.5290418\u001b[39m | \u001b[39m8.6617614\u001b[39m | \u001b[39m6.0111501\u001b[39m | \u001b[39m199.24186\u001b[39m | \u001b[39m530.87674\u001b[39m | \u001b[39m0.9699098\u001b[39m |\n",
            "| \u001b[35m2        \u001b[39m | \u001b[35m-0.535020\u001b[39m | \u001b[35m0.1681641\u001b[39m | \u001b[35m5.6987128\u001b[39m | \u001b[35m59.637992\u001b[39m | \u001b[35m13.253202\u001b[39m | \u001b[35m3.0431181\u001b[39m | \u001b[35m0.7623782\u001b[39m | \u001b[35m0.7159725\u001b[39m | \u001b[35m2.9122914\u001b[39m | \u001b[35m6.1185289\u001b[39m | \u001b[35m90.643327\u001b[39m | \u001b[35m938.21697\u001b[39m | \u001b[35m0.3663618\u001b[39m |\n",
            "| \u001b[39m3        \u001b[39m | \u001b[39m-0.545200\u001b[39m | \u001b[39m0.0966532\u001b[39m | \u001b[39m10.281407\u001b[39m | \u001b[39m63.921707\u001b[39m | \u001b[39m28.140549\u001b[39m | \u001b[39m5.9245532\u001b[39m | \u001b[39m0.5232252\u001b[39m | \u001b[39m0.8037724\u001b[39m | \u001b[39m1.7052412\u001b[39m | \u001b[39m0.6505159\u001b[39m | \u001b[39m245.23713\u001b[39m | \u001b[39m1948.4480\u001b[39m | \u001b[39m0.8083973\u001b[39m |\n",
            "| \u001b[39m4        \u001b[39m | \u001b[39m-0.554283\u001b[39m | \u001b[39m0.0678766\u001b[39m | \u001b[39m4.7813769\u001b[39m | \u001b[39m180.21592\u001b[39m | \u001b[39m24.806862\u001b[39m | \u001b[39m1.2212603\u001b[39m | \u001b[39m0.7475884\u001b[39m | \u001b[39m0.5171942\u001b[39m | \u001b[39m9.0932040\u001b[39m | \u001b[39m2.5877998\u001b[39m | \u001b[39m190.54175\u001b[39m | \u001b[39m967.56661\u001b[39m | \u001b[39m0.5200680\u001b[39m |\n",
            "| \u001b[39m5        \u001b[39m | \u001b[39m-0.540593\u001b[39m | \u001b[39m0.1138749\u001b[39m | \u001b[39m5.4788356\u001b[39m | \u001b[39m248.70031\u001b[39m | \u001b[39m39.880977\u001b[39m | \u001b[39m9.3950499\u001b[39m | \u001b[39m0.9474136\u001b[39m | \u001b[39m0.7989499\u001b[39m | \u001b[39m9.2187423\u001b[39m | \u001b[39m0.8849250\u001b[39m | \u001b[39m101.43272\u001b[39m | \u001b[39m567.84093\u001b[39m | \u001b[39m0.3253303\u001b[39m |\n",
            "| \u001b[39m6        \u001b[39m | \u001b[39m-0.751934\u001b[39m | \u001b[39m0.0183725\u001b[39m | \u001b[39m6.4325622\u001b[39m | \u001b[39m243.25250\u001b[39m | \u001b[39m43.682785\u001b[39m | \u001b[39m1.6042364\u001b[39m | \u001b[39m0.9114591\u001b[39m | \u001b[39m0.5062517\u001b[39m | \u001b[39m9.7536577\u001b[39m | \u001b[39m2.8398398\u001b[39m | \u001b[39m93.864971\u001b[39m | \u001b[39m571.15433\u001b[39m | \u001b[39m0.8245505\u001b[39m |\n",
            "| \u001b[39m7        \u001b[39m | \u001b[39m-0.545398\u001b[39m | \u001b[39m0.1788825\u001b[39m | \u001b[39m9.5076358\u001b[39m | \u001b[39m161.95333\u001b[39m | \u001b[39m35.851609\u001b[39m | \u001b[39m9.0603382\u001b[39m | \u001b[39m0.5697483\u001b[39m | \u001b[39m0.6674877\u001b[39m | \u001b[39m7.5562660\u001b[39m | \u001b[39m5.9207042\u001b[39m | \u001b[39m172.32126\u001b[39m | \u001b[39m807.69232\u001b[39m | \u001b[39m0.0952688\u001b[39m |\n",
            "| \u001b[39m8        \u001b[39m | \u001b[39m-0.541222\u001b[39m | \u001b[39m0.1842165\u001b[39m | \u001b[39m9.0205595\u001b[39m | \u001b[39m60.269143\u001b[39m | \u001b[39m20.853209\u001b[39m | \u001b[39m4.4291670\u001b[39m | \u001b[39m0.7429935\u001b[39m | \u001b[39m0.9597698\u001b[39m | \u001b[39m4.8659351\u001b[39m | \u001b[39m9.0661711\u001b[39m | \u001b[39m187.92311\u001b[39m | \u001b[39m1414.6526\u001b[39m | \u001b[39m0.0140201\u001b[39m |\n",
            "| \u001b[39m9        \u001b[39m | \u001b[39m-0.538752\u001b[39m | \u001b[39m0.1153452\u001b[39m | \u001b[39m6.8682718\u001b[39m | \u001b[39m57.268134\u001b[39m | \u001b[39m23.567588\u001b[39m | \u001b[39m7.8153215\u001b[39m | \u001b[39m0.9622977\u001b[39m | \u001b[39m0.8895236\u001b[39m | \u001b[39m7.4404627\u001b[39m | \u001b[39m8.8275662\u001b[39m | \u001b[39m183.23700\u001b[39m | \u001b[39m1410.0166\u001b[39m | \u001b[39m0.9888862\u001b[39m |\n",
            "| \u001b[35m10       \u001b[39m | \u001b[35m-0.533826\u001b[39m | \u001b[35m0.0878350\u001b[39m | \u001b[35m5.8598046\u001b[39m | \u001b[35m57.023771\u001b[39m | \u001b[35m15.113251\u001b[39m | \u001b[35m3.3707475\u001b[39m | \u001b[35m0.7944326\u001b[39m | \u001b[35m0.6243752\u001b[39m | \u001b[35m9.5999770\u001b[39m | \u001b[35m7.1510699\u001b[39m | \u001b[35m186.44856\u001b[39m | \u001b[35m1419.0467\u001b[39m | \u001b[35m0.9352488\u001b[39m |\n",
            "| \u001b[39m11       \u001b[39m | \u001b[39m-0.537227\u001b[39m | \u001b[39m0.0924907\u001b[39m | \u001b[39m4.8549818\u001b[39m | \u001b[39m56.975835\u001b[39m | \u001b[39m17.845892\u001b[39m | \u001b[39m5.9111025\u001b[39m | \u001b[39m0.8901749\u001b[39m | \u001b[39m0.7008772\u001b[39m | \u001b[39m9.6224527\u001b[39m | \u001b[39m6.6855193\u001b[39m | \u001b[39m187.15708\u001b[39m | \u001b[39m1412.2485\u001b[39m | \u001b[39m1.0      \u001b[39m |\n",
            "| \u001b[39m12       \u001b[39m | \u001b[39m-0.540867\u001b[39m | \u001b[39m0.1172919\u001b[39m | \u001b[39m7.1568277\u001b[39m | \u001b[39m56.049464\u001b[39m | \u001b[39m14.054167\u001b[39m | \u001b[39m3.2469212\u001b[39m | \u001b[39m0.8836415\u001b[39m | \u001b[39m0.8305703\u001b[39m | \u001b[39m3.9096311\u001b[39m | \u001b[39m8.5491733\u001b[39m | \u001b[39m176.80155\u001b[39m | \u001b[39m1413.8226\u001b[39m | \u001b[39m0.8891188\u001b[39m |\n",
            "| \u001b[39m13       \u001b[39m | \u001b[39m-0.535319\u001b[39m | \u001b[39m0.0848690\u001b[39m | \u001b[39m7.8702977\u001b[39m | \u001b[39m46.856200\u001b[39m | \u001b[39m20.091704\u001b[39m | \u001b[39m7.4813285\u001b[39m | \u001b[39m0.9366219\u001b[39m | \u001b[39m0.7806464\u001b[39m | \u001b[39m5.2134141\u001b[39m | \u001b[39m8.9960157\u001b[39m | \u001b[39m184.11225\u001b[39m | \u001b[39m1418.8070\u001b[39m | \u001b[39m0.8270262\u001b[39m |\n",
            "| \u001b[39m14       \u001b[39m | \u001b[39m-0.578342\u001b[39m | \u001b[39m0.1748933\u001b[39m | \u001b[39m9.7443223\u001b[39m | \u001b[39m69.089731\u001b[39m | \u001b[39m33.626155\u001b[39m | \u001b[39m4.1009603\u001b[39m | \u001b[39m0.9310903\u001b[39m | \u001b[39m0.5536558\u001b[39m | \u001b[39m0.3194874\u001b[39m | \u001b[39m3.1569498\u001b[39m | \u001b[39m251.51637\u001b[39m | \u001b[39m1954.0919\u001b[39m | \u001b[39m0.7026934\u001b[39m |\n",
            "| \u001b[39m15       \u001b[39m | \u001b[39m-0.534707\u001b[39m | \u001b[39m0.1452557\u001b[39m | \u001b[39m6.2739166\u001b[39m | \u001b[39m51.502737\u001b[39m | \u001b[39m8.9599257\u001b[39m | \u001b[39m0.8334879\u001b[39m | \u001b[39m0.7098548\u001b[39m | \u001b[39m0.9155134\u001b[39m | \u001b[39m6.5876393\u001b[39m | \u001b[39m8.7599173\u001b[39m | \u001b[39m170.16726\u001b[39m | \u001b[39m1415.2118\u001b[39m | \u001b[39m0.1274596\u001b[39m |\n",
            "| \u001b[39m16       \u001b[39m | \u001b[39m-0.546664\u001b[39m | \u001b[39m0.0345614\u001b[39m | \u001b[39m9.6511657\u001b[39m | \u001b[39m57.810195\u001b[39m | \u001b[39m12.411619\u001b[39m | \u001b[39m0.0555077\u001b[39m | \u001b[39m0.6825459\u001b[39m | \u001b[39m0.6317034\u001b[39m | \u001b[39m4.3052122\u001b[39m | \u001b[39m7.3053832\u001b[39m | \u001b[39m174.07406\u001b[39m | \u001b[39m1405.5063\u001b[39m | \u001b[39m0.6393402\u001b[39m |\n",
            "| \u001b[39m17       \u001b[39m | \u001b[39m-0.536105\u001b[39m | \u001b[39m0.0613437\u001b[39m | \u001b[39m7.5247152\u001b[39m | \u001b[39m60.351282\u001b[39m | \u001b[39m15.818701\u001b[39m | \u001b[39m2.4015138\u001b[39m | \u001b[39m0.7672055\u001b[39m | \u001b[39m0.5171279\u001b[39m | \u001b[39m9.4552814\u001b[39m | \u001b[39m4.6855392\u001b[39m | \u001b[39m192.71131\u001b[39m | \u001b[39m1415.3303\u001b[39m | \u001b[39m0.6972678\u001b[39m |\n",
            "| \u001b[39m18       \u001b[39m | \u001b[39m-0.668359\u001b[39m | \u001b[39m0.01     \u001b[39m | \u001b[39m4.0      \u001b[39m | \u001b[39m48.820258\u001b[39m | \u001b[39m22.676815\u001b[39m | \u001b[39m0.001    \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m10.0     \u001b[39m | \u001b[39m0.0      \u001b[39m | \u001b[39m174.37077\u001b[39m | \u001b[39m1419.3454\u001b[39m | \u001b[39m0.0      \u001b[39m |\n",
            "| \u001b[39m19       \u001b[39m | \u001b[39m-0.545913\u001b[39m | \u001b[39m0.1492859\u001b[39m | \u001b[39m12.0     \u001b[39m | \u001b[39m51.274374\u001b[39m | \u001b[39m9.0885538\u001b[39m | \u001b[39m8.5752116\u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m0.9685259\u001b[39m | \u001b[39m5.3497918\u001b[39m | \u001b[39m10.0     \u001b[39m | \u001b[39m187.53406\u001b[39m | \u001b[39m1414.3262\u001b[39m | \u001b[39m1.0      \u001b[39m |\n",
            "| \u001b[39m20       \u001b[39m | \u001b[39m-0.550160\u001b[39m | \u001b[39m0.1339846\u001b[39m | \u001b[39m8.0663400\u001b[39m | \u001b[39m55.125514\u001b[39m | \u001b[39m15.256042\u001b[39m | \u001b[39m7.7192393\u001b[39m | \u001b[39m0.6267661\u001b[39m | \u001b[39m0.7132984\u001b[39m | \u001b[39m5.2963962\u001b[39m | \u001b[39m5.7983761\u001b[39m | \u001b[39m176.00095\u001b[39m | \u001b[39m1403.8691\u001b[39m | \u001b[39m0.1727475\u001b[39m |\n",
            "| \u001b[39m21       \u001b[39m | \u001b[39m-0.538629\u001b[39m | \u001b[39m0.1692289\u001b[39m | \u001b[39m10.215381\u001b[39m | \u001b[39m59.067149\u001b[39m | \u001b[39m8.3058792\u001b[39m | \u001b[39m4.6134668\u001b[39m | \u001b[39m0.7483471\u001b[39m | \u001b[39m0.9855348\u001b[39m | \u001b[39m8.1734589\u001b[39m | \u001b[39m9.8826225\u001b[39m | \u001b[39m194.22311\u001b[39m | \u001b[39m1413.1015\u001b[39m | \u001b[39m0.9513252\u001b[39m |\n",
            "| \u001b[39m22       \u001b[39m | \u001b[39m-0.548871\u001b[39m | \u001b[39m0.0521924\u001b[39m | \u001b[39m4.0      \u001b[39m | \u001b[39m51.313549\u001b[39m | \u001b[39m18.285004\u001b[39m | \u001b[39m10.0     \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m0.7710292\u001b[39m | \u001b[39m6.5781057\u001b[39m | \u001b[39m10.0     \u001b[39m | \u001b[39m197.84612\u001b[39m | \u001b[39m1422.2015\u001b[39m | \u001b[39m1.0      \u001b[39m |\n",
            "| \u001b[39m23       \u001b[39m | \u001b[39m-0.543270\u001b[39m | \u001b[39m0.1034507\u001b[39m | \u001b[39m10.391577\u001b[39m | \u001b[39m58.631054\u001b[39m | \u001b[39m25.844377\u001b[39m | \u001b[39m6.2776982\u001b[39m | \u001b[39m0.7638231\u001b[39m | \u001b[39m0.6609507\u001b[39m | \u001b[39m3.1923829\u001b[39m | \u001b[39m9.9909535\u001b[39m | \u001b[39m176.49032\u001b[39m | \u001b[39m1414.6174\u001b[39m | \u001b[39m0.8589688\u001b[39m |\n",
            "| \u001b[39m24       \u001b[39m | \u001b[39m-0.536566\u001b[39m | \u001b[39m0.2      \u001b[39m | \u001b[39m4.3873189\u001b[39m | \u001b[39m254.97258\u001b[39m | \u001b[39m35.524884\u001b[39m | \u001b[39m10.0     \u001b[39m | \u001b[39m0.9873915\u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m8.5910003\u001b[39m | \u001b[39m0.0      \u001b[39m | \u001b[39m110.24837\u001b[39m | \u001b[39m564.05935\u001b[39m | \u001b[39m0.0      \u001b[39m |\n",
            "| \u001b[35m25       \u001b[39m | \u001b[35m-0.528236\u001b[39m | \u001b[35m0.1608027\u001b[39m | \u001b[35m5.2666844\u001b[39m | \u001b[35m66.476126\u001b[39m | \u001b[35m5.0      \u001b[39m | \u001b[35m8.0298602\u001b[39m | \u001b[35m1.0      \u001b[39m | \u001b[35m0.8033561\u001b[39m | \u001b[35m10.0     \u001b[39m | \u001b[35m10.0     \u001b[39m | \u001b[35m178.69269\u001b[39m | \u001b[35m1416.9610\u001b[39m | \u001b[35m1.0      \u001b[39m |\n",
            "=========================================================================================================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural Network"
      ],
      "metadata": {
        "id": "BWEMb7rdfKUc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorboard"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HfJGQtzLSuqQ",
        "outputId": "e728e520-1ff7-41b2-d296-d0b00331ae3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (1.74.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (3.8.2)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorboard) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (5.29.5)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (75.2.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.tensorboard import SummaryWriter"
      ],
      "metadata": {
        "id": "bG5X59A1TjrH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gamma"
      ],
      "metadata": {
        "id": "VH90V4sDUCYA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- MODEL ---\n",
        "\n",
        "class GammaNet1(nn.Module):\n",
        "  def __init__(self, input_dim, hidden_dims=[1]):\n",
        "    super().__init__()\n",
        "    layers = []\n",
        "    prev_dim = input_dim\n",
        "    for h in hidden_dims:\n",
        "      layers.append(nn.Linear(prev_dim, h))\n",
        "      layers.append(nn.LayerNorm(h))\n",
        "      layers.append(nn.ReLU())\n",
        "      prev_dim = h\n",
        "\n",
        "    self.hidden = nn.Sequential(*layers)\n",
        "\n",
        "    # gamma-distr params\n",
        "    self.shape_head = nn.Linear(prev_dim, 1)\n",
        "    self.rate_head = nn.Linear(prev_dim, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    h = self.hidden(x)\n",
        "    shape = F.softplus(self.shape_head(h)) + 1e-3\n",
        "    rate = F.softplus(self.rate_head(h)) + 1e-3\n",
        "\n",
        "\n",
        "    return shape, rate\n",
        "\n",
        "  def predict(self, X):\n",
        "    self.eval()\n",
        "    with torch.no_grad():\n",
        "      shape, rate = self(X)\n",
        "\n",
        "      y_pred = (shape / rate).squeeze(-1).cpu().numpy()\n",
        "    return y_pred\n",
        "\n",
        "\n",
        "\n",
        "# --- LOSS ---\n",
        "\n",
        "\n",
        "def gamma_nll(y_true, shape, rate):\n",
        "  dist = torch.distributions.Gamma(shape, rate)\n",
        "  return -dist.log_prob(y_true).mean()\n",
        "\n",
        "\n",
        "def rmsle_loss(y_true, y_pred):\n",
        "  return torch.sqrt(torch.mean((torch.log(y_true) - torch.log(y_pred)) ** 2))\n",
        "\n",
        "def gamma_rmsle_loss(y_true, shape, rate, alpha=0.5):\n",
        "  nll = gamma_nll(y_true, shape, rate)\n",
        "  y_pred = (shape / rate) + 1\n",
        "  rmsle = rmsle_loss(y_true, y_pred)\n",
        "\n",
        "  return nll + alpha * rmsle, nll, rmsle\n",
        "\n",
        "# --- TENSOR BOARD ---\n",
        "writter1 = SummaryWriter(log_dir='runs/gammanet_1')"
      ],
      "metadata": {
        "id": "PJvnPVfmUFvI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train"
      ],
      "metadata": {
        "id": "RPNXmCOJUELb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- TRAIN ---\n",
        "def train_gamma_net(X, y, Model=GammaNet1, writter=writter1, hidden_dims=[256, 256, 128],\n",
        "                    alpha=0.5, epochs=20, batch_size=128, lr=3e-4, weight_decay=0,\n",
        "                    verbose=2):\n",
        "  input_dim = X.shape[1]\n",
        "\n",
        "  train_ds = TensorDataset(X, y)\n",
        "  train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "  model = Model(input_dim, hidden_dims).to(device)\n",
        "  model.train()\n",
        "  opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "  for epoch in range(epochs):\n",
        "    train_loss, train_nll, train_rmsle = 0, 0, 0\n",
        "    for xb, yb in train_loader:\n",
        "      shape, rate = model(xb)\n",
        "      loss, nll, rmsle = gamma_rmsle_loss(yb, shape, rate, alpha)\n",
        "      opt.zero_grad()\n",
        "      loss.backward()\n",
        "      opt.step()\n",
        "      train_loss += loss.item() * len(xb)\n",
        "      train_rmsle += rmsle.item() * len(xb)\n",
        "      train_nll += nll.item() * len(xb)\n",
        "\n",
        "    train_loss /= X.shape[0]\n",
        "    train_nll /= X.shape[0]\n",
        "    train_rmsle /= X.shape[0]\n",
        "\n",
        "\n",
        "    if verbose == 2:\n",
        "      writter.add_scalar('Loss/total', train_loss, epoch)\n",
        "      writter.add_scalar('Loss/nll', train_nll, epoch)\n",
        "      writter.add_scalar('Loss/rmsle', train_rmsle, epoch)\n",
        "\n",
        "    if verbose == 1:\n",
        "      print(f'\\nEpoch: {epoch}')\n",
        "      print(f'Train loss: {train_loss:.4f}')\n",
        "      print(f'Train nll: {train_nll:.4f}')\n",
        "      print(f'Train rmsle: {train_rmsle:.4f}')\n",
        "\n",
        "  if verbose == 2:\n",
        "    writter.close()\n",
        "  return model"
      ],
      "metadata": {
        "id": "phejK3gOgB0e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gammanet_model = train_gamma_net(X_5, y_tensor + 1, Model=GammaNet1, writter=writter1, epochs=20)"
      ],
      "metadata": {
        "id": "m8iyKWKUk1F1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd07e8a2-8937-48a4-a895-d81030417307"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0\n",
            "Train loss: 3.4012\n",
            "Train nll: 2.9371\n",
            "Train rmsle: 0.9281\n",
            "\n",
            "Epoch: 1\n",
            "Train loss: 3.3675\n",
            "Train nll: 2.8980\n",
            "Train rmsle: 0.9391\n",
            "\n",
            "Epoch: 2\n",
            "Train loss: 3.3657\n",
            "Train nll: 2.8965\n",
            "Train rmsle: 0.9385\n",
            "\n",
            "Epoch: 3\n",
            "Train loss: 3.3642\n",
            "Train nll: 2.8952\n",
            "Train rmsle: 0.9379\n",
            "\n",
            "Epoch: 4\n",
            "Train loss: 3.3638\n",
            "Train nll: 2.8949\n",
            "Train rmsle: 0.9378\n",
            "\n",
            "Epoch: 5\n",
            "Train loss: 3.3631\n",
            "Train nll: 2.8944\n",
            "Train rmsle: 0.9373\n",
            "\n",
            "Epoch: 6\n",
            "Train loss: 3.3610\n",
            "Train nll: 2.8927\n",
            "Train rmsle: 0.9367\n",
            "\n",
            "Epoch: 7\n",
            "Train loss: 3.3599\n",
            "Train nll: 2.8918\n",
            "Train rmsle: 0.9361\n",
            "\n",
            "Epoch: 8\n",
            "Train loss: 3.3581\n",
            "Train nll: 2.8903\n",
            "Train rmsle: 0.9354\n",
            "\n",
            "Epoch: 9\n",
            "Train loss: 3.3567\n",
            "Train nll: 2.8893\n",
            "Train rmsle: 0.9347\n",
            "\n",
            "Epoch: 10\n",
            "Train loss: 3.3558\n",
            "Train nll: 2.8886\n",
            "Train rmsle: 0.9343\n",
            "\n",
            "Epoch: 11\n",
            "Train loss: 3.3538\n",
            "Train nll: 2.8871\n",
            "Train rmsle: 0.9334\n",
            "\n",
            "Epoch: 12\n",
            "Train loss: 3.3534\n",
            "Train nll: 2.8868\n",
            "Train rmsle: 0.9332\n",
            "\n",
            "Epoch: 13\n",
            "Train loss: 3.3512\n",
            "Train nll: 2.8850\n",
            "Train rmsle: 0.9323\n",
            "\n",
            "Epoch: 14\n",
            "Train loss: 3.3505\n",
            "Train nll: 2.8845\n",
            "Train rmsle: 0.9321\n",
            "\n",
            "Epoch: 15\n",
            "Train loss: 3.3487\n",
            "Train nll: 2.8831\n",
            "Train rmsle: 0.9312\n",
            "\n",
            "Epoch: 16\n",
            "Train loss: 3.3479\n",
            "Train nll: 2.8826\n",
            "Train rmsle: 0.9307\n",
            "\n",
            "Epoch: 17\n",
            "Train loss: 3.3467\n",
            "Train nll: 2.8816\n",
            "Train rmsle: 0.9302\n",
            "\n",
            "Epoch: 18\n",
            "Train loss: 3.3465\n",
            "Train nll: 2.8815\n",
            "Train rmsle: 0.9300\n",
            "\n",
            "Epoch: 19\n",
            "Train loss: 3.3463\n",
            "Train nll: 2.8814\n",
            "Train rmsle: 0.9298\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "families_models.set_family(family='AUTOMOTIVE', model=gammanet_model, input_func=input_func_5, output_func=output_func_2)\n",
        "_ = families_models.predict_family(valid, \"AUTOMOTIVE\", True,  True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jxvd5wW1GKgY",
        "outputId": "3a106f08-c827-44cc-9841-d9a08e9419fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "FAMILY AUTOMOTIVE\n",
            "\n",
            "Loss on 2017-08-01: 0.7878431188780256\n",
            "Loss on 2017-08-02: 0.7854910282601003\n",
            "Loss on 2017-08-03: 0.6886270090397157\n",
            "Loss on 2017-08-04: 0.7478302132275366\n",
            "Loss on 2017-08-05: 0.6456735386227908\n",
            "Loss on 2017-08-06: 0.7315692833150528\n",
            "Loss on 2017-08-07: 0.7327707820329871\n",
            "Loss on 2017-08-08: 0.8159320674552754\n",
            "Loss on 2017-08-09: 0.6347572596340775\n",
            "Loss on 2017-08-10: 0.7119156879188498\n",
            "Loss on 2017-08-11: 0.7405154239073334\n",
            "Loss on 2017-08-12: 0.7318707829834115\n",
            "Loss on 2017-08-13: 0.8749049087683982\n",
            "Loss on 2017-08-14: 0.7624191084226284\n",
            "Loss on 2017-08-15: 0.6857141677221669\n",
            "\n",
            "Common Loss: 0.7385222920125567\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Hyperopt"
      ],
      "metadata": {
        "id": "KyTaigCfYq6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pbounds = {\n",
        "    'lr': (1e-2, 0.3),\n",
        "    'alpha': (0.1, 2),\n",
        "    'weight_decay': (0, 1e-2),\n",
        "    'batch_size': (128, 1024),\n",
        "}\n",
        "\n",
        "\n",
        "def objective(lr,alpha, weight_decay, batch_size):\n",
        "  batch_size = int(batch_size)\n",
        "\n",
        "  model = train_gamma_net(X_5, y_tensor + 1, Model=GammaNet1, lr=lr, alpha=alpha, hidden_dims=[64, 64, 64],\n",
        "                          weight_decay=weight_decay, batch_size=batch_size,  writter=writter1, epochs=5, verbose=1)\n",
        "\n",
        "  families_models.set_family('AUTOMOTIVE', model=model, input_func=input_func_5, output_func=output_func_2, predict_params={})\n",
        "  _, common_loss = families_models.predict_family(valid, 'AUTOMOTIVE', True, verbose=False)\n",
        "  return -common_loss #optimize rmsle metric\n",
        "\n",
        "\n",
        "optimizer = BayesianOptimization(\n",
        "    f=objective,\n",
        "    pbounds=pbounds,\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "\n",
        "optimizer.maximize(init_points=5, n_iter=20)\n",
        "\n",
        "best_params = optimizer.max['params']\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3wt3ujCZYjvr",
        "outputId": "0ea0b661-07cf-4365-e311-54a21ec0dab8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|   iter    |  target   |    lr     |   alpha   | weight... | batch_... |\n",
            "-------------------------------------------------------------------------\n",
            "\n",
            "Epoch: 0\n",
            "Train loss: 4.9204\n",
            "Train nll: 3.0971\n",
            "Train rmsle: 0.9564\n",
            "\n",
            "Epoch: 1\n",
            "Train loss: 4.7341\n",
            "Train nll: 2.9749\n",
            "Train rmsle: 0.9228\n",
            "\n",
            "Epoch: 2\n",
            "Train loss: 4.7318\n",
            "Train nll: 2.9733\n",
            "Train rmsle: 0.9225\n",
            "\n",
            "Epoch: 3\n",
            "Train loss: 4.7473\n",
            "Train nll: 2.9831\n",
            "Train rmsle: 0.9254\n",
            "\n",
            "Epoch: 4\n",
            "Train loss: 4.7224\n",
            "Train nll: 2.9684\n",
            "Train rmsle: 0.9201\n",
            "| \u001b[39m1        \u001b[39m | \u001b[39m-0.760274\u001b[39m | \u001b[39m0.1186166\u001b[39m | \u001b[39m1.9063571\u001b[39m | \u001b[39m0.0073199\u001b[39m | \u001b[39m664.39800\u001b[39m |\n",
            "\n",
            "Epoch: 0\n",
            "Train loss: 3.3866\n",
            "Train nll: 2.9916\n",
            "Train rmsle: 0.9965\n",
            "\n",
            "Epoch: 1\n",
            "Train loss: 3.3014\n",
            "Train nll: 2.9146\n",
            "Train rmsle: 0.9759\n",
            "\n",
            "Epoch: 2\n",
            "Train loss: 3.3017\n",
            "Train nll: 2.9150\n",
            "Train rmsle: 0.9755\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception ignored in: <function _xla_gc_callback at 0x79e485b74fe0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/jax/_src/lib/__init__.py\", line 96, in _xla_gc_callback\n",
            "    def _xla_gc_callback(*args):\n",
            "    \n",
            "KeyboardInterrupt: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 3\n",
            "Train loss: 3.3053\n",
            "Train nll: 2.9181\n",
            "Train rmsle: 0.9769\n",
            "\n",
            "Epoch: 4\n",
            "Train loss: 3.3024\n",
            "Train nll: 2.9158\n",
            "Train rmsle: 0.9753\n",
            "| \u001b[35m2        \u001b[39m | \u001b[35m-0.705630\u001b[39m | \u001b[35m0.0552454\u001b[39m | \u001b[35m0.3963895\u001b[39m | \u001b[35m0.0005808\u001b[39m | \u001b[35m904.09382\u001b[39m |\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-867781708.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_points\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mbest_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/bayes_opt/bayesian_optimization.py\u001b[0m in \u001b[0;36mmaximize\u001b[0;34m(self, init_points, n_iter)\u001b[0m\n\u001b[1;32m    320\u001b[0m                 \u001b[0mx_probe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuggest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 \u001b[0miteration\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_probe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlazy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bounds_transformer\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/bayes_opt/bayesian_optimization.py\u001b[0m in \u001b[0;36mprobe\u001b[0;34m(self, params, lazy)\u001b[0m\n\u001b[1;32m    237\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m             self.logger.log_optimization_step(\n\u001b[1;32m    241\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/bayes_opt/target_space.py\u001b[0m in \u001b[0;36mprobe\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    553\u001b[0m             \u001b[0merror_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"No target function has been provided.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mdict_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constraint\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-867781708.py\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(lr, alpha, weight_decay, batch_size)\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m   model = train_gamma_net(X_5, y_tensor + 1, Model=GammaNet1, lr=lr, alpha=alpha, hidden_dims=[64, 64, 64],\n\u001b[0m\u001b[1;32m     13\u001b[0m                           weight_decay=weight_decay, batch_size=batch_size,  writter=writter1, epochs=5, verbose=1)\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2347335144.py\u001b[0m in \u001b[0;36mtrain_gamma_net\u001b[0;34m(X, y, Model, writter, hidden_dims, alpha, epochs, batch_size, lr, weight_decay, verbose)\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_nll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_rmsle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m       \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m       \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrmsle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgamma_rmsle_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    732\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    788\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 790\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    791\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model stops at the same results, I think the experiments with Gamma can end there."
      ],
      "metadata": {
        "id": "2OhEcXQJ9zYy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RMSE"
      ],
      "metadata": {
        "id": "qbUYiFEY_yjM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RMSENet(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dims=[128, 64]):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        prev_dim = input_dim\n",
        "        for h in hidden_dims:\n",
        "            layers.append(nn.Linear(prev_dim, h))\n",
        "            layers.append(nn.BatchNorm1d(h))\n",
        "            layers.append(nn.ReLU())\n",
        "            prev_dim = h\n",
        "        self.hidden = nn.Sequential(*layers)\n",
        "        self.out = nn.Linear(prev_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.hidden(x)\n",
        "        y_pred = self.out(h)\n",
        "        y_pred = y_pred + 1\n",
        "        return y_pred\n",
        "\n",
        "    def predict(self, X):\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            y_pred = self(X).squeeze(-1).cpu().numpy()\n",
        "        return y_pred\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def rmse_loss(y_true, y_pred):\n",
        "    return torch.sqrt(torch.mean((y_true - y_pred)**2))\n",
        "\n",
        "\n",
        "\n",
        "writter = SummaryWriter(log_dir='runs/gammanet_rmse')"
      ],
      "metadata": {
        "id": "UFOmvGi0_yjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train"
      ],
      "metadata": {
        "id": "3xbGEie5_yjR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- TRAIN FUNCTION ---\n",
        "def train_rmse_net(X, y, Model=RMSENet, writter=writter, hidden_dims=[256, 256, 128],\n",
        "                   epochs=20, batch_size=128, lr=3e-4, weight_decay=0, verbose=2):\n",
        "\n",
        "    input_dim = X.shape[1]\n",
        "    train_ds = TensorDataset(X, y)\n",
        "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    model = Model(input_dim, hidden_dims).to(device)\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    loss_fn = nn.MSELoss()\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        train_loss = 0\n",
        "        for xb, yb in train_loader:\n",
        "            y_pred = model(xb)\n",
        "            loss = loss_fn(y_pred, yb)\n",
        "            opt.zero_grad()\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            train_loss += loss.item() * len(xb)\n",
        "\n",
        "        train_loss /= X.shape[0]\n",
        "\n",
        "        if verbose == 2:\n",
        "            writter.add_scalar('Loss/RMSE', train_loss, epoch)\n",
        "\n",
        "        if verbose >= 1:\n",
        "            print(f'Epoch {epoch}: Train RMSE = {train_loss:.4f}')\n",
        "\n",
        "    if verbose == 2:\n",
        "        writter.close()\n",
        "    return model"
      ],
      "metadata": {
        "id": "k9Vv2TPm_yjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Hyperopt"
      ],
      "metadata": {
        "id": "9jMHkUtn_yjS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bayes_opt import BayesianOptimization\n",
        "\n",
        "pbounds = {\n",
        "    'lr': (1e-4, 3e-2),\n",
        "    'weight_decay': (0, 1e-2),\n",
        "    'batch_size': (128, 1024),\n",
        "}\n",
        "\n",
        "def objective(lr, weight_decay, batch_size):\n",
        "    batch_size = int(batch_size)\n",
        "\n",
        "    model = train_rmse_net(\n",
        "        X_5, torch.log1p(y_tensor),\n",
        "        Model=RMSENet,\n",
        "        lr=lr,\n",
        "        hidden_dims=[256, 256, 256, 128, 128],\n",
        "        weight_decay=weight_decay,\n",
        "        batch_size=batch_size,\n",
        "        writter=writter,\n",
        "        epochs=5,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    families_models.set_family(\n",
        "        'AUTOMOTIVE',\n",
        "        model=model,\n",
        "        input_func=input_func_5,\n",
        "        output_func=output_func_1,\n",
        "        predict_params={}\n",
        "    )\n",
        "    _, common_loss = families_models.predict_family(valid, 'AUTOMOTIVE', True, verbose=False)\n",
        "\n",
        "    return -common_loss\n",
        "\n",
        "optimizer = BayesianOptimization(\n",
        "    f=objective,\n",
        "    pbounds=pbounds,\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "optimizer.maximize(init_points=5, n_iter=20)\n",
        "\n",
        "best_params = optimizer.max['params']\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        },
        "outputId": "647a5159-0558-48d8-f443-3b3dfb2c9c86",
        "id": "i5o-c1HP_yjS"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|   iter    |  target   |    lr     | weight... | batch_... |\n",
            "-------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/loss.py:616: UserWarning: Using a target size (torch.Size([783])) that is different to the input size (torch.Size([783, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/loss.py:616: UserWarning: Using a target size (torch.Size([297])) that is different to the input size (torch.Size([297, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: Train RMSE = 0.8419\n",
            "Epoch 1: Train RMSE = 0.7853\n",
            "Epoch 2: Train RMSE = 0.7853\n",
            "Epoch 3: Train RMSE = 0.7854\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3861719977.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m )\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_points\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0mbest_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/bayes_opt/bayesian_optimization.py\u001b[0m in \u001b[0;36mmaximize\u001b[0;34m(self, init_points, n_iter)\u001b[0m\n\u001b[1;32m    320\u001b[0m                 \u001b[0mx_probe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuggest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 \u001b[0miteration\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_probe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlazy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bounds_transformer\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/bayes_opt/bayesian_optimization.py\u001b[0m in \u001b[0;36mprobe\u001b[0;34m(self, params, lazy)\u001b[0m\n\u001b[1;32m    237\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m             self.logger.log_optimization_step(\n\u001b[1;32m    241\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/bayes_opt/target_space.py\u001b[0m in \u001b[0;36mprobe\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    553\u001b[0m             \u001b[0merror_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"No target function has been provided.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mdict_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constraint\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3861719977.py\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(lr, weight_decay, batch_size)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     model = train_rmse_net(\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mX_5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog1p\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mModel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRMSENet\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-768999292.py\u001b[0m in \u001b[0;36mtrain_rmse_net\u001b[0;34m(X, y, Model, writter, hidden_dims, epochs, batch_size, lr, weight_decay, verbose)\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    645\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m             )\n\u001b[0;32m--> 647\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    648\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    830\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model stops at the same results, I think the experiments with Gamma can end there."
      ],
      "metadata": {
        "id": "NLHcrNeV_yjS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary"
      ],
      "metadata": {
        "id": "Y99SmONsOaot"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "families_models.set_family('AUTOMOTIVE', model=catboost_model, input_func=input_func_1, output_func=output_func_1)\n",
        "catboost_pred_frame, _ = families_models.predict_family(valid, 'AUTOMOTIVE', True, True)"
      ],
      "metadata": {
        "id": "Se2XsIwMvZ_h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23943a63-5838-49df-d037-ceffbf39eb7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "FAMILY AUTOMOTIVE\n",
            "\n",
            "Loss on 2017-08-01: 0.5775082589591541\n",
            "Loss on 2017-08-02: 0.5334307955432448\n",
            "Loss on 2017-08-03: 0.5433681602547188\n",
            "Loss on 2017-08-04: 0.5985909638151694\n",
            "Loss on 2017-08-05: 0.4190313747056162\n",
            "Loss on 2017-08-06: 0.4148358598257852\n",
            "Loss on 2017-08-07: 0.48617911762790106\n",
            "Loss on 2017-08-08: 0.5476553731057744\n",
            "Loss on 2017-08-09: 0.4449775338232431\n",
            "Loss on 2017-08-10: 0.4518620533788081\n",
            "Loss on 2017-08-11: 0.5428742693261916\n",
            "Loss on 2017-08-12: 0.5170109762057188\n",
            "Loss on 2017-08-13: 0.5616189820595208\n",
            "Loss on 2017-08-14: 0.5108692940117888\n",
            "Loss on 2017-08-15: 0.522279869776\n",
            "\n",
            "Common Loss: 0.511472858827909\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "families_models.set_family('AUTOMOTIVE', lgb_model, input_func_1, output_func_2)\n",
        "lgb_pred_frame, _ = families_models.predict_family(valid, 'AUTOMOTIVE', True, verbose=True)"
      ],
      "metadata": {
        "id": "QmDOVE0IXYmU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc406e73-882f-491c-c885-390bd5f19845"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "FAMILY AUTOMOTIVE\n",
            "\n",
            "Loss on 2017-08-01: 0.5324777118211809\n",
            "Loss on 2017-08-02: 0.5427822625315272\n",
            "Loss on 2017-08-03: 0.5055258253802852\n",
            "Loss on 2017-08-04: 0.5625021131462608\n",
            "Loss on 2017-08-05: 0.4105246555187616\n",
            "Loss on 2017-08-06: 0.38511600387318207\n",
            "Loss on 2017-08-07: 0.47672661326704246\n",
            "Loss on 2017-08-08: 0.5610884227181391\n",
            "Loss on 2017-08-09: 0.42640391051951576\n",
            "Loss on 2017-08-10: 0.48664563327026766\n",
            "Loss on 2017-08-11: 0.49827879860952007\n",
            "Loss on 2017-08-12: 0.5363926501930589\n",
            "Loss on 2017-08-13: 0.5718453359292137\n",
            "Loss on 2017-08-14: 0.5199454086946992\n",
            "Loss on 2017-08-15: 0.5235316070234651\n",
            "\n",
            "Common Loss: 0.5026524634997414\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(1, 3)\n",
        "axes[0].hist(automotive_valid['sales'])\n",
        "axes[1].hist(catboost_pred_frame['sales'])\n",
        "axes[2].hist(lgb_pred_frame['sales'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGeJnrsVNuE2",
        "outputId": "c6754859-3b34-49d5-d521-7ed01972ac16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([160., 217., 158., 106.,  63.,  62.,  20.,  11.,   8.,   5.]),\n",
              " array([ 1. ,  3.2,  5.4,  7.6,  9.8, 12. , 14.2, 16.4, 18.6, 20.8, 23. ]),\n",
              " <BarContainer object of 10 artists>)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 3 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "plt.scatter(range(len(automotive_valid)), automotive_valid['sales'], s=5, label='Actual Sales')\n",
        "plt.scatter(range(len(automotive_valid)), catboost_pred_frame['sales'], s=5, label='CatBoost Predictions')\n",
        "plt.scatter(range(len(automotive_valid)), lgb_pred_frame['sales'], s=5, label='LGBM Predictions')\n",
        "\n",
        "plt.title('Sales Predictions vs. Actual Sales')\n",
        "plt.xlabel('Index')\n",
        "plt.ylabel('Sales')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UL577uvGO3gb",
        "outputId": "2ac496b1-84c8-4d40-a031-0171960c06c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x1000 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from catboost import Pool\n",
        "\n",
        "def normalize_importances(series):\n",
        "    s = series.copy()\n",
        "    return 100 * s / s.sum() if s.sum() > 0 else s\n",
        "\n",
        "def compare_feature_importances(cat_model, lgb_model, X, y):\n",
        "\n",
        "    pool = Pool(X, y, cat_features=cat_features)\n",
        "    cat_importance = pd.DataFrame({\n",
        "        \"feature\": X.columns,\n",
        "        \"catboost_importance\": normalize_importances(\n",
        "            pd.Series(cat_model.get_feature_importance(pool, type=\"FeatureImportance\"))\n",
        "        ),\n",
        "        \"catboost_pred_change\": normalize_importances(\n",
        "            pd.Series(cat_model.get_feature_importance(pool, type=\"PredictionValuesChange\"))\n",
        "        ),\n",
        "    })\n",
        "\n",
        "\n",
        "    booster = getattr(lgb_model, \"booster_\", lgb_model)\n",
        "    lgb_importance = pd.DataFrame({\n",
        "        \"feature\": booster.feature_name(),\n",
        "        \"lgb_gain\": normalize_importances(\n",
        "            pd.Series(booster.feature_importance(importance_type=\"gain\"))\n",
        "        ),\n",
        "        \"lgb_split\": normalize_importances(\n",
        "            pd.Series(booster.feature_importance(importance_type=\"split\"))\n",
        "        ),\n",
        "    })\n",
        "\n",
        "\n",
        "    df = pd.merge(cat_importance, lgb_importance, on=\"feature\", how=\"outer\").fillna(0)\n",
        "    df = df.sort_values(by=\"catboost_importance\", ascending=False).reset_index(drop=True)\n",
        "    return df\n",
        "\n",
        "df_importances = compare_feature_importances(catboost_model, lgb_model, X_1, y_log)\n",
        "df_importances\n"
      ],
      "metadata": {
        "id": "0_k7x218Xpll",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fca9f0b-d0d5-4f47-f022-35e53fd562c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                   feature  catboost_importance  catboost_pred_change  \\\n",
              "0             transactions                24.01                 24.01   \n",
              "1    sales_rolling_mean_30                13.78                 13.78   \n",
              "2    sales_rolling_mean_15                 7.77                  7.77   \n",
              "3               is_weekend                 7.66                  7.66   \n",
              "4     sales_rolling_mean_7                 5.05                  5.05   \n",
              "5              day_of_week                 4.30                  4.30   \n",
              "6            store_cluster                 2.44                  2.44   \n",
              "7     sales_rolling_std_30                 2.30                  2.30   \n",
              "8               store_type                 2.12                  2.12   \n",
              "9               store_city                 1.93                  1.93   \n",
              "10  sales_rolling_median_7                 1.76                  1.76   \n",
              "11                cos_30.5                 1.75                  1.75   \n",
              "12             store_state                 1.69                  1.69   \n",
              "13    sales_rolling_std_15                 1.56                  1.56   \n",
              "14             sales_lag_1                 1.53                  1.53   \n",
              "15                sin_30.5                 1.47                  1.47   \n",
              "16              cos_365.25                 1.31                  1.31   \n",
              "17      oil_rolling_std_30                 1.18                  1.18   \n",
              "18             sales_lag_7                 1.03                  1.03   \n",
              "19             oil_lag_365                 0.98                  0.98   \n",
              "20      oil_rolling_std_15                 0.97                  0.97   \n",
              "21              sin_365.25                 0.96                  0.96   \n",
              "22            sales_lag_30                 0.92                  0.92   \n",
              "23                   sin_7                 0.92                  0.92   \n",
              "24           sales_lag_365                 0.88                  0.88   \n",
              "25            sales_lag_15                 0.87                  0.87   \n",
              "26             onpromotion                 0.84                  0.84   \n",
              "27          holiday_locale                 0.83                  0.83   \n",
              "28                    year                 0.76                  0.76   \n",
              "29                   month                 0.75                  0.75   \n",
              "30     store_in_event_area                 0.61                  0.61   \n",
              "31              oil_lag_15                 0.56                  0.56   \n",
              "32              dcoilwtico                 0.52                  0.52   \n",
              "33            is_christmas                 0.52                  0.52   \n",
              "34               oil_lag_7                 0.44                  0.44   \n",
              "35              oil_lag_30                 0.43                  0.43   \n",
              "36               oil_lag_1                 0.39                  0.39   \n",
              "37     oil_rolling_mean_30                 0.37                  0.37   \n",
              "38              is_holiday                 0.33                  0.33   \n",
              "39      oil_rolling_mean_7                 0.32                  0.32   \n",
              "40                   cos_7                 0.32                  0.32   \n",
              "41    oil_rolling_median_7                 0.32                  0.32   \n",
              "42     oil_rolling_mean_15                 0.24                  0.24   \n",
              "43                is_event                 0.12                  0.12   \n",
              "44               is_payday                 0.12                  0.12   \n",
              "45       earthquake_effect                 0.07                  0.07   \n",
              "\n",
              "    lgb_gain  lgb_split  \n",
              "0      18.86       5.46  \n",
              "1      26.52       3.03  \n",
              "2      22.00       2.32  \n",
              "3       3.58       0.51  \n",
              "4       4.94       3.28  \n",
              "5       1.10       5.12  \n",
              "6       1.09       7.55  \n",
              "7       0.26       2.97  \n",
              "8       0.06       0.34  \n",
              "9       1.43       8.46  \n",
              "10      0.58       1.18  \n",
              "11      0.49       3.07  \n",
              "12      0.08       0.69  \n",
              "13     11.65       3.26  \n",
              "14      0.57       2.53  \n",
              "15      0.54       2.87  \n",
              "16      0.20       1.72  \n",
              "17      0.22       2.67  \n",
              "18      0.50       2.52  \n",
              "19      0.37       3.35  \n",
              "20      0.20       2.64  \n",
              "21      0.11       1.36  \n",
              "22      0.18       2.44  \n",
              "23      0.38       2.87  \n",
              "24      0.31       2.84  \n",
              "25      0.17       2.28  \n",
              "26      0.33       0.81  \n",
              "27      0.07       0.19  \n",
              "28      0.06       0.67  \n",
              "29      0.97       9.09  \n",
              "30      0.11       0.19  \n",
              "31      0.16       1.59  \n",
              "32      0.63       1.04  \n",
              "33      0.50       0.16  \n",
              "34      0.09       1.01  \n",
              "35      0.15       1.86  \n",
              "36      0.06       0.86  \n",
              "37      0.06       0.68  \n",
              "38      0.06       0.21  \n",
              "39      0.04       0.38  \n",
              "40      0.18       2.42  \n",
              "41      0.06       0.59  \n",
              "42      0.05       0.61  \n",
              "43      0.01       0.20  \n",
              "44      0.01       0.11  \n",
              "45      0.00       0.00  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-89aec6d1-2e88-4202-9ec8-6c0b48ae35ab\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>feature</th>\n",
              "      <th>catboost_importance</th>\n",
              "      <th>catboost_pred_change</th>\n",
              "      <th>lgb_gain</th>\n",
              "      <th>lgb_split</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>transactions</td>\n",
              "      <td>24.01</td>\n",
              "      <td>24.01</td>\n",
              "      <td>18.86</td>\n",
              "      <td>5.46</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>sales_rolling_mean_30</td>\n",
              "      <td>13.78</td>\n",
              "      <td>13.78</td>\n",
              "      <td>26.52</td>\n",
              "      <td>3.03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>sales_rolling_mean_15</td>\n",
              "      <td>7.77</td>\n",
              "      <td>7.77</td>\n",
              "      <td>22.00</td>\n",
              "      <td>2.32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>is_weekend</td>\n",
              "      <td>7.66</td>\n",
              "      <td>7.66</td>\n",
              "      <td>3.58</td>\n",
              "      <td>0.51</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>sales_rolling_mean_7</td>\n",
              "      <td>5.05</td>\n",
              "      <td>5.05</td>\n",
              "      <td>4.94</td>\n",
              "      <td>3.28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>day_of_week</td>\n",
              "      <td>4.30</td>\n",
              "      <td>4.30</td>\n",
              "      <td>1.10</td>\n",
              "      <td>5.12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>store_cluster</td>\n",
              "      <td>2.44</td>\n",
              "      <td>2.44</td>\n",
              "      <td>1.09</td>\n",
              "      <td>7.55</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>sales_rolling_std_30</td>\n",
              "      <td>2.30</td>\n",
              "      <td>2.30</td>\n",
              "      <td>0.26</td>\n",
              "      <td>2.97</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>store_type</td>\n",
              "      <td>2.12</td>\n",
              "      <td>2.12</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>store_city</td>\n",
              "      <td>1.93</td>\n",
              "      <td>1.93</td>\n",
              "      <td>1.43</td>\n",
              "      <td>8.46</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>sales_rolling_median_7</td>\n",
              "      <td>1.76</td>\n",
              "      <td>1.76</td>\n",
              "      <td>0.58</td>\n",
              "      <td>1.18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>cos_30.5</td>\n",
              "      <td>1.75</td>\n",
              "      <td>1.75</td>\n",
              "      <td>0.49</td>\n",
              "      <td>3.07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>store_state</td>\n",
              "      <td>1.69</td>\n",
              "      <td>1.69</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.69</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>sales_rolling_std_15</td>\n",
              "      <td>1.56</td>\n",
              "      <td>1.56</td>\n",
              "      <td>11.65</td>\n",
              "      <td>3.26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>sales_lag_1</td>\n",
              "      <td>1.53</td>\n",
              "      <td>1.53</td>\n",
              "      <td>0.57</td>\n",
              "      <td>2.53</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>sin_30.5</td>\n",
              "      <td>1.47</td>\n",
              "      <td>1.47</td>\n",
              "      <td>0.54</td>\n",
              "      <td>2.87</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>cos_365.25</td>\n",
              "      <td>1.31</td>\n",
              "      <td>1.31</td>\n",
              "      <td>0.20</td>\n",
              "      <td>1.72</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>oil_rolling_std_30</td>\n",
              "      <td>1.18</td>\n",
              "      <td>1.18</td>\n",
              "      <td>0.22</td>\n",
              "      <td>2.67</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>sales_lag_7</td>\n",
              "      <td>1.03</td>\n",
              "      <td>1.03</td>\n",
              "      <td>0.50</td>\n",
              "      <td>2.52</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>oil_lag_365</td>\n",
              "      <td>0.98</td>\n",
              "      <td>0.98</td>\n",
              "      <td>0.37</td>\n",
              "      <td>3.35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>oil_rolling_std_15</td>\n",
              "      <td>0.97</td>\n",
              "      <td>0.97</td>\n",
              "      <td>0.20</td>\n",
              "      <td>2.64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>sin_365.25</td>\n",
              "      <td>0.96</td>\n",
              "      <td>0.96</td>\n",
              "      <td>0.11</td>\n",
              "      <td>1.36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>sales_lag_30</td>\n",
              "      <td>0.92</td>\n",
              "      <td>0.92</td>\n",
              "      <td>0.18</td>\n",
              "      <td>2.44</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>sin_7</td>\n",
              "      <td>0.92</td>\n",
              "      <td>0.92</td>\n",
              "      <td>0.38</td>\n",
              "      <td>2.87</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>sales_lag_365</td>\n",
              "      <td>0.88</td>\n",
              "      <td>0.88</td>\n",
              "      <td>0.31</td>\n",
              "      <td>2.84</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>sales_lag_15</td>\n",
              "      <td>0.87</td>\n",
              "      <td>0.87</td>\n",
              "      <td>0.17</td>\n",
              "      <td>2.28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>onpromotion</td>\n",
              "      <td>0.84</td>\n",
              "      <td>0.84</td>\n",
              "      <td>0.33</td>\n",
              "      <td>0.81</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>holiday_locale</td>\n",
              "      <td>0.83</td>\n",
              "      <td>0.83</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>year</td>\n",
              "      <td>0.76</td>\n",
              "      <td>0.76</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.67</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>month</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.97</td>\n",
              "      <td>9.09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>store_in_event_area</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>oil_lag_15</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.16</td>\n",
              "      <td>1.59</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>dcoilwtico</td>\n",
              "      <td>0.52</td>\n",
              "      <td>0.52</td>\n",
              "      <td>0.63</td>\n",
              "      <td>1.04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>is_christmas</td>\n",
              "      <td>0.52</td>\n",
              "      <td>0.52</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>oil_lag_7</td>\n",
              "      <td>0.44</td>\n",
              "      <td>0.44</td>\n",
              "      <td>0.09</td>\n",
              "      <td>1.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>oil_lag_30</td>\n",
              "      <td>0.43</td>\n",
              "      <td>0.43</td>\n",
              "      <td>0.15</td>\n",
              "      <td>1.86</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>oil_lag_1</td>\n",
              "      <td>0.39</td>\n",
              "      <td>0.39</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.86</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>oil_rolling_mean_30</td>\n",
              "      <td>0.37</td>\n",
              "      <td>0.37</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.68</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>is_holiday</td>\n",
              "      <td>0.33</td>\n",
              "      <td>0.33</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>oil_rolling_mean_7</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>cos_7</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.18</td>\n",
              "      <td>2.42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>oil_rolling_median_7</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.59</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>oil_rolling_mean_15</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.61</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>is_event</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>is_payday</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>earthquake_effect</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-89aec6d1-2e88-4202-9ec8-6c0b48ae35ab')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-89aec6d1-2e88-4202-9ec8-6c0b48ae35ab button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-89aec6d1-2e88-4202-9ec8-6c0b48ae35ab');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-17ad7bf5-40e5-4a45-ab57-937c4c5a2ae7\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-17ad7bf5-40e5-4a45-ab57-937c4c5a2ae7')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-17ad7bf5-40e5-4a45-ab57-937c4c5a2ae7 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_998444a7-fd23-45a4-a1cb-fd23b114817d\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_importances')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_998444a7-fd23-45a4-a1cb-fd23b114817d button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_importances');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_importances",
              "summary": "{\n  \"name\": \"df_importances\",\n  \"rows\": 46,\n  \"fields\": [\n    {\n      \"column\": \"feature\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 46,\n        \"samples\": [\n          \"oil_rolling_mean_7\",\n          \"sales_lag_15\",\n          \"onpromotion\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"catboost_importance\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4.116144399456153,\n        \"min\": 0.07058825645996528,\n        \"max\": 24.00846185944838,\n        \"num_unique_values\": 46,\n        \"samples\": [\n          0.324560047458668,\n          0.8748479663699974,\n          0.839365704103593\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"catboost_pred_change\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4.116144399456153,\n        \"min\": 0.07058825645996528,\n        \"max\": 24.00846185944838,\n        \"num_unique_values\": 46,\n        \"samples\": [\n          0.324560047458668,\n          0.8748479663699974,\n          0.839365704103593\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lgb_gain\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5.786787513645567,\n        \"min\": 0.0,\n        \"max\": 26.517952586012235,\n        \"num_unique_values\": 46,\n        \"samples\": [\n          0.04090499506952284,\n          0.17198854095900903,\n          0.3344243279083657\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lgb_split\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2.1193268598910007,\n        \"min\": 0.0,\n        \"max\": 9.085408022130014,\n        \"num_unique_values\": 45,\n        \"samples\": [\n          2.420470262793914,\n          2.2821576763485476,\n          0.8125864453665284\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "lgb better on rmse but catboost understands cats better\n",
        "\n",
        "lgb is faster\n"
      ],
      "metadata": {
        "id": "GwBynnVL2A9S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Baby care"
      ],
      "metadata": {
        "id": "M1CeWFYev5wk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "baby_train = get_family_data(train, 'BABY CARE')\n",
        "baby_train = baby_train[baby_train['date'] >= '2016-01-01']\n",
        "baby_valid = get_family_data(valid, 'BABY CARE')"
      ],
      "metadata": {
        "id": "xlnzj2uDkFK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_1 = input_func_1(baby_train)"
      ],
      "metadata": {
        "id": "bSi4byVdyMCz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = baby_train['sales']\n",
        "y_log = np.log1p(y)\n",
        "plt.hist(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBeiB4n7wsAO",
        "outputId": "453bfd04-4d08-4fc4-f421-c134ff277b57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([3.1147e+04, 6.3000e+02, 1.5100e+02, 2.5000e+01, 9.0000e+00,\n",
              "        3.0000e+00, 2.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00]),\n",
              " array([ 0. ,  2.2,  4.4,  6.6,  8.8, 11. , 13.2, 15.4, 17.6, 19.8, 22. ]),\n",
              " <BarContainer object of 10 artists>)"
            ]
          },
          "metadata": {},
          "execution_count": 48
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(y_log)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lUY2a-63zfIf",
        "outputId": "fa1f6b26-c871-4e26-e02f-c5f726027d8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([2.7461e+04, 0.0000e+00, 2.6270e+03, 1.0590e+03, 4.4200e+02,\n",
              "        2.9100e+02, 6.9000e+01, 1.6000e+01, 2.0000e+00, 1.0000e+00]),\n",
              " array([0.        , 0.31354942, 0.62709884, 0.94064826, 1.25419769,\n",
              "        1.56774711, 1.88129653, 2.19484595, 2.50839537, 2.82194479,\n",
              "        3.13549422]),\n",
              " <BarContainer object of 10 artists>)"
            ]
          },
          "metadata": {},
          "execution_count": 49
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from catboost import CatBoostRegressor, Pool\n",
        "from bayes_opt import BayesianOptimization\n",
        "\n",
        "pbounds = {\n",
        "    'depth': (4, 10),\n",
        "    'border_count': (32, 255),\n",
        "    'iterations': (100, 1000),\n",
        "    'learning_rate': (0.01, 0.1),\n",
        "    'l2_leaf_reg': (1, 10),\n",
        "    'random_strength': (0.0, 8.0),\n",
        "    'bagging_temperature': (0.0, 1.0),\n",
        "}\n",
        "\n",
        "\n",
        "train_pool = Pool(X_1, y_log, cat_features)\n",
        "def objective(depth, border_count, iterations, learning_rate, l2_leaf_reg, random_strength,\n",
        "              bagging_temperature):\n",
        "\n",
        "    iterations = int(iterations)\n",
        "    depth = int(depth)\n",
        "    border_count = int(border_count)\n",
        "\n",
        "    model = CatBoostRegressor(\n",
        "        depth=depth,\n",
        "        border_count=border_count,\n",
        "        iterations=iterations,\n",
        "        learning_rate=learning_rate,\n",
        "        l2_leaf_reg=l2_leaf_reg,\n",
        "        random_strength=random_strength,\n",
        "        bagging_temperature=bagging_temperature,\n",
        "        loss_function='RMSE',\n",
        "        verbose=0,\n",
        "        # task_type='GPU',\n",
        "    )\n",
        "\n",
        "    model.fit(train_pool)\n",
        "    print('\\ntrain score:', model.get_best_score()['learn'])\n",
        "\n",
        "    families_models.set_family('BABY CARE', model=model, input_func=input_func_1, output_func=output_func_1)\n",
        "    pred_frame, common_loss = families_models.predict_family(valid, 'BABY CARE', True, verbose=False)\n",
        "\n",
        "    return -common_loss\n",
        "\n",
        "optimizer = BayesianOptimization(\n",
        "    f=objective,\n",
        "    pbounds=pbounds,\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "optimizer.maximize(init_points=5, n_iter=20)\n",
        "\n",
        "best_params = optimizer.max['params']\n",
        "print(best_params)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NTvyA4Djw32s",
        "outputId": "dff2e359-353b-4aed-9bbc-e3d9a1001d94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|   iter    |  target   |   depth   | border... | iterat... | learni... | l2_lea... | random... | baggin... |\n",
            "-------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "train score: {'RMSE': 0.2524331208570953}\n",
            "| \u001b[39m1        \u001b[39m | \u001b[39m-0.293040\u001b[39m | \u001b[39m6.2472407\u001b[39m | \u001b[39m244.00929\u001b[39m | \u001b[39m758.79454\u001b[39m | \u001b[39m0.0638792\u001b[39m | \u001b[39m2.4041677\u001b[39m | \u001b[39m1.2479561\u001b[39m | \u001b[39m0.0580836\u001b[39m |\n",
            "\n",
            "train score: {'RMSE': 0.2885958911553199}\n",
            "| \u001b[39m2        \u001b[39m | \u001b[39m-0.294092\u001b[39m | \u001b[39m9.1970568\u001b[39m | \u001b[39m166.04864\u001b[39m | \u001b[39m737.26532\u001b[39m | \u001b[39m0.0118526\u001b[39m | \u001b[39m9.7291886\u001b[39m | \u001b[39m6.6595411\u001b[39m | \u001b[39m0.2123391\u001b[39m |\n",
            "\n",
            "train score: {'RMSE': 0.28306168457093034}\n",
            "| \u001b[35m3        \u001b[39m | \u001b[35m-0.291786\u001b[39m | \u001b[35m5.0909498\u001b[39m | \u001b[35m72.899205\u001b[39m | \u001b[35m373.81801\u001b[39m | \u001b[35m0.0572280\u001b[39m | \u001b[35m4.8875051\u001b[39m | \u001b[35m2.3298331\u001b[39m | \u001b[35m0.6118528\u001b[39m |\n",
            "\n",
            "train score: {'RMSE': 0.2866030818543626}\n",
            "| \u001b[35m4        \u001b[39m | \u001b[35m-0.291651\u001b[39m | \u001b[35m4.8369631\u001b[39m | \u001b[35m97.148256\u001b[39m | \u001b[35m429.72565\u001b[39m | \u001b[35m0.0510462\u001b[39m | \u001b[35m8.0665836\u001b[39m | \u001b[35m1.5973902\u001b[39m | \u001b[35m0.5142344\u001b[39m |\n",
            "\n",
            "train score: {'RMSE': 0.27867553500467596}\n",
            "| \u001b[39m5        \u001b[39m | \u001b[39m-0.291786\u001b[39m | \u001b[39m7.5544874\u001b[39m | \u001b[39m42.358442\u001b[39m | \u001b[39m646.79036\u001b[39m | \u001b[39m0.0253471\u001b[39m | \u001b[39m1.5854643\u001b[39m | \u001b[39m7.5910842\u001b[39m | \u001b[39m0.9656320\u001b[39m |\n",
            "\n",
            "train score: {'RMSE': 0.2828394540359185}\n",
            "| \u001b[35m6        \u001b[39m | \u001b[35m-0.290432\u001b[39m | \u001b[35m4.9086731\u001b[39m | \u001b[35m32.999417\u001b[39m | \u001b[35m529.62537\u001b[39m | \u001b[35m0.0683422\u001b[39m | \u001b[35m4.9697827\u001b[39m | \u001b[35m4.9720294\u001b[39m | \u001b[35m0.2612269\u001b[39m |\n",
            "\n",
            "train score: {'RMSE': 0.2908511821172583}\n",
            "| \u001b[39m7        \u001b[39m | \u001b[39m-0.291703\u001b[39m | \u001b[39m7.3384755\u001b[39m | \u001b[39m35.405899\u001b[39m | \u001b[39m527.74804\u001b[39m | \u001b[39m0.0134512\u001b[39m | \u001b[39m5.1298587\u001b[39m | \u001b[39m4.7048616\u001b[39m | \u001b[39m0.3924243\u001b[39m |\n",
            "\n",
            "train score: {'RMSE': 0.2785528409861815}\n",
            "| \u001b[39m8        \u001b[39m | \u001b[39m-0.290783\u001b[39m | \u001b[39m4.0      \u001b[39m | \u001b[39m33.115106\u001b[39m | \u001b[39m531.43688\u001b[39m | \u001b[39m0.1      \u001b[39m | \u001b[39m6.2303625\u001b[39m | \u001b[39m7.0255998\u001b[39m | \u001b[39m0.1518438\u001b[39m |\n",
            "\n",
            "train score: {'RMSE': 0.2768194700238358}\n",
            "| \u001b[35m9        \u001b[39m | \u001b[35m-0.290134\u001b[39m | \u001b[35m4.0      \u001b[39m | \u001b[35m32.0     \u001b[39m | \u001b[35m533.22791\u001b[39m | \u001b[35m0.1      \u001b[39m | \u001b[35m4.2996997\u001b[39m | \u001b[35m1.6508132\u001b[39m | \u001b[35m1.0      \u001b[39m |\n",
            "\n",
            "train score: {'RMSE': 0.2913890383048606}\n",
            "| \u001b[39m10       \u001b[39m | \u001b[39m-0.293387\u001b[39m | \u001b[39m4.0      \u001b[39m | \u001b[39m34.655920\u001b[39m | \u001b[39m540.57770\u001b[39m | \u001b[39m0.01     \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m0.6390323\u001b[39m | \u001b[39m1.0      \u001b[39m |\n",
            "\n",
            "train score: {'RMSE': 0.29081047843264096}\n",
            "| \u001b[39m11       \u001b[39m | \u001b[39m-0.291651\u001b[39m | \u001b[39m5.6259637\u001b[39m | \u001b[39m107.38977\u001b[39m | \u001b[39m425.33250\u001b[39m | \u001b[39m0.0176852\u001b[39m | \u001b[39m2.5002426\u001b[39m | \u001b[39m2.2879889\u001b[39m | \u001b[39m0.4412829\u001b[39m |\n",
            "\n",
            "train score: {'RMSE': 0.2797279375305395}\n",
            "| \u001b[35m12       \u001b[39m | \u001b[35m-0.289163\u001b[39m | \u001b[35m4.0      \u001b[39m | \u001b[35m32.0     \u001b[39m | \u001b[35m530.79865\u001b[39m | \u001b[35m0.1      \u001b[39m | \u001b[35m8.3530224\u001b[39m | \u001b[35m1.0060980\u001b[39m | \u001b[35m0.6138229\u001b[39m |\n",
            "\n",
            "train score: {'RMSE': 0.23616796356086472}\n",
            "| \u001b[39m13       \u001b[39m | \u001b[39m-0.291672\u001b[39m | \u001b[39m9.4927359\u001b[39m | \u001b[39m60.848066\u001b[39m | \u001b[39m979.08399\u001b[39m | \u001b[39m0.0379925\u001b[39m | \u001b[39m4.9440100\u001b[39m | \u001b[39m4.0886791\u001b[39m | \u001b[39m0.2392471\u001b[39m |\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3665163471.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m )\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_points\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0mbest_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/bayes_opt/bayesian_optimization.py\u001b[0m in \u001b[0;36mmaximize\u001b[0;34m(self, init_points, n_iter)\u001b[0m\n\u001b[1;32m    320\u001b[0m                 \u001b[0mx_probe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuggest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 \u001b[0miteration\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_probe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlazy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bounds_transformer\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/bayes_opt/bayesian_optimization.py\u001b[0m in \u001b[0;36mprobe\u001b[0;34m(self, params, lazy)\u001b[0m\n\u001b[1;32m    237\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m             self.logger.log_optimization_step(\n\u001b[1;32m    241\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/bayes_opt/target_space.py\u001b[0m in \u001b[0;36mprobe\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    553\u001b[0m             \u001b[0merror_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"No target function has been provided.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mdict_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constraint\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3665163471.py\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(depth, border_count, iterations, learning_rate, l2_leaf_reg, random_strength, bagging_temperature)\u001b[0m\n\u001b[1;32m     34\u001b[0m     )\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_pool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\ntrain score:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_best_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'learn'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/catboost/core.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, cat_features, text_features, embedding_features, graph, sample_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[1;32m   5871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'loss_function'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5872\u001b[0m             \u001b[0mCatBoostRegressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_is_compatible_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss_function'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5873\u001b[0;31m         return self._fit(X, y, cat_features, text_features, embedding_features, None, graph, sample_weight, None, None, None, None, baseline,\n\u001b[0m\u001b[1;32m   5874\u001b[0m                          \u001b[0muse_best_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogging_level\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn_description\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5875\u001b[0m                          \u001b[0mverbose_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_period\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msilent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/catboost/core.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, cat_features, text_features, embedding_features, pairs, graph, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[1;32m   2408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2409\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mplot_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Training plots'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_get_train_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2410\u001b[0;31m                 self._train(\n\u001b[0m\u001b[1;32m   2411\u001b[0m                     \u001b[0mtrain_pool\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2412\u001b[0m                     \u001b[0mtrain_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"eval_sets\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/catboost/core.py\u001b[0m in \u001b[0;36m_train\u001b[0;34m(self, train_pool, test_pool, params, allow_clear_pool, init_model)\u001b[0m\n\u001b[1;32m   1788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1789\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_clear_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1790\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_clear_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_object\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minit_model\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1791\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_trained_model_attributes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1792\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._CatBoost._train\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._CatBoost._train\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from catboost import CatBoostRegressor, Pool\n",
        "from bayes_opt import BayesianOptimization\n",
        "\n",
        "pbounds = {\n",
        "    'depth': (4, 10),\n",
        "    'border_count': (32, 255),\n",
        "    'iterations': (100, 1000),\n",
        "    'learning_rate': (0.01, 0.1),\n",
        "    'l2_leaf_reg': (1, 10),\n",
        "    'random_strength': (0.0, 8.0),\n",
        "    'bagging_temperature': (0.0, 1.0),\n",
        "}\n",
        "\n",
        "\n",
        "train_pool = Pool(X_1, y + 1, cat_features)\n",
        "def objective(depth, border_count, iterations, learning_rate, l2_leaf_reg, random_strength,\n",
        "              bagging_temperature):\n",
        "\n",
        "    iterations = int(iterations)\n",
        "    depth = int(depth)\n",
        "    border_count = int(border_count)\n",
        "\n",
        "    model = CatBoostRegressor(\n",
        "        depth=depth,\n",
        "        border_count=border_count,\n",
        "        iterations=iterations,\n",
        "        learning_rate=learning_rate,\n",
        "        l2_leaf_reg=l2_leaf_reg,\n",
        "        random_strength=random_strength,\n",
        "        bagging_temperature=bagging_temperature,\n",
        "        loss_function='Poisson',\n",
        "        verbose=0,\n",
        "        # task_type='GPU',\n",
        "    )\n",
        "\n",
        "    model.fit(train_pool)\n",
        "    print('\\ntrain score:', model.get_best_score()['learn'])\n",
        "\n",
        "    families_models.set_family('BABY CARE', model=model, input_func=input_func_1, output_func=output_func_2)\n",
        "    pred_frame, common_loss = families_models.predict_family(valid, 'BABY CARE', True, verbose=False)\n",
        "\n",
        "    return -common_loss\n",
        "\n",
        "optimizer = BayesianOptimization(\n",
        "    f=objective,\n",
        "    pbounds=pbounds,\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "optimizer.maximize(init_points=5, n_iter=20)\n",
        "\n",
        "best_params = optimizer.max['params']\n",
        "print(best_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q1MX7ubI0WAk",
        "outputId": "694ab63e-0314-40aa-9ce5-3ea4326b0d58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|   iter    |  target   |   depth   | border... | iterat... | learni... | l2_lea... | random... | baggin... |\n",
            "-------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "train score: {'Poisson': 0.8808094706117052}\n",
            "| \u001b[39m1        \u001b[39m | \u001b[39m-0.328762\u001b[39m | \u001b[39m6.2472407\u001b[39m | \u001b[39m244.00929\u001b[39m | \u001b[39m758.79454\u001b[39m | \u001b[39m0.0638792\u001b[39m | \u001b[39m2.4041677\u001b[39m | \u001b[39m1.2479561\u001b[39m | \u001b[39m0.0580836\u001b[39m |\n",
            "\n",
            "train score: {'Poisson': 0.9067002924673592}\n",
            "| \u001b[35m2        \u001b[39m | \u001b[35m-0.317815\u001b[39m | \u001b[35m9.1970568\u001b[39m | \u001b[35m166.04864\u001b[39m | \u001b[35m737.26532\u001b[39m | \u001b[35m0.0118526\u001b[39m | \u001b[35m9.7291886\u001b[39m | \u001b[35m6.6595411\u001b[39m | \u001b[35m0.2123391\u001b[39m |\n",
            "\n",
            "train score: {'Poisson': 0.902919724306483}\n",
            "| \u001b[35m3        \u001b[39m | \u001b[35m-0.309513\u001b[39m | \u001b[35m5.0909498\u001b[39m | \u001b[35m72.899205\u001b[39m | \u001b[35m373.81801\u001b[39m | \u001b[35m0.0572280\u001b[39m | \u001b[35m4.8875051\u001b[39m | \u001b[35m2.3298331\u001b[39m | \u001b[35m0.6118528\u001b[39m |\n",
            "\n",
            "train score: {'Poisson': 0.9063045595708541}\n",
            "| \u001b[39m4        \u001b[39m | \u001b[39m-0.329726\u001b[39m | \u001b[39m4.8369631\u001b[39m | \u001b[39m97.148256\u001b[39m | \u001b[39m429.72565\u001b[39m | \u001b[39m0.0510462\u001b[39m | \u001b[39m8.0665836\u001b[39m | \u001b[39m1.5973902\u001b[39m | \u001b[39m0.5142344\u001b[39m |\n",
            "\n",
            "train score: {'Poisson': 0.8996791310908525}\n",
            "| \u001b[39m5        \u001b[39m | \u001b[39m-0.311812\u001b[39m | \u001b[39m7.5544874\u001b[39m | \u001b[39m42.358442\u001b[39m | \u001b[39m646.79036\u001b[39m | \u001b[39m0.0253471\u001b[39m | \u001b[39m1.5854643\u001b[39m | \u001b[39m7.5910842\u001b[39m | \u001b[39m0.9656320\u001b[39m |\n",
            "\n",
            "train score: {'Poisson': 0.9014057573275777}\n",
            "| \u001b[39m6        \u001b[39m | \u001b[39m-0.319733\u001b[39m | \u001b[39m5.5851848\u001b[39m | \u001b[39m73.959623\u001b[39m | \u001b[39m373.73606\u001b[39m | \u001b[39m0.0609401\u001b[39m | \u001b[39m4.8122539\u001b[39m | \u001b[39m2.2395278\u001b[39m | \u001b[39m0.4410861\u001b[39m |\n",
            "\n",
            "train score: {'Poisson': 0.8895348010172217}\n",
            "| \u001b[39m7        \u001b[39m | \u001b[39m-0.311664\u001b[39m | \u001b[39m6.7164321\u001b[39m | \u001b[39m205.76170\u001b[39m | \u001b[39m755.05905\u001b[39m | \u001b[39m0.0445504\u001b[39m | \u001b[39m4.1170128\u001b[39m | \u001b[39m4.6918416\u001b[39m | \u001b[39m0.9808633\u001b[39m |\n",
            "\n",
            "train score: {'Poisson': 0.8896437004246962}\n",
            "| \u001b[39m8        \u001b[39m | \u001b[39m-0.340504\u001b[39m | \u001b[39m4.5345368\u001b[39m | \u001b[39m163.26095\u001b[39m | \u001b[39m853.08720\u001b[39m | \u001b[39m0.0997253\u001b[39m | \u001b[39m4.0160199\u001b[39m | \u001b[39m2.7685559\u001b[39m | \u001b[39m0.9490182\u001b[39m |\n",
            "\n",
            "train score: {'Poisson': 0.8840743779361403}\n",
            "| \u001b[39m9        \u001b[39m | \u001b[39m-0.341467\u001b[39m | \u001b[39m6.9833509\u001b[39m | \u001b[39m205.81140\u001b[39m | \u001b[39m755.15070\u001b[39m | \u001b[39m0.0580360\u001b[39m | \u001b[39m3.5046056\u001b[39m | \u001b[39m5.7752230\u001b[39m | \u001b[39m0.7838875\u001b[39m |\n",
            "\n",
            "train score: {'Poisson': 0.9068442851333983}\n",
            "| \u001b[39m10       \u001b[39m | \u001b[39m-0.316926\u001b[39m | \u001b[39m5.1002742\u001b[39m | \u001b[39m74.651338\u001b[39m | \u001b[39m373.82731\u001b[39m | \u001b[39m0.0432137\u001b[39m | \u001b[39m6.0467888\u001b[39m | \u001b[39m3.0284482\u001b[39m | \u001b[39m0.9970695\u001b[39m |\n",
            "\n",
            "train score: {'Poisson': 0.8675873928875997}\n",
            "| \u001b[39m11       \u001b[39m | \u001b[39m-0.319294\u001b[39m | \u001b[39m7.2513442\u001b[39m | \u001b[39m109.62503\u001b[39m | \u001b[39m561.55171\u001b[39m | \u001b[39m0.0978254\u001b[39m | \u001b[39m1.5210555\u001b[39m | \u001b[39m2.5004192\u001b[39m | \u001b[39m0.5843103\u001b[39m |\n",
            "\n",
            "train score: {'Poisson': 0.8636120575839228}\n",
            "| \u001b[39m12       \u001b[39m | \u001b[39m-0.315913\u001b[39m | \u001b[39m8.0713333\u001b[39m | \u001b[39m168.05403\u001b[39m | \u001b[39m826.85314\u001b[39m | \u001b[39m0.0534927\u001b[39m | \u001b[39m3.7676547\u001b[39m | \u001b[39m7.9564239\u001b[39m | \u001b[39m0.3134158\u001b[39m |\n",
            "\n",
            "train score: {'Poisson': 0.8539180531523255}\n",
            "| \u001b[39m13       \u001b[39m | \u001b[39m-0.331840\u001b[39m | \u001b[39m9.4927359\u001b[39m | \u001b[39m60.848066\u001b[39m | \u001b[39m979.08399\u001b[39m | \u001b[39m0.0379925\u001b[39m | \u001b[39m4.9440100\u001b[39m | \u001b[39m4.0886791\u001b[39m | \u001b[39m0.2392471\u001b[39m |\n",
            "\n",
            "train score: {'Poisson': 0.8480178363922826}\n",
            "| \u001b[39m14       \u001b[39m | \u001b[39m-0.326868\u001b[39m | \u001b[39m9.9380892\u001b[39m | \u001b[39m227.39709\u001b[39m | \u001b[39m996.04215\u001b[39m | \u001b[39m0.0429257\u001b[39m | \u001b[39m8.3054692\u001b[39m | \u001b[39m1.2160092\u001b[39m | \u001b[39m0.9713499\u001b[39m |\n",
            "\n",
            "train score: {'Poisson': 0.8879299397168791}\n",
            "| \u001b[39m15       \u001b[39m | \u001b[39m-0.318120\u001b[39m | \u001b[39m7.5245980\u001b[39m | \u001b[39m121.60065\u001b[39m | \u001b[39m708.48934\u001b[39m | \u001b[39m0.0341482\u001b[39m | \u001b[39m3.2979447\u001b[39m | \u001b[39m3.1799639\u001b[39m | \u001b[39m0.3486651\u001b[39m |\n",
            "\n",
            "train score: {'Poisson': 0.8596198366674908}\n",
            "| \u001b[39m16       \u001b[39m | \u001b[39m-0.320677\u001b[39m | \u001b[39m9.7413955\u001b[39m | \u001b[39m125.99341\u001b[39m | \u001b[39m549.16096\u001b[39m | \u001b[39m0.0623701\u001b[39m | \u001b[39m9.8020184\u001b[39m | \u001b[39m6.3308291\u001b[39m | \u001b[39m0.5551452\u001b[39m |\n",
            "\n",
            "train score: {'Poisson': 0.904587219387}\n",
            "| \u001b[39m17       \u001b[39m | \u001b[39m-0.315532\u001b[39m | \u001b[39m9.8659102\u001b[39m | \u001b[39m249.82953\u001b[39m | \u001b[39m317.51636\u001b[39m | \u001b[39m0.0319078\u001b[39m | \u001b[39m9.9718369\u001b[39m | \u001b[39m6.2727282\u001b[39m | \u001b[39m0.1132726\u001b[39m |\n",
            "\n",
            "train score: {'Poisson': 0.8484513246268628}\n",
            "| \u001b[39m18       \u001b[39m | \u001b[39m-0.324053\u001b[39m | \u001b[39m8.4567273\u001b[39m | \u001b[39m191.40048\u001b[39m | \u001b[39m669.15258\u001b[39m | \u001b[39m0.0978971\u001b[39m | \u001b[39m4.7462319\u001b[39m | \u001b[39m1.1737298\u001b[39m | \u001b[39m0.5888336\u001b[39m |\n",
            "\n",
            "train score: {'Poisson': 0.8711029601866581}\n",
            "| \u001b[39m19       \u001b[39m | \u001b[39m-0.336711\u001b[39m | \u001b[39m6.8002328\u001b[39m | \u001b[39m208.80362\u001b[39m | \u001b[39m856.53242\u001b[39m | \u001b[39m0.0846279\u001b[39m | \u001b[39m8.0907464\u001b[39m | \u001b[39m3.7308564\u001b[39m | \u001b[39m0.1620674\u001b[39m |\n",
            "\n",
            "train score: {'Poisson': 0.835017850807822}\n",
            "| \u001b[39m20       \u001b[39m | \u001b[39m-0.315077\u001b[39m | \u001b[39m9.0138614\u001b[39m | \u001b[39m44.650360\u001b[39m | \u001b[39m842.09340\u001b[39m | \u001b[39m0.0876681\u001b[39m | \u001b[39m1.9745626\u001b[39m | \u001b[39m7.3276652\u001b[39m | \u001b[39m0.0092846\u001b[39m |\n",
            "\n",
            "train score: {'Poisson': 0.9129031121687422}\n",
            "| \u001b[35m21       \u001b[39m | \u001b[35m-0.301103\u001b[39m | \u001b[35m7.5126907\u001b[39m | \u001b[35m110.53015\u001b[39m | \u001b[35m165.08330\u001b[39m | \u001b[35m0.0170102\u001b[39m | \u001b[35m4.5191930\u001b[39m | \u001b[35m1.3537148\u001b[39m | \u001b[35m0.1943711\u001b[39m |\n",
            "\n",
            "train score: {'Poisson': 0.8742578743864761}\n",
            "| \u001b[39m22       \u001b[39m | \u001b[39m-0.346964\u001b[39m | \u001b[39m7.7916104\u001b[39m | \u001b[39m228.71650\u001b[39m | \u001b[39m728.83031\u001b[39m | \u001b[39m0.0530393\u001b[39m | \u001b[39m4.8715510\u001b[39m | \u001b[39m0.2840120\u001b[39m | \u001b[39m0.1092624\u001b[39m |\n",
            "\n",
            "train score: {'Poisson': 0.8895554261460423}\n",
            "| \u001b[39m23       \u001b[39m | \u001b[39m-0.323200\u001b[39m | \u001b[39m5.6800627\u001b[39m | \u001b[39m215.89345\u001b[39m | \u001b[39m640.84029\u001b[39m | \u001b[39m0.0777957\u001b[39m | \u001b[39m3.0358242\u001b[39m | \u001b[39m5.0284314\u001b[39m | \u001b[39m0.1494642\u001b[39m |\n",
            "\n",
            "train score: {'Poisson': 0.8829284444622926}\n",
            "| \u001b[39m24       \u001b[39m | \u001b[39m-0.330115\u001b[39m | \u001b[39m7.3824453\u001b[39m | \u001b[39m124.38673\u001b[39m | \u001b[39m661.02899\u001b[39m | \u001b[39m0.0433901\u001b[39m | \u001b[39m1.3988657\u001b[39m | \u001b[39m2.2056653\u001b[39m | \u001b[39m0.7866360\u001b[39m |\n",
            "\n",
            "train score: {'Poisson': 0.8966738794902408}\n",
            "| \u001b[39m25       \u001b[39m | \u001b[39m-0.313357\u001b[39m | \u001b[39m5.5167961\u001b[39m | \u001b[39m107.65836\u001b[39m | \u001b[39m419.07387\u001b[39m | \u001b[39m0.0706188\u001b[39m | \u001b[39m5.8552362\u001b[39m | \u001b[39m0.9783119\u001b[39m | \u001b[39m0.6980347\u001b[39m |\n",
            "=============================================================================================================\n",
            "{'depth': np.float64(7.512690703754684), 'border_count': np.float64(110.53015434903553), 'iterations': np.float64(165.08330610499647), 'learning_rate': np.float64(0.017010299284247576), 'l2_leaf_reg': np.float64(4.519193051591925), 'random_strength': np.float64(1.3537148083751678), 'bagging_temperature': np.float64(0.19437116131767196)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import lightgbm as lgb\n",
        "from bayes_opt import BayesianOptimization\n",
        "\n",
        "pbounds_lgb = {\n",
        "    'learning_rate': (0.01, 0.2),\n",
        "    'max_depth': (4, 12),\n",
        "    'num_leaves': (16, 256),\n",
        "    'min_child_samples': (5, 50),\n",
        "    'min_child_weight': (1e-3, 10.0),\n",
        "    'subsample': (0.5, 1.0),\n",
        "    'colsample_bytree': (0.5, 1.0),\n",
        "    'reg_alpha': (0, 10),\n",
        "    'reg_lambda': (0, 10),\n",
        "    'max_bin': (64, 255),\n",
        "    'n_estimators': (500, 2000)\n",
        "}\n",
        "\n",
        "def objective(learning_rate, max_depth, num_leaves, min_child_samples,\n",
        "              min_child_weight, subsample, colsample_bytree,\n",
        "              reg_alpha, reg_lambda, max_bin, n_estimators):\n",
        "\n",
        "    train_pool = lgb.Dataset(X_1, label=y + 1, categorical_feature=cat_features)\n",
        "\n",
        "    max_depth = int(max_depth)\n",
        "    num_leaves = int(num_leaves)\n",
        "    min_child_samples = int(min_child_samples)\n",
        "    max_bin = int(max_bin)\n",
        "    n_estimators = int(n_estimators)\n",
        "\n",
        "    params = {\n",
        "        'objective': 'gamma',\n",
        "        'boosting_type': 'gbdt',\n",
        "        'learning_rate': learning_rate,\n",
        "        'num_leaves': num_leaves,\n",
        "        'max_depth': max_depth,\n",
        "        'min_child_samples': min_child_samples,\n",
        "        'min_child_weight': min_child_weight,\n",
        "        'subsample': subsample,\n",
        "        'colsample_bytree': colsample_bytree,\n",
        "        'reg_alpha': reg_alpha,\n",
        "        'reg_lambda': reg_lambda,\n",
        "        'max_bin': max_bin,\n",
        "        # 'device': 'gpu',\n",
        "        'verbose': -1\n",
        "    }\n",
        "\n",
        "    callbacks = [lgb.log_evaluation(period=n_estimators)]\n",
        "\n",
        "    model = lgb.train(\n",
        "        params,\n",
        "        train_pool,\n",
        "        num_boost_round=n_estimators,\n",
        "        valid_sets=[train_pool],\n",
        "        callbacks=callbacks\n",
        "    )\n",
        "\n",
        "    families_models.set_family('BABY CARE', model, input_func_1, output_func_2)\n",
        "    pred_frame, common_loss = families_models.predict_family(valid, 'BABY CARE', True, verbose=False)\n",
        "\n",
        "    return -common_loss\n",
        "\n",
        "optimizer = BayesianOptimization(\n",
        "    f=objective,\n",
        "    pbounds=pbounds_lgb,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "optimizer.maximize(init_points=5, n_iter=20)\n",
        "\n",
        "best_params = optimizer.max['params']\n",
        "print(best_params)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOy5QufFzeSO",
        "outputId": "2364083b-b55e-4960-c4f6-2a0ee8bd2d11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|   iter    |  target   | learni... | max_depth | num_le... | min_ch... | min_ch... | subsample | colsam... | reg_alpha | reg_la... |  max_bin  | n_esti... |\n",
            "-------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "[530]\ttraining's gamma: 1.15553\n",
            "| \u001b[39m1        \u001b[39m | \u001b[39m-0.322252\u001b[39m | \u001b[39m0.0811626\u001b[39m | \u001b[39m11.605714\u001b[39m | \u001b[39m191.67854\u001b[39m | \u001b[39m31.939631\u001b[39m | \u001b[39m1.5610303\u001b[39m | \u001b[39m0.5779972\u001b[39m | \u001b[39m0.5290418\u001b[39m | \u001b[39m8.6617614\u001b[39m | \u001b[39m6.0111501\u001b[39m | \u001b[39m199.24186\u001b[39m | \u001b[39m530.87674\u001b[39m |\n",
            "[709]\ttraining's gamma: 1.14237\n",
            "| \u001b[39m2        \u001b[39m | \u001b[39m-0.328558\u001b[39m | \u001b[39m0.1942828\u001b[39m | \u001b[39m10.659541\u001b[39m | \u001b[39m66.961386\u001b[39m | \u001b[39m13.182123\u001b[39m | \u001b[39m1.8348616\u001b[39m | \u001b[39m0.6521211\u001b[39m | \u001b[39m0.7623782\u001b[39m | \u001b[39m4.3194501\u001b[39m | \u001b[39m2.9122914\u001b[39m | \u001b[39m180.86390\u001b[39m | \u001b[39m709.24079\u001b[39m |\n",
            "[597]\ttraining's gamma: 1.15621\n",
            "| \u001b[39m3        \u001b[39m | \u001b[39m-0.322968\u001b[39m | \u001b[39m0.0655074\u001b[39m | \u001b[39m6.9308947\u001b[39m | \u001b[39m125.45679\u001b[39m | \u001b[39m40.332918\u001b[39m | \u001b[39m1.9975381\u001b[39m | \u001b[39m0.7571172\u001b[39m | \u001b[39m0.7962072\u001b[39m | \u001b[39m0.4645041\u001b[39m | \u001b[39m6.0754485\u001b[39m | \u001b[39m96.570107\u001b[39m | \u001b[39m597.57738\u001b[39m |\n",
            "[1863]\ttraining's gamma: 1.13485\n",
            "| \u001b[39m4        \u001b[39m | \u001b[39m-0.338296\u001b[39m | \u001b[39m0.1902882\u001b[39m | \u001b[39m11.725056\u001b[39m | \u001b[39m210.01536\u001b[39m | \u001b[39m18.707619\u001b[39m | \u001b[39m0.9776234\u001b[39m | \u001b[39m0.8421165\u001b[39m | \u001b[39m0.7200762\u001b[39m | \u001b[39m1.2203823\u001b[39m | \u001b[39m4.9517691\u001b[39m | \u001b[39m70.568207\u001b[39m | \u001b[39m1863.9806\u001b[39m |\n",
            "[1396]\ttraining's gamma: 1.15195\n",
            "| \u001b[39m5        \u001b[39m | \u001b[39m-0.325620\u001b[39m | \u001b[39m0.0591681\u001b[39m | \u001b[39m9.3001782\u001b[39m | \u001b[39m90.810658\u001b[39m | \u001b[39m28.403060\u001b[39m | \u001b[39m5.4675560\u001b[39m | \u001b[39m0.5924272\u001b[39m | \u001b[39m0.9847923\u001b[39m | \u001b[39m7.7513282\u001b[39m | \u001b[39m9.3949894\u001b[39m | \u001b[39m234.91202\u001b[39m | \u001b[39m1396.8499\u001b[39m |\n",
            "[601]\ttraining's gamma: 1.16197\n",
            "| \u001b[39m6        \u001b[39m | \u001b[39m-0.337998\u001b[39m | \u001b[39m0.1630603\u001b[39m | \u001b[39m4.5104379\u001b[39m | \u001b[39m118.54525\u001b[39m | \u001b[39m40.453891\u001b[39m | \u001b[39m6.9317616\u001b[39m | \u001b[39m0.7576943\u001b[39m | \u001b[39m0.7422358\u001b[39m | \u001b[39m5.9267590\u001b[39m | \u001b[39m4.2077164\u001b[39m | \u001b[39m95.527041\u001b[39m | \u001b[39m601.45059\u001b[39m |\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1152429788.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m )\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_points\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0mbest_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/bayes_opt/bayesian_optimization.py\u001b[0m in \u001b[0;36mmaximize\u001b[0;34m(self, init_points, n_iter)\u001b[0m\n\u001b[1;32m    320\u001b[0m                 \u001b[0mx_probe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuggest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 \u001b[0miteration\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_probe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlazy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bounds_transformer\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/bayes_opt/bayesian_optimization.py\u001b[0m in \u001b[0;36mprobe\u001b[0;34m(self, params, lazy)\u001b[0m\n\u001b[1;32m    237\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m             self.logger.log_optimization_step(\n\u001b[1;32m    241\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/bayes_opt/target_space.py\u001b[0m in \u001b[0;36mprobe\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    553\u001b[0m             \u001b[0merror_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"No target function has been provided.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mdict_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constraint\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1152429788.py\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(learning_rate, max_depth, num_leaves, min_child_samples, min_child_weight, subsample, colsample_bytree, reg_alpha, reg_lambda, max_bin, n_estimators)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperiod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     model = lgb.train(\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mtrain_pool\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, feval, init_model, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalid_sets\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_valid_contain_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                 \u001b[0mevaluation_result_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0mevaluation_result_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_valid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36meval_train\u001b[0;34m(self, feval)\u001b[0m\n\u001b[1;32m   4406\u001b[0m             \u001b[0mList\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_higher_better\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mtuples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4407\u001b[0m         \"\"\"\n\u001b[0;32m-> 4408\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__inner_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_data_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4410\u001b[0m     def eval_valid(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36m__inner_eval\u001b[0;34m(self, data_name, data_idx, feval)\u001b[0m\n\u001b[1;32m   5183\u001b[0m             \u001b[0mtmp_out_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5184\u001b[0m             _safe_call(\n\u001b[0;32m-> 5185\u001b[0;31m                 _LIB.LGBM_BoosterGetEval(\n\u001b[0m\u001b[1;32m   5186\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5187\u001b[0m                     \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.boxplot(y_log)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KfnDQgdvH52V",
        "outputId": "84188a52-b660-4c26-c860-93b88acae7f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'whiskers': [<matplotlib.lines.Line2D at 0x7cfafd2e57f0>,\n",
              "  <matplotlib.lines.Line2D at 0x7cfafd2e5af0>],\n",
              " 'caps': [<matplotlib.lines.Line2D at 0x7cfafd2e5d90>,\n",
              "  <matplotlib.lines.Line2D at 0x7cfafd2e5f10>],\n",
              " 'boxes': [<matplotlib.lines.Line2D at 0x7cfafd2e54f0>],\n",
              " 'medians': [<matplotlib.lines.Line2D at 0x7cfafd2e6210>],\n",
              " 'fliers': [<matplotlib.lines.Line2D at 0x7cfafd2e64e0>],\n",
              " 'means': []}"
            ]
          },
          "metadata": {},
          "execution_count": 70
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAGdCAYAAADAAnMpAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAIiFJREFUeJzt3X9wVNX9//HXJpoNkWSBUZIAi8mXMCIlEApBghOBftAMOsoO8i3iV0IddL620NGGVg3jYIszbEeJtTOlomOVqQ6FAiF0UgtSLBBlLb9Mh+BowfKrmo06A7tJwGCz9/tHv1nZkF+bhHuyu8/HzJ3hnnvu3nf+IPvKueee67AsyxIAAIAhSaYLAAAAiY0wAgAAjCKMAAAAowgjAADAKMIIAAAwijACAACMIowAAACjCCMAAMCo60wX0BOhUEiff/650tPT5XA4TJcDAAB6wLIsNTY2asSIEUpK6nz8IybCyOeffy632226DAAA0Avnzp3TqFGjOj0eE2EkPT1d0n9/mIyMDMPVAACAnggGg3K73eHv8c7ERBhpuzWTkZFBGAEAIMZ0N8WCCawAAMAowggAADCKMAIAAIwijAAAAKMIIwAAwCjCCAAAMIowAgAAjCKMAAAAo2Ji0TMA8am1tVU1NTWqr69Xdna2iouLlZycbLosADZjZASAEZWVlcrLy9Ps2bP14IMPavbs2crLy1NlZaXp0gDYjDACwHaVlZVasGCB8vPz5fP51NjYKJ/Pp/z8fC1YsIBAAiQYh2VZlukiuhMMBuVyuRQIBHg3DRDjWltblZeXp/z8fFVVVUW8VjwUCsnj8aiurk4nTpzglg0Q43r6/c3ICABb1dTU6PTp01q5cmVEEJGkpKQklZeX69SpU6qpqTFUIQC7EUYA2Kq+vl6SNGHChA6Pt7W39QMQ/wgjAGyVnZ0tSaqrq+vweFt7Wz8A8Y8wAsBWxcXFysnJ0Zo1axQKhSKOhUIheb1e5ebmqri42FCFAOxGGAFgq+TkZFVUVKi6uloejyfiaRqPx6Pq6mqtXbuWyatAAmHRMwC2mz9/vrZu3aoVK1ZoxowZ4fbc3Fxt3bpV8+fPN1gdALvxaC8AY1iBFYhvPf3+ZmQEgDHJycmaNWuW6TIAGMacEQAAYBRhBAAAGEUYAQAARhFGAACAUVGFkZdfflkTJ05URkaGMjIyVFRUpL/85S9dnrNlyxaNGzdOqampys/P19tvv92nggEAQHyJKoyMGjVKv/zlL3XkyBEdPnxY3/ve9zRv3jwdP368w/4HDhzQokWLtHTpUn344YfyeDzhN3ICAABI/bDOyLBhw/TCCy9o6dKlVx1buHChmpubVV1dHW6bPn26CgoKtH79+h5fg3VGAACIPT39/u71nJHW1lZt2rRJzc3NKioq6rCPz+fTnDlzItpKSkrk8/m6/OyWlhYFg8GIDQAAxKeow8ixY8c0ePBgOZ1OPfbYY9q+fbvGjx/fYV+/36/MzMyItszMTPn9/i6v4fV65XK5wpvb7Y62TAAAECOiDiO33HKLamtr9fe//10//OEPtWTJEn300Uf9WlR5ebkCgUB4O3fuXL9+PgAAGDiiXg4+JSVFeXl5kqQpU6bo0KFD+vWvf61XXnnlqr5ZWVlqaGiIaGtoaFBWVlaX13A6nXI6ndGWBgAAYlCf1xkJhUJqaWnp8FhRUZH27NkT0bZ79+5O55gAAIDEE9XISHl5uebOnavRo0ersbFRGzdu1N69e7Vr1y5JUmlpqUaOHCmv1ytJevzxxzVz5kxVVFTonnvu0aZNm3T48GG9+uqr/f+TAACAmBRVGPniiy9UWlqq+vp6uVwuTZw4Ubt27dKdd94pSTp79qySkr4dbJkxY4Y2btyoZ555RitXrtTYsWNVVVWlCRMm9O9PAQAAYlaf1xmxA+uMAAAQe675OiMAAAD9gTACAACMIowAAACjCCMAAMAowggAADCKMAIAAIwijAAAAKMIIwAAwCjCCAAAMIowAgAAjCKMAAAAowgjAADAqKje2gsA/am1tVU1NTWqr69Xdna2iouLlZycbLosADZjZASAEZWVlcrLy9Ps2bP14IMPavbs2crLy1NlZaXp0gDYjDACwHaVlZVasGCB8vPz5fP51NjYKJ/Pp/z8fC1YsIBAAiQYh2VZlukiuhMMBuVyuRQIBJSRkWG6HAB90Nraqry8POXn56uqqkpJSd/+TRQKheTxeFRXV6cTJ05wywaIcT39/mZkBICtampqdPr0aa1cuTIiiEhSUlKSysvLderUKdXU1BiqEIDdCCMAbFVfXy9JmjBhQofH29rb+gGIf4QRALbKzs6WJNXV1XV4vK29rR+A+MecEQC2unLOyLZt2/T++++HH+29/fbbdf/99zNnBIgTPf3+Zp0RALZKTk5WRUWF7r//frlcLl26dCl8bNCgQbp06ZK2bdtGEAESCLdpABjhcDg6bOuoHUB84zYNAFtxmwZIHNymATAgtT3a+4c//EHXX3+9Zs2aFXG8vLxcM2bMUE1NzVXHAMQnbtMAsBWP9gJojzACwFY82gugPcIIAFsVFxcrJydHa9asUSgUijgWCoXk9XqVm5ur4uJiQxUCsBthBICt2h7tra6ulsfjiXhRnsfjUXV1tdauXcvkVSCBMIEVgO3mz5+vrVu3asWKFZoxY0a4PTc3V1u3btX8+fMNVgfAbjzaC8CY1tZW1dTUhB/tLS4uZkQEiCM82gtgwEtOTubxXQDMGQEAAGYRRgAAgFGEEQAAYBRhBAAAGEUYAQAARhFGAACAUYQRAABgFGEEAAAYRRgBAABGEUYAAIBRhBEAAGBUVGHE6/WqsLBQ6enpGj58uDwejz755JMuz9mwYYMcDkfElpqa2qeiAQBA/IgqjOzbt0/Lli3TBx98oN27d+ubb77RXXfdpebm5i7Py8jIUH19fXg7c+ZMn4oGEB8uXbqk5cuXq6SkRMuXL9elS5dMlwTAgKje2rtz586I/Q0bNmj48OE6cuSI7rjjjk7PczgcysrK6l2FAOKSx+PRjh07wvvvvPOO1q1bp3nz5qmqqspcYQBs16c5I4FAQJI0bNiwLvs1NTXp5ptvltvt1rx583T8+PG+XBZAjGsLIikpKXr66ad18uRJPf3000pJSdGOHTvk8XhMlwjARg7LsqzenBgKhXTffffpwoULeu+99zrt5/P5dOLECU2cOFGBQEBr167V/v37dfz4cY0aNarDc1paWtTS0hLeDwaDcrvdCgQCysjI6E25AAaIS5cuKS0tTSkpKWpsbFRKSkr42OXLl5Wenq7Lly/r4sWLGjRokMFKAfRVMBiUy+Xq9vu71yMjy5YtU11dnTZt2tRlv6KiIpWWlqqgoEAzZ85UZWWlbrrpJr3yyiudnuP1euVyucKb2+3ubZkABpif/exnkqSysrKIICJJKSkpeuKJJyL6AYh/vQojy5cvV3V1tf72t791OrrRmeuvv16TJ0/WyZMnO+1TXl6uQCAQ3s6dO9ebMgEMQCdOnJAkPfLIIx0eX7p0aUQ/APEvqjBiWZaWL1+u7du3691331Vubm7UF2xtbdWxY8eUnZ3daR+n06mMjIyIDUB8GDt2rCTptdde6/D47373u4h+AOJfVHNGfvSjH2njxo3asWOHbrnllnC7y+UK39stLS3VyJEj5fV6JUmrV6/W9OnTlZeXpwsXLuiFF15QVVWVjhw5ovHjx/fouj295wRg4GPOCJA4rsmckZdfflmBQECzZs1SdnZ2eNu8eXO4z9mzZ1VfXx/eP3/+vB599FHdeuutuvvuuxUMBnXgwIEeBxEA8WXQoEGaN29eOHg89dRT+uc//6mnnnoqHETmzZtHEAESSK+fprETIyNA/Gm/zkgb1hkB4kdPv7+jWvQMAPpLVVWVmpqatHjxYn366acaM2aM3nzzTQ0ePNh0aQBsxovyABhRWVmp/Px8VVVV6dixY6qqqlJ+fr4qKytNlwbAZoQRALarrKzUggULlJ+fL5/Pp8bGRvl8PuXn52vBggUEEiDBMGcEgK1aW1uVl5cXHhVJSvr2b6JQKCSPx6O6ujqdOHFCycnJBisF0FfXfAVWAOiNmpoanT59WitXrowIIpKUlJSk8vJynTp1SjU1NYYqBGA3wggAW7U9+j9hwoQOj7e1X7lEAID4RhgBYKu21Zfr6uo6PN7W3tUqzQDiC2EEgK2Ki4uVk5OjNWvWKBQKRRwLhULyer3Kzc1VcXGxoQoB2I0wAsBWycnJqqioUHV1tTweT8TTNB6PR9XV1Vq7di2TV4EEwqJnAGw3f/58bd26VStWrNCMGTPC7bm5udq6davmz59vsDoAduPRXgDGfPbZZ8rPz1djY6PS09N17NgxjRw50nRZAPoJy8EDGNBuuOEGXbx4Mbx//vx5jRo1SmlpaWpubjZYGQC7MWcEgO2uDCK5ubnasmWLcnNzJUkXL17UDTfcYLI8ADZjZASArfx+fziInD9/XkOGDJEkLViwQBcuXNDQoUN18eJF+f1+ZWVlGawUgF0YGQFgq4KCAkn/HRFpCyJthgwZoptvvjmiH4D4RxgBYKsLFy5Ikp5//vkOj69ZsyaiH4D4RxgBYKu20ZAnn3yyw+MrV66M6Acg/hFGANiqtrZWknTq1KmrRj8uXLigM2fORPQDEP8IIwBslZWVpbS0NEnS0KFDlZOTo40bNyonJ0dDhw6VJKWlpTF5FUggLHoGwIj264y0YZ0RIH709PubkREARjQ3N6u+vl6ZmZlyOp3KzMxUfX09QQRIQKwzAsCYwYMHq6ioSJ9++qnGjBmjwYMHmy4JgAGEEQBGTJs2TYcOHQrvHzt2TOnp6SosLNTBgwcNVgbAbtymAWC7tiDicDi0ePFi/eMf/9DixYvlcDh06NAhTZs2zXSJAGzEBFYAtmpqalJ6erocDocuXryo1NTU8LGvv/5aaWlpsixLjY2N3LYBYhwTWAEMSIsXL5YkPfTQQxFBRJJSU1P14IMPRvQDEP8IIwBs9emnn0qSfvrTn3Z4vKysLKIfgPhHGAFgqzFjxkiS1q5d2+HxF198MaIfgPjHnBEAtmLOCJA4mDMCYEAaPHiwCgsLZVmW0tLS9NBDD+no0aN66KGHwkGksLCQIAIkEEZGABjRfp2RNqwzAsSPnn5/s+gZACMOHjyopqYmLV68OLwC65tvvsmICJCACCMAjBk8eLC2b99uugwAhjFnBAAAGEUYAWDMvn375HA4wtu+fftMlwTAAG7TADDC4XBc1TZr1ixJUgzMqwfQjxgZAWC79kHk+9//fpfHAcQ3wggAW115K+b48eOyLEubN2+WZVk6fvx4h/0AxDfWGQFgqytHPTr69dPdcQCxgxVYAQxo7W/NtLnvvvtsrgSAaYyMALAVIyNA4mBkBMCAtHfv3vC/P/roo4hjV+5f2Q9AfGNkBIDt2j8tc9999+lPf/pTRFsM/GoC0I1rMjLi9XpVWFio9PR0DR8+XB6PR5988km3523ZskXjxo1Tamqq8vPz9fbbb0dzWQBxpn3QIIgAiS2qMLJv3z4tW7ZMH3zwgXbv3q1vvvlGd911l5qbmzs958CBA1q0aJGWLl2qDz/8UB6PRx6PR3V1dX0uHkDssizrqlsxe/fuJYgACahPt2m+/PJLDR8+XPv27dMdd9zRYZ+FCxequblZ1dXV4bbp06eroKBA69ev79F1uE0DAEDssWUCayAQkCQNGzas0z4+n09z5syJaCspKZHP5+v0nJaWFgWDwYgNAADEp16HkVAopCeeeEK33367JkyY0Gk/v9+vzMzMiLbMzEz5/f5Oz/F6vXK5XOHN7Xb3tkwAADDA9TqMLFu2THV1ddq0aVN/1iNJKi8vVyAQCG/nzp3r92sAMO+5556LeGvvc889Z7okAAb06q29y5cvV3V1tfbv369Ro0Z12TcrK0sNDQ0RbQ0NDcrKyur0HKfTKafT2ZvSAMSIjl6Gt2rVKq1atYpJrECCiWpkxLIsLV++XNu3b9e7776r3Nzcbs8pKirSnj17Itp2796toqKi6CoFEDfaB5EhQ4Z0eRxAfIsqjCxbtkxvvfWWNm7cqPT0dPn9fvn9fl26dCncp7S0VOXl5eH9xx9/XDt37lRFRYU+/vhj/fznP9fhw4e1fPny/vspAMSMK2/FvPHGG7IsS+fPn5dlWXrjjTc67AcgvkX1aG9nf6288cYb+sEPfiBJmjVrlnJycrRhw4bw8S1btuiZZ57R6dOnNXbsWD3//PO6++67e1wkj/YC8YN30wCJo6ff3ywHD8BWbWFjyJAhOn/+/FXHMzIy1NjYKIkwAsQ6XpQHYEC7cOFCh+1tQQRA4iCMALDV6tWrw/++8nZu+/0r+wGIb9ymAWC79vPP0tPTrxoRiYFfTQC6wW0aAANW+6BBEAESG2EEgBGWZV11K2b16tUEESABcZsGAABcE9ymAQAAMYEwAgAAjCKMAAAAowgjAADAKMIIAGMKCwvlcDjCW2FhoemSABhwnekCACSmjl68efjwYTkcDh7vBRIMIyMAbNfZG8B7ehxAfCGMALDVlbdiHnjgAVmWFd4eeOCBDvsBiG8segbAVleOenT066e74wBiB4ueAQCAmEAYAQAARhFGANhq6tSp4X8vWrQo4tiV+1f2AxDfmDMCwHY9eVomBn41AegGc0YADFjdBQ2CCJBYCCMAjLAs66pbMVOnTiWIAAmIFVgBGHPo0CHTJQAYABgZAQAARhFGAACAUYQRAABgFGEEAAAYRRgBAABGEUYAAIBRhBEAAGAUYQQAABjFomcAjOnoHTWswAokHkZGABjR2cvyevISPQDxhTACwHbdBQ4CCZBYCCMAbHVl0EhOTpZlWeEtOTm5w34A4hthBIAx//nPf7rcB5AYCCMAAMAowggAADCKMALAmOuuu67LfQCJgf/5AGxlWVZ4cmpra2unE1VZbwRIHIyMALBdd0GDIAIkFsIIACM6CxwEESDxcJsGgDEEDwASIyMAAMCwqMPI/v37de+992rEiBFyOByqqqrqsv/evXvlcDiu2vx+f29rBgAAcSTqMNLc3KxJkyZp3bp1UZ33ySefqL6+PrwNHz482ksDAIA4FPWckblz52ru3LlRX2j48OEaMmRI1OcBAID4ZtuckYKCAmVnZ+vOO+/U+++/32XflpYWBYPBiA0AAMSnax5GsrOztX79em3btk3btm2T2+3WrFmzdPTo0U7P8Xq9crlc4c3tdl/rMgEAgCEOqw/P1jkcDm3fvl0ejyeq82bOnKnRo0frzTff7PB4S0uLWlpawvvBYFBut1uBQEAZGRm9LRcAANgoGAzK5XJ1+/1tZJ2RadOm6b333uv0uNPplNPptLEiAABgipF1Rmpra5WdnW3i0gAAYICJemSkqalJJ0+eDO+fOnVKtbW1GjZsmEaPHq3y8nJ99tln+v3vfy9Jeumll5Sbm6vvfOc7+vrrr/Xaa6/p3Xff1TvvvNN/PwUAAIhZUYeRw4cPa/bs2eH9srIySdKSJUu0YcMG1dfX6+zZs+Hjly9f1ooVK/TZZ58pLS1NEydO1F//+teIzwAAAImrTxNY7dLTCTAAAGDgGNATWAFA+u8Tee3FwN9HAPoZL8oDYERHQaSrdgDxizACwHbdBQ4CCZBYCCMAbNU+aFiWFd666gcgfhFGABjTPoAwXwRITIQRAABgFGEEAAAYRRgBYEz7eSHMEwESE+uMALCVZVkRoaOzAML8ESBxMDICwHbdBQ2CCJBYCCMAjOgscBBEgMTDbRoAxhA8AEiMjAAAAMMIIwAAwCjCCAAAMIowAgAAjCKMAAAAowgjAADAKMIIAAAwijACAACMIowAAACjCCMAAMAowggAADCKMAIAAIwijAAAAKMIIwAAwCjCCAAAMIowAgAAjCKMAAAAowgjAADAKMIIAAAw6jrTBQBIXA6H46o2y7IMVALAJEZGABjRURDpqh1A/CKMALBdd4GDQAIkFsIIAFu1DxqWZYW3rvoBiF+EEQDGtA8gzBcBEhNhBAAAGEUYAQAARhFGABjTfl4I80SAxMQ6IwBsZVlWROjoLIAwfwRIHIyMALBdd0GDIAIkFsIIACM6CxwEESDxRB1G9u/fr3vvvVcjRoyQw+FQVVVVt+fs3btX3/3ud+V0OpWXl6cNGzb0olQA8ebKNUY6WmsEQGKIOow0Nzdr0qRJWrduXY/6nzp1Svfcc49mz56t2tpaPfHEE3rkkUe0a9euqIsFAADxJ+oJrHPnztXcuXN73H/9+vXKzc1VRUWFJOnWW2/Ve++9p1/96lcqKSmJ9vIAACDOXPM5Iz6fT3PmzIloKykpkc/n6/SclpYWBYPBiA0AAMSnax5G/H6/MjMzI9oyMzMVDAZ16dKlDs/xer1yuVzhze12X+syAQCAIQPyaZry8nIFAoHwdu7cOdMlAQCAa+SaL3qWlZWlhoaGiLaGhgZlZGRo0KBBHZ7jdDrldDqvdWkAAGAAuOYjI0VFRdqzZ09E2+7du1VUVHStLw0AAGJA1GGkqalJtbW1qq2tlfTfR3dra2t19uxZSf+9xVJaWhru/9hjj+lf//qXnnzySX388cf67W9/qz/+8Y/6yU9+0j8/AQAAiGlRh5HDhw9r8uTJmjx5siSprKxMkydP1qpVqyRJ9fX14WAiSbm5ufrzn/+s3bt3a9KkSaqoqNBrr73GY70AAECS5LBiYMnDYDAol8ulQCCgjIwM0+UAAIAe6On394B8mgYAACQOwggAADCKMAIAAIwijAAAAKMIIwAAwCjCCAAAMIowAgAAjCKMAAAAowgjAADAKMIIAAAwijACAACMIowAAACjCCMAAMAowggAADCKMAIAAIwijAAAAKMIIwAAwCjCCAAAMIowAgAAjCKMAAAAowgjAADAqOtMFwAgcTkcjqvaLMsyUAkAkxgZAWBER0Gkq3YA8YswAsB23QUOAgmQWAgjAGzVPmhYlhXeuuoHIH4RRgAY0z6AMF8ESEyEEQAAYBRhBAAAGEUYAWBM+3khzBMBEhPrjACwlWVZEaGjswDC/BEgcTAyAsB23QUNggiQWAgjAIzoLHAQRIDEw20aAMYQPABIjIwAAADDCCMAAMAowggAADCKMAIAAIwijAAAAKMIIwAAwCjCCAAAMIowAgAAjCKMAAAAowgjAADAqF6FkXXr1iknJ0epqam67bbbdPDgwU77btiwQQ6HI2JLTU3tdcEAACC+RB1GNm/erLKyMj377LM6evSoJk2apJKSEn3xxRednpORkaH6+vrwdubMmT4VDQAA4kfUYeTFF1/Uo48+qocffljjx4/X+vXrlZaWptdff73TcxwOh7KyssJbZmZmn4oGAADxI6owcvnyZR05ckRz5sz59gOSkjRnzhz5fL5Oz2tqatLNN98st9utefPm6fjx411ep6WlRcFgMGIDAADxKaow8tVXX6m1tfWqkY3MzEz5/f4Oz7nlllv0+uuva8eOHXrrrbcUCoU0Y8YM/fvf/+70Ol6vVy6XK7y53e5oygQAADHkmj9NU1RUpNLSUhUUFGjmzJmqrKzUTTfdpFdeeaXTc8rLyxUIBMLbuXPnrnWZAADAkOui6XzjjTcqOTlZDQ0NEe0NDQ3Kysrq0Wdcf/31mjx5sk6ePNlpH6fTKafTGU1pAAAgRkU1MpKSkqIpU6Zoz5494bZQKKQ9e/aoqKioR5/R2tqqY8eOKTs7O7pKAQBAXIpqZESSysrKtGTJEk2dOlXTpk3TSy+9pObmZj388MOSpNLSUo0cOVJer1eStHr1ak2fPl15eXm6cOGCXnjhBZ05c0aPPPJI//4kAAAgJkUdRhYuXKgvv/xSq1atkt/vV0FBgXbu3Bme1Hr27FklJX074HL+/Hk9+uij8vv9Gjp0qKZMmaIDBw5o/Pjx/fdTAACAmOWwLMsyXUR3gsGgXC6XAoGAMjIyTJcDAAB6oKff37ybBgAAGEUYAQAARhFGAACAUYQRAABgFGEEAAAYRRgBAABGEUYAAIBRhBEAAGAUYQQAABhFGAEAAEYRRgAAgFGEEQAAYBRhBAAAGEUYAQAARhFGAACAUYQRAABgFGEEAAAYRRgBAABGEUYAAIBRhBEAAGAUYQQAABhFGAEAAEYRRgAAgFGEEQAAYBRhBAAAGEUYAQAARhFGAACAUYQRAABgFGEEAAAYRRgBAABGEUYAAIBRhBEAAGAUYQQAABhFGAEAAEYRRgAAgFGEEQAAYBRhBAAAGEUYAQAARhFGAACAUYQRAABgFGEEAAAYRRgBAABG9SqMrFu3Tjk5OUpNTdVtt92mgwcPdtl/y5YtGjdunFJTU5Wfn6+33367V8UCAID4E3UY2bx5s8rKyvTss8/q6NGjmjRpkkpKSvTFF1902P/AgQNatGiRli5dqg8//FAej0cej0d1dXV9Lh4AAMQ+h2VZVjQn3HbbbSosLNRvfvMbSVIoFJLb7daPf/xjPf3001f1X7hwoZqbm1VdXR1umz59ugoKCrR+/foeXTMYDMrlcikQCCgjIyOacgEAgCE9/f6+LpoPvXz5so4cOaLy8vJwW1JSkubMmSOfz9fhOT6fT2VlZRFtJSUlqqqq6vQ6LS0tamlpCe8Hg8FoygTQia+++kq7tv1eaa19+z918WKzPv30X/1UVf8aM+Z/KS3thj5/zo2531Hx3P/dDxUB6E5UYeSrr75Sa2urMjMzI9ozMzP18ccfd3iO3+/vsL/f7+/0Ol6vV7/4xS+iKQ1AD1RVVenff1ipn89y9v3DMrvvYkTT/9/66Od/bNFNufkaN25c3z8MQJeiCiN2KS8vjxhNCQaDcrvdBisC4oPH49Gu1qC2MzLSrf956jsEEcAmUYWRG2+8UcnJyWpoaIhob2hoUFZWVofnZGVlRdVfkpxOp5zOfvjLDUCEG2+8Uf/n/5Z13xEAbBTV0zQpKSmaMmWK9uzZE24LhULas2ePioqKOjynqKgoor8k7d69u9P+AAAgsUR9m6asrExLlizR1KlTNW3aNL300ktqbm7Www8/LEkqLS3VyJEj5fV6JUmPP/64Zs6cqYqKCt1zzz3atGmTDh8+rFdffbV/fxIAABCTog4jCxcu1JdffqlVq1bJ7/eroKBAO3fuDE9SPXv2rJKSvh1wmTFjhjZu3KhnnnlGK1eu1NixY1VVVaUJEyb0308BAABiVtTrjJjAOiMAAMSenn5/824aAABgFGEEAAAYRRgBAABGEUYAAIBRhBEAAGAUYQQAABhFGAEAAEYRRgAAgFGEEQAAYFTUy8Gb0LZIbDDYt9eeAwAA+7R9b3e32HtMhJHGxkZJktvtNlwJAACIVmNjo1wuV6fHY+LdNKFQSJ9//rnS09PlcDhMlwOgHwWDQbndbp07d453TwFxxrIsNTY2asSIEREv0W0vJsIIgPjFizABMIEVAAAYRRgBAABGEUYAGOV0OvXss8/K6XSaLgWAIcwZAQAARjEyAgAAjCKMAAAAowgjAADAKMIIAAAwijACwIj9+/fr3nvv1YgRI+RwOFRVVWW6JACGEEYAGNHc3KxJkyZp3bp1pksBYFhMvCgPQPyZO3eu5s6da7oMAAMAIyMAAMAowggAADCKMAIAAIwijAAAAKMIIwAAwCiepgFgRFNTk06ePBneP3XqlGprazVs2DCNHj3aYGUA7MZbewEYsXfvXs2ePfuq9iVLlmjDhg32FwTAGMIIAAAwijkjAADAKMIIAAAwijACAACMIowAAACjCCMAAMAowggAADCKMAIAAIwijAAAAKMIIwAAwCjCCAAAMIowAgAAjCKMAAAAo/4fvQKG/MbY4k8AAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import lightgbm as lgb\n",
        "from bayes_opt import BayesianOptimization\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "pbounds_lgb = {\n",
        "    'learning_rate': (0.01, 0.2),\n",
        "    'max_depth': (4, 12),\n",
        "    'num_leaves': (16, 256),\n",
        "    'min_child_samples': (5, 50),\n",
        "    'min_child_weight': (1e-3, 10.0),\n",
        "    'subsample': (0.5, 1.0),\n",
        "    'colsample_bytree': (0.5, 1.0),\n",
        "    'reg_alpha': (0, 10),\n",
        "    'reg_lambda': (0, 10),\n",
        "    'max_bin': (64, 255),\n",
        "    'n_estimators': (500, 2000)\n",
        "}\n",
        "\n",
        "def objective(learning_rate, max_depth, num_leaves, min_child_samples,\n",
        "              min_child_weight, subsample, colsample_bytree,\n",
        "              reg_alpha, reg_lambda, max_bin, n_estimators):\n",
        "\n",
        "    train_pool = lgb.Dataset(X_1, label=y_log, categorical_feature=cat_features)\n",
        "\n",
        "    max_depth = int(max_depth)\n",
        "    num_leaves = int(num_leaves)\n",
        "    min_child_samples = int(min_child_samples)\n",
        "    max_bin = int(max_bin)\n",
        "    n_estimators = int(n_estimators)\n",
        "\n",
        "    params = {\n",
        "        'objective': 'regression',\n",
        "        'metric': 'rmse',\n",
        "        'boosting_type': 'gbdt',\n",
        "        'learning_rate': learning_rate,\n",
        "        'num_leaves': num_leaves,\n",
        "        'max_depth': max_depth,\n",
        "        'min_child_samples': min_child_samples,\n",
        "        'min_child_weight': min_child_weight,\n",
        "        'subsample': subsample,\n",
        "        'colsample_bytree': colsample_bytree,\n",
        "        'reg_alpha': reg_alpha,\n",
        "        'reg_lambda': reg_lambda,\n",
        "        'max_bin': max_bin,\n",
        "        # 'device': 'gpu',\n",
        "        'verbose': -1\n",
        "    }\n",
        "\n",
        "    callbacks = [lgb.log_evaluation(period=100)]\n",
        "\n",
        "    model = lgb.train(\n",
        "        params,\n",
        "        train_pool,\n",
        "        num_boost_round=n_estimators,\n",
        "        valid_sets=[train_pool],\n",
        "        callbacks=callbacks\n",
        "    )\n",
        "\n",
        "    families_models.set_family('BABY CARE', model, input_func_1, output_func_2)\n",
        "    pred_frame, common_loss = families_models.predict_family(valid, 'BABY CARE', True, verbose=False)\n",
        "\n",
        "    return -common_loss\n",
        "\n",
        "optimizer = BayesianOptimization(\n",
        "    f=objective,\n",
        "    pbounds=pbounds_lgb,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "optimizer.maximize(init_points=5, n_iter=20)\n",
        "\n",
        "best_params = optimizer.max['params']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2LQAcsWyags",
        "outputId": "b1a22adf-b452-4505-d432-c368b18243c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|   iter    |  target   | learni... | max_depth | num_le... | min_ch... | min_ch... | subsample | colsam... | reg_alpha | reg_la... |  max_bin  | n_esti... |\n",
            "-------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "[100]\ttraining's rmse: 0.502578\n",
            "[200]\ttraining's rmse: 0.415818\n",
            "[300]\ttraining's rmse: 0.36084\n",
            "[400]\ttraining's rmse: 0.326649\n",
            "[500]\ttraining's rmse: 0.315158\n",
            "| \u001b[39m1        \u001b[39m | \u001b[39m-0.342218\u001b[39m | \u001b[39m0.0811626\u001b[39m | \u001b[39m11.605714\u001b[39m | \u001b[39m191.67854\u001b[39m | \u001b[39m31.939631\u001b[39m | \u001b[39m1.5610303\u001b[39m | \u001b[39m0.5779972\u001b[39m | \u001b[39m0.5290418\u001b[39m | \u001b[39m8.6617614\u001b[39m | \u001b[39m6.0111501\u001b[39m | \u001b[39m199.24186\u001b[39m | \u001b[39m530.87674\u001b[39m |\n",
            "[100]\ttraining's rmse: 0.397738\n",
            "[200]\ttraining's rmse: 0.307469\n",
            "[300]\ttraining's rmse: 0.254234\n",
            "[400]\ttraining's rmse: 0.218713\n",
            "[500]\ttraining's rmse: 0.192154\n",
            "[600]\ttraining's rmse: 0.175042\n",
            "[700]\ttraining's rmse: 0.170545\n",
            "| \u001b[39m2        \u001b[39m | \u001b[39m-0.355864\u001b[39m | \u001b[39m0.1942828\u001b[39m | \u001b[39m10.659541\u001b[39m | \u001b[39m66.961386\u001b[39m | \u001b[39m13.182123\u001b[39m | \u001b[39m1.8348616\u001b[39m | \u001b[39m0.6521211\u001b[39m | \u001b[39m0.7623782\u001b[39m | \u001b[39m4.3194501\u001b[39m | \u001b[39m2.9122914\u001b[39m | \u001b[39m180.86390\u001b[39m | \u001b[39m709.24079\u001b[39m |\n",
            "[100]\ttraining's rmse: 0.589537\n",
            "[200]\ttraining's rmse: 0.556144\n",
            "[300]\ttraining's rmse: 0.526371\n",
            "[400]\ttraining's rmse: 0.500096\n",
            "[500]\ttraining's rmse: 0.478764\n",
            "| \u001b[39m3        \u001b[39m | \u001b[39m-0.362021\u001b[39m | \u001b[39m0.0655074\u001b[39m | \u001b[39m6.9308947\u001b[39m | \u001b[39m125.45679\u001b[39m | \u001b[39m40.332918\u001b[39m | \u001b[39m1.9975381\u001b[39m | \u001b[39m0.7571172\u001b[39m | \u001b[39m0.7962072\u001b[39m | \u001b[39m0.4645041\u001b[39m | \u001b[39m6.0754485\u001b[39m | \u001b[39m96.570107\u001b[39m | \u001b[39m597.57738\u001b[39m |\n",
            "[100]\ttraining's rmse: 0.284324\n",
            "[200]\ttraining's rmse: 0.164644\n",
            "[300]\ttraining's rmse: 0.109409\n",
            "[400]\ttraining's rmse: 0.0816714\n",
            "[500]\ttraining's rmse: 0.0671934\n",
            "[600]\ttraining's rmse: 0.0636369\n",
            "[700]\ttraining's rmse: 0.0636369\n",
            "[800]\ttraining's rmse: 0.0636369\n",
            "[900]\ttraining's rmse: 0.0636369\n",
            "[1000]\ttraining's rmse: 0.0636369\n",
            "[1100]\ttraining's rmse: 0.0636369\n",
            "[1200]\ttraining's rmse: 0.0636369\n",
            "[1300]\ttraining's rmse: 0.0636369\n",
            "[1400]\ttraining's rmse: 0.0636369\n",
            "[1500]\ttraining's rmse: 0.0636369\n",
            "[1600]\ttraining's rmse: 0.0636369\n",
            "[1700]\ttraining's rmse: 0.0636369\n",
            "[1800]\ttraining's rmse: 0.0636369\n",
            "| \u001b[39m4        \u001b[39m | \u001b[39m-0.349515\u001b[39m | \u001b[39m0.1902882\u001b[39m | \u001b[39m11.725056\u001b[39m | \u001b[39m210.01536\u001b[39m | \u001b[39m18.707619\u001b[39m | \u001b[39m0.9776234\u001b[39m | \u001b[39m0.8421165\u001b[39m | \u001b[39m0.7200762\u001b[39m | \u001b[39m1.2203823\u001b[39m | \u001b[39m4.9517691\u001b[39m | \u001b[39m70.568207\u001b[39m | \u001b[39m1863.9806\u001b[39m |\n",
            "[100]\ttraining's rmse: 0.535352\n",
            "[200]\ttraining's rmse: 0.472298\n",
            "[300]\ttraining's rmse: 0.421201\n",
            "[400]\ttraining's rmse: 0.382418\n",
            "[500]\ttraining's rmse: 0.352172\n",
            "[600]\ttraining's rmse: 0.326659\n",
            "[700]\ttraining's rmse: 0.306757\n",
            "[800]\ttraining's rmse: 0.292765\n",
            "[900]\ttraining's rmse: 0.287976\n",
            "[1000]\ttraining's rmse: 0.287976\n",
            "[1100]\ttraining's rmse: 0.287976\n",
            "[1200]\ttraining's rmse: 0.287976\n",
            "[1300]\ttraining's rmse: 0.287976\n",
            "| \u001b[35m5        \u001b[39m | \u001b[35m-0.338299\u001b[39m | \u001b[35m0.0591681\u001b[39m | \u001b[35m9.3001782\u001b[39m | \u001b[35m90.810658\u001b[39m | \u001b[35m28.403060\u001b[39m | \u001b[35m5.4675560\u001b[39m | \u001b[35m0.5924272\u001b[39m | \u001b[35m0.9847923\u001b[39m | \u001b[35m7.7513282\u001b[39m | \u001b[35m9.3949894\u001b[39m | \u001b[35m234.91202\u001b[39m | \u001b[35m1396.8499\u001b[39m |\n",
            "[100]\ttraining's rmse: 0.606407\n",
            "[200]\ttraining's rmse: 0.557381\n",
            "[300]\ttraining's rmse: 0.530415\n",
            "[400]\ttraining's rmse: 0.503373\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3324591159.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m )\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_points\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0mbest_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/bayes_opt/bayesian_optimization.py\u001b[0m in \u001b[0;36mmaximize\u001b[0;34m(self, init_points, n_iter)\u001b[0m\n\u001b[1;32m    320\u001b[0m                 \u001b[0mx_probe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuggest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 \u001b[0miteration\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_probe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlazy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bounds_transformer\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/bayes_opt/bayesian_optimization.py\u001b[0m in \u001b[0;36mprobe\u001b[0;34m(self, params, lazy)\u001b[0m\n\u001b[1;32m    237\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m             self.logger.log_optimization_step(\n\u001b[1;32m    241\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/bayes_opt/target_space.py\u001b[0m in \u001b[0;36mprobe\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    553\u001b[0m             \u001b[0merror_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"No target function has been provided.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mdict_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constraint\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3324591159.py\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(learning_rate, max_depth, num_leaves, min_child_samples, min_child_weight, subsample, colsample_bytree, reg_alpha, reg_lambda, max_bin, n_estimators)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperiod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     model = lgb.train(\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mtrain_pool\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, feval, init_model, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    320\u001b[0m             )\n\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m         \u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_LGBM_BoosterEvalMethodResultType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   4153\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mLightGBMError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot update due to null objective function.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4154\u001b[0m             _safe_call(\n\u001b[0;32m-> 4155\u001b[0;31m                 _LIB.LGBM_BoosterUpdateOneIter(\n\u001b[0m\u001b[1;32m   4156\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4157\u001b[0m                     \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbyref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_finished\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}